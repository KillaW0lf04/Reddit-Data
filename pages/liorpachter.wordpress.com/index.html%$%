<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<!--
	generated in 1.006 seconds
	309769 bytes batcached for 300 seconds
-->
<head profile="http://gmpg.org/xfn/11">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Bits of DNA | Reviews and commentary on computational biology</title>
	<link rel="pingback" href="http://liorpachter.wordpress.com/xmlrpc.php" />
	<link rel="alternate" type="application/rss+xml" title="Bits of DNA &raquo; Feed" href="http://liorpachter.wordpress.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Bits of DNA &raquo; Comments Feed" href="http://liorpachter.wordpress.com/comments/feed/" />
<script type="text/javascript">
/* <![CDATA[ */
function addLoadEvent(func){var oldonload=window.onload;if(typeof window.onload!='function'){window.onload=func;}else{window.onload=function(){oldonload();func();}}}
/* ]]> */
</script>
<link rel='stylesheet' id='all-css-0' href='http://s1.wp.com/_static/??-eJyNUu1uwyAMfKEx2rWa9mfas1DiECcGI3AU5e1Hmi5RlQ3tF2e488cZPUVlOQgE0X5UkUaHIevskWCOiXuw8hy92pxf9O8ywgGy7kGisYO6Rwf6jdjtAnYOGh5FtUzEk56wcVCtgaHFgDJv4F9k6cCX1uJ402JSHrCmyh0nsdwUQWHpTFhgx5PaHmpqywnKvY9GFoaHBg1QKR+qc/n4/qNaYFccPJp3GENnmenRDurAgoWaN1Ar6IAVsTWCHJ4C1ZLBVJMmWLZYoLsbtIc1UeQsa+pisEmL+HHWuwyQ1qF2+Jcv3VU74puhWsb1i62rdcxNAtPcM375z/P1dD2f3z5Ol/4b4aIsPQ==' type='text/css' media='all' />
<link rel='stylesheet' id='print-css-0' href='http://s1.wp.com/wp-content/themes/pub/tarski/print.css?m=1323834012g' type='text/css' media='print' />
<script type='text/javascript'>
/* <![CDATA[ */
var LoggedOutFollow = {"invalid_email":"Your subscription did not succeed, please try again with a valid email address."};
/* ]]> */
</script>
<script type='text/javascript' src='http://s0.wp.com/_static/??-eJyFkNsOwiAMQH9IxrZo4ovxW3bpSBEoUhjRrxeTGTXO7KlNe9LTVmYvBnIRXJSapaUeDYjEEDpVagLdRJXmnSwcusGkEfgJ6muCcFvCJiAsqtBFqCy6F/xh9cTRAnNRrnS/VehmhLyJaYi+Gy4iAOP9Z2pvSAlvkkLHsuQKRkpRTGQMZZlxVBDXjmL/3v//NxbqbE/Nvq4Px6atW/0AiPqGzQ=='></script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://liorpachter.wordpress.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://liorpachter.wordpress.com/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress.com" />
<link rel='shortlink' href='http://wp.me/Rx4V' />

<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="website" />
<meta property="og:title" content="Bits of DNA" />
<meta property="og:description" content="Reviews and commentary on computational biology" />
<meta property="og:url" content="http://liorpachter.wordpress.com/" />
<meta property="og:site_name" content="Bits of DNA" />
<meta property="og:image" content="http://wordpress.com/i/blank.jpg?m=1383295312g" />
<meta name="twitter:site" content="@wordpressdotcom" />
<meta property="fb:app_id" content="249643311490" />
<meta property="article:publisher" content="https://www.facebook.com/WordPresscom" />
<meta property="article:author" content="https://www.facebook.com/profile.php?id=1096730610" />
<link rel="shortcut icon" type="image/x-icon" href="http://s2.wp.com/i/favicon.ico?m=1311975824g" sizes="16x16 24x24 32x32 48x48" />
<link rel="icon" type="image/x-icon" href="http://s2.wp.com/i/favicon.ico?m=1311975824g" sizes="16x16 24x24 32x32 48x48" />
<link rel="apple-touch-icon-precomposed" href="http://s0.wp.com/i/webclip.png?m=1391188133g" />
<link rel='openid.server' href='http://liorpachter.wordpress.com/?openidserver=1' />
<link rel='openid.delegate' href='http://liorpachter.wordpress.com/' />
<link rel="search" type="application/opensearchdescription+xml" href="http://liorpachter.wordpress.com/osd.xml" title="Bits of DNA" />
<link rel="search" type="application/opensearchdescription+xml" href="http://wordpress.com/opensearch.xml" title="WordPress.com" />
	<style type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</style>
		<style type="text/css">
			.recentcomments a {
				display: inline !important;
				padding: 0 !important;
				margin: 0 !important;
			}

			table.recentcommentsavatartop img.avatar, table.recentcommentsavatarend img.avatar {
				border: 0px;
				margin: 0;
			}

			table.recentcommentsavatartop a, table.recentcommentsavatarend a {
				border: 0px !important;
				background-color: transparent !important;
			}

			td.recentcommentsavatarend, td.recentcommentsavatartop {
				padding: 0px 0px 1px 0px;
				margin: 0px;
			}

			td.recentcommentstextend {
				border: none !important;
				padding: 0px 0px 2px 10px;
			}

			.rtl td.recentcommentstextend {
				padding: 0px 10px 2px 0px;
			}

			td.recentcommentstexttop {
				border: none !important;
				padding: 0px 0px 0px 10px;
			}

			.rtl td.recentcommentstexttop {
				padding: 0px 10px 0px 0px;
			}
		</style>
		<meta name="application-name" content="Bits of DNA" /><meta name="msapplication-window" content="width=device-width;height=device-height" /><meta name="msapplication-tooltip" content="Reviews and commentary on computational biology" /><meta name="msapplication-task" content="name=Subscribe;action-uri=http://liorpachter.wordpress.com/feed/;icon-uri=http://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=http://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=http://s2.wp.com/i/favicon.ico" /><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=http://s2.wp.com/i/favicon.ico" /><style type="text/css" id="syntaxhighlighteranchor"></style>
		<link rel="stylesheet" id="custom-css-css" type="text/css" href="http://s0.wp.com/?custom-css=1&#038;csblog=Rx4V&#038;cscache=6&#038;csrev=4" />
		</head>

<body class="home blog center  mp6 typekit-enabled highlander-enabled highlander-light infinite-scroll"><div id="wrapper">

<div id="header">

		<div id="header-image">
		<img alt="" src="http://liorpachter.files.wordpress.com/2013/08/cropped-figures.jpg" />	</div>
	
	<div id="title">
		<h1 id="blog-title">Bits of DNA</h1>		<p id="tagline">Reviews and commentary on computational biology</p>	</div>

	<div id="navigation">
		<ul id="nav-1">
	<li class='current_page_item'><a title="Return to front page" href="http://liorpachter.wordpress.com/">Home</a></li>
	<li class="page_item page-item-91 page_item_has_children"><a href="http://liorpachter.wordpress.com/seq/">*Seq</a></li>
<li class="page_item page-item-63"><a href="http://liorpachter.wordpress.com/about/">About</a></li>
</ul>

		<ul id="nav-2">
			<li><a class="feed" title="Subscribe to the Bits of DNA feed" href="http://liorpachter.wordpress.com/feed/">Subscribe to feed</a></li>
		</ul>
	</div>

</div>

<div id="content">

<div id="primary">


	<div class="post-3101 post type-post status-publish format-standard hentry category-guest-post category-sophistry tag-a-troublesome-inheritance tag-genetics tag-griqua tag-nicholas-wade tag-racism tag-the-british-empire entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-3101"><a href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/" rel="bookmark">Nicholas Wade&#8217;s troublesome&nbsp;inheritance</a></h2>
			<p class="post-metadata">June 4, 2014 in <a href="http://liorpachter.wordpress.com/category/guest-post/" rel="category tag">guest post</a>, <a href="http://liorpachter.wordpress.com/category/sophistry/" rel="category tag">sophistry</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/a-troublesome-inheritance/" rel="tag">A Troublesome Inheritance</a>, <a href="http://liorpachter.wordpress.com/tag/genetics/" rel="tag">genetics</a>, <a href="http://liorpachter.wordpress.com/tag/griqua/" rel="tag">Griqua</a>, <a href="http://liorpachter.wordpress.com/tag/nicholas-wade/" rel="tag">Nicholas Wade</a>, <a href="http://liorpachter.wordpress.com/tag/racism/" rel="tag">racism</a>, <a href="http://liorpachter.wordpress.com/tag/the-british-empire/" rel="tag">The British empire</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/#comments" title="Comment on Nicholas Wade&#8217;s troublesome&nbsp;inheritance">10 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>There has recently been something of an uproar over the new book <em>A Troublesome Inheritance</em> by Nicholas Wade, with much of the criticism centering on Wade&#8217;s claim that race is a meaningful biological category. This subject is one with which I<sup><a href="#footnotes">1</a></sup> have some personal connection since as a child growing up in South Africa in the 1980s, I was myself categorized very neatly by the <a href="http://en.wikipedia.org/wiki/Population_Registration_Act,_1950" target="_blank">Office for Race Classification</a>: 10. A simple pair of digits that conferred on me numerous rights and privileges denied to the majority of the population.</p>
<p><a href="https://liorpachter.files.wordpress.com/2014/06/apartheidpopulationgroups.jpg"><img class="aligncenter size-full wp-image-3105" src="http://liorpachter.files.wordpress.com/2014/06/apartheidpopulationgroups.jpg?w=490" alt="ApartheidPopulationGroups"   /></a></p>
<p style="text-align:center;"> Explanation of identity numbers assigned to citizens by the South African government during apartheid.</p>
<p>And yet the system behind those digits was anything but simple. The group to which an individual was assigned could be based on not only their skin color but also their employment, eating and drinking habits, and indeed explicitly social factors as <a href="http://archive-za.com/page/2517436/2013-07-28/http://heritage.thetimes.co.za/memorials/wc/RaceClassificationBoard/article.aspx?id=591128" target="_blank">related</a> by Muriel Horrell of the South African Institute of Race Relations: &#8220;Should a man who is initially classified white have a number of coloured friends and spend many of his leisure hours in their company, he stands to risk being re-classified as coloured.&#8221;</p>
<p>With these memories in mind, I found Wade&#8217;s concept of race as a biological category quite confusing, a confusion which only deepened when I discovered that he identifies not the eight races designated by the South African <a href="http://en.wikipedia.org/wiki/Population_Registration_Act,_1950" target="_blank">Population Registration Act</a> of 1950, but rather five, none of which was the <a href="http://en.wikipedia.org/wiki/Griqua_people" target="_blank">Griqua!</a> With the full force of modern science on his side<sup><a href="#footnotes">2</a></sup>, it seemed unlikely that these disparities represented an error on Wade&#8217;s part. And so I was left with a perplexing question: how could it be that the South African apartheid regime &#8212; racists par excellence &#8212; had failed to institutionalize their racism correctly? How had Wade gotten it right when <a href="http://en.wikipedia.org/wiki/Hendrik_Verwoerd" target="_blank">Hendrik Verwoerd</a> had gone awry?</p>
<p>Eventually I realized that <em>A Troublesome Inheritance</em> itself might contain the answer to this conundrum. Institutions, Wade explains, are genetic: &#8220;they grow out of instinctual social behaviors&#8221; and &#8220;one indication of such a genetic effect is that, if institutions were purely cultural, it should be easy to transfer an institution from one society to another.&#8221;<sup><a href="#footnotes">3</a></sup> So perhaps it is Wade&#8217;s genetic instincts as a Briton that explain how he has navigated these waters more skillfully than the Dutch-descended Afrikaners who designed the institutions of apartheid.</p>
<p>One might initially be inclined to scoff at such a suggestion or even to find it offensive. However, we must press boldly on in the name of truth and try to explain why this hypothesis might be true. Again, <em>A Troublesome Inheritance</em> comes to our aid. There, Wade discusses the decline in English interest rates between 1400 and 1850. This is the result, we learn, of rich English people producing more children than the poor and thereby genetically propagating those qualities which the rich are so famous for possessing: &#8220;less impulsive, more patient, and more willing to save.&#8221;<sup><a href="#footnotes">4</a></sup> However this period of time saw not only falling interest rates but also the rise of the British Empire. It was a period when Englishmen not only built steam engines and textile mills, but also trafficked in slaves by the millions and colonized countries whose people lacked their imperial genes. These latter activities, with an obvious appeal to the more racially minded among England&#8217;s population, could bring great wealth to those who engaged in them and so perhaps the greater reproductive fitness of England&#8217;s economic elite propagated not only patience but a predisposition to racism. This would explain, for example, the ability of <a href="http://en.wikipedia.org/wiki/John_Hanning_Speke" target="_blank">John Hanning Speke</a> to sniff out &#8220;the best blood of Abyssinia&#8221; when distinguishing the Tutsi from their Hutu neighbors.</p>
<p>Some might be tempted to speculate that Wade is himself a racist. While Wade &#8212; who freely speculates about billions of human beings &#8212; would no doubt support such an activity, those who engage in such speculation should perhaps not judge him too harshly. After all, racism may simply be Wade&#8217;s own troublesome inheritance.</p>
<h4 id="footnotes">Footnotes</h4>
<p>1.  In the spirit of authorship designation as discussed in <a title="Time to end ordered authorship" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/" target="_blank">this post</a>, we describe the author contributions as follows: the recollections of South Africa are those of <a href="http://math.berkeley.edu/~lpachter/" target="_blank">Lior Pachter</a>, who distinctly remembers his classification as &#8220;white&#8221;. <a href="http://math.berkeley.edu/~nbray/" target="_blank">Nicolas Bray</a> conceived and composed the post with input from LP. LP discloses no conflicts of interest. NB discloses being of British ancestry.<br />
2. Perhaps not quite the full <a href="http://www.nybooks.com/articles/archives/2014/jun/05/stretch-genes/" target="_blank">force</a>, given <a href="http://www.huffingtonpost.com/agustin-fuentes/the-troublesome-ignorance-of-nicholas-wade_b_5344248.html" target="_blank">the</a> <a href="http://paintmychromosomes.blogspot.jp/2014/06/what-did-we-learn-from-rosenberg-et-al.html" target="_blank">reception</a> <a href="https://whyevolutionistrue.wordpress.com/2014/05/14/new-book-on-race-by-nicholas-wade-professor-ceiling-cat-says-paws-down/" target="_blank">his</a> book <a href="http://www.huffingtonpost.com/american-anthropological-association/review-of-a-troublesome-i_b_5316217.html">has</a> received <a href="http://www.slate.com/articles/health_and_science/science/2014/05/troublesome_inheritance_critique_nicholas_wade_s_dated_assumptions_about.html" target="_blank">from</a> actual <a href="http://www.huffingtonpost.com/jennifer-raff/nicholas-wade-and-race-building-a-scientific-facade_b_5375137.html" target="_blank">scientists</a>.<br />
3. While this post is satirical, it should be noted for clarity that, improbably, this is an actual quote from Wade&#8217;s book.<br />
4. Again, not satire.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-3101"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-3101"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-3101"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/?share=facebook" title="Share on Facebook" id="sharing-facebook-3101"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-3101-53b3e857306d2' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=3101&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-3101-53b3e857306d2' data-name='like-post-frame-12758541-3101-53b3e857306d2'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2640 post type-post status-publish format-standard hentry category-education category-expository tag-affine-space tag-factor-analysis tag-jeopardy tag-least-squares tag-low-rank-approximation tag-maximum-likelihood tag-mds tag-multi-dimensional-scaling tag-pca tag-ppca tag-principal-component-analysis tag-probabilistic-pca tag-pythagoras-theorem tag-regression tag-schoenbergs-theorem tag-singular-value-decomposition entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2640"><a href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/" rel="bookmark">What is principal component&nbsp;analysis?</a></h2>
			<p class="post-metadata">May 26, 2014 in <a href="http://liorpachter.wordpress.com/category/education/" rel="category tag">education</a>, <a href="http://liorpachter.wordpress.com/category/expository/" rel="category tag">expository</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/affine-space/" rel="tag">affine space</a>, <a href="http://liorpachter.wordpress.com/tag/factor-analysis/" rel="tag">factor analysis</a>, <a href="http://liorpachter.wordpress.com/tag/jeopardy/" rel="tag">Jeopardy!</a>, <a href="http://liorpachter.wordpress.com/tag/least-squares/" rel="tag">least squares</a>, <a href="http://liorpachter.wordpress.com/tag/low-rank-approximation/" rel="tag">low rank approximation</a>, <a href="http://liorpachter.wordpress.com/tag/maximum-likelihood/" rel="tag">maximum likelihood</a>, <a href="http://liorpachter.wordpress.com/tag/mds/" rel="tag">MDS</a>, <a href="http://liorpachter.wordpress.com/tag/multi-dimensional-scaling/" rel="tag">multi-dimensional scaling</a>, <a href="http://liorpachter.wordpress.com/tag/pca/" rel="tag">PCA</a>, <a href="http://liorpachter.wordpress.com/tag/ppca/" rel="tag">pPCA</a>, <a href="http://liorpachter.wordpress.com/tag/principal-component-analysis/" rel="tag">principal component analysis</a>, <a href="http://liorpachter.wordpress.com/tag/probabilistic-pca/" rel="tag">probabilistic PCA</a>, <a href="http://liorpachter.wordpress.com/tag/pythagoras-theorem/" rel="tag">Pythagoras' theorem</a>, <a href="http://liorpachter.wordpress.com/tag/regression/" rel="tag">regression</a>, <a href="http://liorpachter.wordpress.com/tag/schoenbergs-theorem/" rel="tag">Schoenberg's theorem</a>, <a href="http://liorpachter.wordpress.com/tag/singular-value-decomposition/" rel="tag">singular value decomposition</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/#comments" title="Comment on What is principal component&nbsp;analysis?">14 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>In the <strong><em>Jeopardy!</em></strong> game show contestants are presented with questions formulated as answers that require answers in the form questions. For example, if a contestant selects &#8220;Normality for $200&#8243; she might be shown the following clue:</p>
<p style="text-align:center;">&#8220;The average <img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7Bx_1%2Bx_2%2B%5Ccdots+%2B+x_n%7D%7Bn%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{x_1+x_2+&#92;cdots + x_n}{n}' title='&#92;frac{x_1+x_2+&#92;cdots + x_n}{n}' class='latex' />,&#8221;</p>
<p>to which she would reply &#8220;What is the maximum likelihood estimate for the mean of <em>n </em>independent identically distributed Gaussian random variables from which samples <img src='http://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_1,x_2,&#92;ldots,x_n' title='x_1,x_2,&#92;ldots,x_n' class='latex' /> have been obtained?&#8221; Host Alex Trebek would immediately exclaim &#8220;That is the correct answer for $200!&#8221;</p>
<p>The process of doing mathematics involves repeatedly playing <em><strong>Jeopardy!</strong></em> with oneself in an unending quest to understand everything just a little bit better. The purpose of this blog post is to provide an exposition of how this works for understanding principal component analysis (PCA): I present four Jeopardy clues in the &#8220;Normality&#8221; category that all share the same answer: &#8220;What is principal component analysis?&#8221; The post was motivated by a conversation I recently had with a well-known population geneticist at a conference I was attending. I mentioned to him that I would be saying something about PCA in my talk, and that he might find what I have to say interesting because I knew he had used the method in many of his papers. Without hesitation he replied that he was well aware that PCA was not a statistical method and merely a heuristic visualization tool.</p>
<p>The problem, of course, is that PCA does have a statistical interpretation and is not at all an ad-hoc heuristic. Unfortunately, the previously mentioned population geneticist is not alone; there is a lot of confusion about what PCA is really about. For example, in <a href="http://www.amazon.com/Remington-Essentials-Pharmaceutics-Linda-Felton/dp/0857111051/ref=sr_1_1?ie=UTF8&amp;qid=1399746875&amp;sr=8-1&amp;keywords=remington+essentials+of+pharmaceutics" target="_blank">one textbook</a> it is stated that &#8220;<strong>PCA is not a statistical method</strong> to infer parameters or test hypotheses. Instead, it provides a method to reduce a complex dataset to lower dimension to reveal sometimes hidden, simplified structure that often underlie it.&#8221; <a href="http://books.google.com/books?id=P7jVziUfN7YC&amp;pg=PA158&amp;lpg=PA158&amp;dq=%22pca+is+a+statistical+method%22&amp;source=bl&amp;ots=oBwD61Y1jl&amp;sig=CFEwKtVgnDp3GxS_KxJCwn6piaM&amp;hl=en&amp;sa=X&amp;ei=ZnFuU_WHLtbtoASz0IDgBg&amp;ved=0CG4Q6AEwCQ#v=onepage&amp;q=%22pca%20is%20a%20statistical%20method%22&amp;f=false" target="_blank">In another</a> one finds out that &#8220;<strong>PCA is a statistical method</strong> routinely used to analyze interrelationships among large numbers of objects.&#8221; In a <a href="http://www.nature.com/nrd/journal/v1/n12/abs/nrd961.html" target="_blank">highly cited review on gene expression analysis</a> PCA is described as &#8220;<strong>more useful as a visualization technique than as an analytical method</strong>&#8221; but then in a paper by Markus Ringnér  titled the same as this post, i.e. <a href="http://www.nature.com/nbt/journal/v26/n3/abs/nbt0308-303.html" target="_blank">What is principal component analysis?</a> in <em>Nature Biotechnology</em>, 2008, the author writes that &#8220;<strong>Principal component analysis (PCA) is a</strong> <b>mathematical algorithm</b> that reduces the dimensionality of the data while retaining most of the variation in the data set&#8221; (the author then avoids going into the details because &#8220;understanding the details underlying PCA requires knowledge of linear algebra&#8221;). All of these statements are both correct and incorrect and confusing. A major issue is that the description by Ringnér of PCA in terms of the procedure for computing it (singular value decomposition) is common and unfortunately does not shed light on when it should be used. But knowing <em>when</em> to use a method is far more important than knowing <em>how </em>to do it.</p>
<p>I therefore offer four <em><strong>Jeopardy</strong>!</em> clues for principal component analysis that I think help to understand both when and how to use the method:</p>
<p><strong>1. <strong>An affine subspace closest to a set of points.</strong></strong></p>
<p>Suppose we are given <em>n </em>numbers <img src='http://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_1,&#92;ldots,x_n' title='x_1,&#92;ldots,x_n' class='latex' /> as in the initial example above. We are interested in finding the &#8220;closest&#8221; number to these numbers. By &#8220;closest&#8221; we mean in the sense of total squared difference. That is, we are looking for a number <img src='http://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m' title='m' class='latex' /> such that <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%28m-x_i%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sum_{i=1}^n (m-x_i)^2' title='&#92;sum_{i=1}^n (m-x_i)^2' class='latex' /> is minimized.</p>
<p>This is a (straightforward) calculus problem, solved by taking the derivative of the function above and setting it equal to zero. If we let <img src='http://s0.wp.com/latex.php?latex=f%28m%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%28m-x_i%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='f(m) = &#92;sum_{i=1}^n (m-x_i)^2' title='f(m) = &#92;sum_{i=1}^n (m-x_i)^2' class='latex' /> then <img src='http://s0.wp.com/latex.php?latex=f%27%28m%29+%3D+2+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+%28m-x_i%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='f&#039;(m) = 2 &#92;cdot &#92;sum_{i=1}^n (m-x_i)' title='f&#039;(m) = 2 &#92;cdot &#92;sum_{i=1}^n (m-x_i)' class='latex' /> and setting <img src='http://s0.wp.com/latex.php?latex=f%27%28m%29%3D0&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='f&#039;(m)=0' title='f&#039;(m)=0' class='latex' /> we can solve for <img src='http://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m' title='m' class='latex' /> to obtain <img src='http://s0.wp.com/latex.php?latex=m+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En+x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m = &#92;frac{1}{n} &#92;sum_{i=1}^n x_i' title='m = &#92;frac{1}{n} &#92;sum_{i=1}^n x_i' class='latex' />.</p>
<p>The right hand side of the equation is just the average of the <em>n</em> numbers and the optimization problem provides an interpretation of it as the number minimizing the total squared difference with the given numbers (note that one can replace squared difference by absolute value, i.e. minimization of <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%7Cm-x_i%7C&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sum_{i=1}^n |m-x_i|' title='&#92;sum_{i=1}^n |m-x_i|' class='latex' />, in which case the solution for <em>m </em>is the median; we return to this point and its implications for PCA later).</p>
<p>Suppose that instead of <em>n</em> numbers, one is given <em>n points</em> in <img src='http://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ep&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mathbb{R}^p' title='&#92;mathbb{R}^p' class='latex' />. That is, point <em>i </em>is <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+x%7D%5Ei+%3D+%28x%5Ei_1%2C%5Cldots%2Cx%5Ei_p%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf x}^i = (x^i_1,&#92;ldots,x^i_p)' title='{&#92;bf x}^i = (x^i_1,&#92;ldots,x^i_p)' class='latex' />. We can now ask for a point <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf m}' title='{&#92;bf m}' class='latex' /> with the property that the squared distance of <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf m}' title='{&#92;bf m}' class='latex' /> to the <em>n</em> points is minimized. This is asking for <img src='http://s0.wp.com/latex.php?latex=min_%7B%5Cbf+m%7D+%5Csum_%7Bi%3D1%7D%5En+%7C%7C%7B%5Cbf+m%7D-%7B%5Cbf+x%7D%5Ei%7C%7C_2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='min_{&#92;bf m} &#92;sum_{i=1}^n ||{&#92;bf m}-{&#92;bf x}^i||_2' title='min_{&#92;bf m} &#92;sum_{i=1}^n ||{&#92;bf m}-{&#92;bf x}^i||_2' class='latex' />.</p>
<p>The solution for <img src='http://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m' title='m' class='latex' /> can be obtained by minimizing each coordinate independently, thereby reducing the problem to the simpler version of numbers above, and it follows that <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+m%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En+%7B%5Cbf+x%7D%5Ei&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf m} = &#92;frac{1}{n} &#92;sum_{i=1}^n {&#92;bf x}^i' title='{&#92;bf m} = &#92;frac{1}{n} &#92;sum_{i=1}^n {&#92;bf x}^i' class='latex' />.</p>
<p>This is <i>0-dimensional PCA, </i>i.e., PCA of a set of points onto a single point, and it is the centroid of the points. The generalization of this concept provides a definition for PCA:</p>
<p><strong>Definition:</strong> Given <em>n</em> points in <img src='http://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ep&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mathbb{R}^p' title='&#92;mathbb{R}^p' class='latex' />, principal components analysis consists of choosing a dimension <img src='http://s0.wp.com/latex.php?latex=k+%3C+p&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='k &lt; p' title='k &lt; p' class='latex' /> and then finding the affine space of dimension <em>k </em>with the property that the squared distance of the points to their orthogonal projection onto the space is minimized.</p>
<p>This definition can be thought of as a generalization of the centroid (or average) of the points. To understand this generalization, it is useful to think of the simplest case that is not 0-dimensional PCA, namely 1-dimensional PCA of a set of points in two dimensions:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg"><img class="aligncenter size-full wp-image-2897" src="http://liorpachter.files.wordpress.com/2014/05/pca_figure1.jpg?w=490&#038;h=490" alt="PCA_Figure1" width="490" height="490" /></a>In this case the 1-dimensional PCA subspace can be thought of as the <em>line</em> that best represents the average of the points. The blue points are the orthogonal projections of the points onto the &#8220;average line&#8221; (see, e.g., the red point projected orthogonally), which minimizes the squared lengths of the dashed lines. In higher dimensions line is replaced by affine subspace, and the orthogonal projections are to points on that subspace. There are a few properties of the PCA affine subspaces that are worth noting:</p>
<ol>
<li>The set of PCA subspaces (translated to the origin) form a <em><a href="http://en.wikipedia.org/wiki/Flag_(linear_algebra)" target="_blank">flag</a>. </em>This means that the PCA subspace of dimension <em>k</em> is contained in the PCA subspace of dimension <em>k+1. </em>For example, all PCA subspaces contain the centroid of the points (in the figure above the centroid is the green point). This follows from the fact that the PCA subspaces can be incrementally constructed by building a basis from eigenvectors of a single matrix, a point we will return to later.</li>
<li>The PCA subspaces are not scale invariant. For example, if the points are scaled by multiplying one of the coordinates by a constant, then the PCA subspaces change. This is obvious because the centroid of the points will change. For this reason, when PCA is applied to data obtained from heterogeneous measurements, the units matter. One can form a &#8220;common&#8221; set of units by scaling the values in each coordinate to have the same variance.</li>
<li>If the data points are represented in matrix form as an <img src='http://s0.wp.com/latex.php?latex=n+%5Ctimes+p&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='n &#92;times p' title='n &#92;times p' class='latex' /> matrix <img src='http://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='X' title='X' class='latex' />, and the points orthogonally projected onto the PCA subspace of dimension <em>k </em>are represented as in the ambient <em>p</em> dimensional space by a matrix <img src='http://s0.wp.com/latex.php?latex=%5Ctilde%7BX%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;tilde{X}' title='&#92;tilde{X}' class='latex' />, then <img src='http://s0.wp.com/latex.php?latex=%5Ctilde%7BX%7D+%3D+argmin_%7BM%3Ark%28M%29%3Dk%7D+%7C%7CX-M%7C%7C_2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;tilde{X} = argmin_{M:rk(M)=k} ||X-M||_2' title='&#92;tilde{X} = argmin_{M:rk(M)=k} ||X-M||_2' class='latex' />. That is, <img src='http://s0.wp.com/latex.php?latex=%5Ctilde%7BX%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;tilde{X}' title='&#92;tilde{X}' class='latex' /> is the matrix of rank <em>k</em> with the property that the Frobenius norm <img src='http://s0.wp.com/latex.php?latex=%7C%7CX-%5Ctilde%7BX%7D%7C%7C_2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='||X-&#92;tilde{X}||_2' title='||X-&#92;tilde{X}||_2' class='latex' /> is minimized. This is just a rephrasing in linear algebra of the definition of PCA given above.</li>
</ol>
<p>At this point it is useful to mention some terminology confusion associated with PCA. Unfortunately there is no standard for describing the various parts of an analysis. What I have called the &#8220;PCA subspaces&#8221; are also sometimes called &#8220;principal axes&#8221;. The orthogonal vectors forming the flag mentioned above are called &#8220;weight vectors&#8221;, or &#8220;loadings&#8221;. Sometimes they are called &#8220;principal components&#8221;, although that term is sometimes used to refer to points projected onto a principal axis. In this post I stick to &#8220;PCA subspaces&#8221; and &#8220;PCA points&#8221; to avoid confusion.</p>
<p>Returning to <strong><em>Jeopardy!</em></strong>, we have &#8220;Normality for $400&#8243; with the answer &#8220;An affine subspace closest to a set of points&#8221; and the question &#8220;What is PCA?&#8221;. One question at this point is why the <strong><em>Jeopardy!</em></strong><em> </em>question just asked is in the category &#8220;Normality&#8221;. After all, the normal distribution does not seem to be related to the optimization problem just discussed. The connection is as follows:</p>
<p><strong>2. A generalization of linear regression in which the Gaussian noise is isotropic.</strong></p>
<p>PCA has an interpretation as the maximum likelihood parameter of a linear Gaussian model, a point that is crucial in understanding the scope of its application. To explain this point of view, we begin by elaborating on the opening<strong> <em>Jeopardy!</em> </strong>question about Normality for $200:</p>
<p>The point of the question was that the average of <em>n </em>numbers can be interpreted as a maximum likelihood estimation of the mean of a Gaussian. The Gaussian distribution is</p>
<p><img src='http://s0.wp.com/latex.php?latex=f%28x%2C%5Cmu%2C%5Csigma%29+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi+%5Csigma%5E2%7D%7D+e%5E%7B-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='f(x,&#92;mu,&#92;sigma) = &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;sigma^2}} e^{-&#92;frac{(x-&#92;mu)^2}{2&#92;sigma^2}}' title='f(x,&#92;mu,&#92;sigma) = &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;sigma^2}} e^{-&#92;frac{(x-&#92;mu)^2}{2&#92;sigma^2}}' class='latex' />. Given the numbers <img src='http://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_1,&#92;ldots,x_n' title='x_1,&#92;ldots,x_n' class='latex' />, the likelihood function is therefore</p>
<p><img src='http://s0.wp.com/latex.php?latex=L%28%5Cmu%2C%5Csigma%29+%3D+%5Cprod_%7Bi%3D1%7D%5En+%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi+%5Csigma%5E2%7D%7D+e%5E%7B-%5Cfrac%7B%28x_i-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='L(&#92;mu,&#92;sigma) = &#92;prod_{i=1}^n &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;sigma^2}} e^{-&#92;frac{(x_i-&#92;mu)^2}{2&#92;sigma^2}}' title='L(&#92;mu,&#92;sigma) = &#92;prod_{i=1}^n &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;sigma^2}} e^{-&#92;frac{(x_i-&#92;mu)^2}{2&#92;sigma^2}}' class='latex' />. The maximum of this function is the same as the maximum of its logarithm, which is</p>
<p><img src='http://s0.wp.com/latex.php?latex=log+L%28%5Cmu%2C%5Csigma%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28+log+%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi+%5Csigma%5E2%7D%7D+-%5Cfrac%7B%28x_i-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D+%5Cright%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='log L(&#92;mu,&#92;sigma) = &#92;sum_{i=1}^n &#92;left( log &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;sigma^2}} -&#92;frac{(x_i-&#92;mu)^2}{2&#92;sigma^2} &#92;right)' title='log L(&#92;mu,&#92;sigma) = &#92;sum_{i=1}^n &#92;left( log &#92;frac{1}{&#92;sqrt{2 &#92;pi &#92;sigma^2}} -&#92;frac{(x_i-&#92;mu)^2}{2&#92;sigma^2} &#92;right)' class='latex' />. Therefore the problem of finding the maximum likelihood estimate for the mean is equivalent to that of finding the <em>minimum</em> of the function</p>
<p><img src='http://s0.wp.com/latex.php?latex=S%28%5Cmu%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%28x_i-%5Cmu%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='S(&#92;mu) = &#92;sum_{i=1}^n (x_i-&#92;mu)^2' title='S(&#92;mu) = &#92;sum_{i=1}^n (x_i-&#92;mu)^2' class='latex' />. This is exactly the optimization problem solved by 0-dimensional PCA, as we saw above. With this calculation at hand, we turn to the statistical interpretation of least squares:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_figure2.jpg"><img class="aligncenter size-full wp-image-2896" src="http://liorpachter.files.wordpress.com/2014/05/pca_figure2.jpg?w=490&#038;h=490" alt="PCA_Figure2" width="490" height="490" /></a></p>
<p>Given <em>n</em> points <img src='http://s0.wp.com/latex.php?latex=%5C%7B%28x_i%2Cy_i%29%5C%7D_%7Bi%3D1%7D%5En&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;{(x_i,y_i)&#92;}_{i=1}^n' title='&#92;{(x_i,y_i)&#92;}_{i=1}^n' class='latex' /> in the plane (see figure above),  the least squares line <img src='http://s0.wp.com/latex.php?latex=y%3Dmx%2Bb&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='y=mx+b' title='y=mx+b' class='latex' /> (purple in figure) is the one that minimizes the sum of the squares <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28+%28mx_i%2Bb%29+-+y_i+%5Cright%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sum_{i=1}^n &#92;left( (mx_i+b) - y_i &#92;right)^2' title='&#92;sum_{i=1}^n &#92;left( (mx_i+b) - y_i &#92;right)^2' class='latex' />. That is, the least squares line is the one minimizing the sum of the squared vertical distances to the points. As with the average of numbers, the least squares line has a statistical interpretation: Suppose that there is some line <img src='http://s0.wp.com/latex.php?latex=y%3Dm%5E%7B%2A%7Dx%2Bb%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='y=m^{*}x+b^{*}' title='y=m^{*}x+b^{*}' class='latex' /> (black line in figure) that is unknown, but that &#8220;generated&#8221; the observed points, in the sense that each observed point <em>i </em>was obtained by perturbing the point <img src='http://s0.wp.com/latex.php?latex=m%5E%7B%2A%7Dx_i+%2Bb%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m^{*}x_i +b^{*}' title='m^{*}x_i +b^{*}' class='latex' /> vertically by a random amount from a single Gaussian distribution with mean 0 and variance <img src='http://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sigma^2' title='&#92;sigma^2' class='latex' />. In the figure, an example is shown where the blue point on the unknown line &#8220;generates&#8221; the observed red point; the Gaussian is indicated with the blue streak around the point. Note that the model specified so far is not fully generative, as it depends on the hidden points <img src='http://s0.wp.com/latex.php?latex=m%5E%7B%2A%7Dx_i+%2Bb%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m^{*}x_i +b^{*}' title='m^{*}x_i +b^{*}' class='latex' /> and there is no procedure given to generate the <img src='http://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i' title='x_i' class='latex' />. This can be done by positing that the <img src='http://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i' title='x_i' class='latex' /> are generated from a Gaussian distribution along the line <img src='http://s0.wp.com/latex.php?latex=y%3Dm%5E%7B%2A%7Dx%2Bb&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='y=m^{*}x+b' title='y=m^{*}x+b' class='latex' /> (followed by the points <img src='http://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='y_i' title='y_i' class='latex' /> generated by Gaussian perturbation of the <em>y</em> coordinate on the line). The coordinates <img src='http://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i' title='x_i' class='latex' /> can then be deduced directly from the observed points as the Gaussian perturbations are all vertical. The relationship between the statistical model just described and least squares is made precise by a theorem (which we state informally, but is a special case of the Gauss-Markov theorem):</p>
<p><a href="http://en.wikipedia.org/wiki/Gauss–Markov_theorem" target="_blank"><em>Theorem </em>(Gauss-Markov)</a>: The maximum likelihood estimate for the line (the parameters <em>m </em>and <em>b</em>) in the model described above correspond to the least squares line.</p>
<p>The proof is analogous to the argument given for the average of numbers above so we omit it. It can be generalized to higher dimensions where it forms the basis of what is known as <a href="http://en.wikipedia.org/wiki/Regression_analysis" target="_blank">linear regression</a>. In regression, the <img src='http://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i' title='x_i' class='latex' /> are known as independent variables and <img src='http://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='y' title='y' class='latex' /> the dependent variable. The generative model provides an interpretation of the independent variables as fixed measured quantities, whereas the dependent variable is a linear combination of the independent variables with added noise. It is important to note that the origins of linear regression are in physics, specifically in work of Legendre (1805) and Gauss (1809) who applied least squares to the astronomical problem of calculating the orbits of comets around the sun. In their application, the independent variables were time (for which accurate measurements were possible with clocks; <a href="http://universe-review.ca/I15-49-clocks.jpg" target="_blank">by 1800 clocks were accurate to less than 0.15 seconds per day</a>) and the (noisy) dependent variable the measurement of location. Linear regression has become one of the most (if not <em>the </em>most) widely used statistical tools but as we now explain, <strong>PCA (and its generalization factor analysis), with a statistical interpretation that includes noise in the <img src='http://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i' title='x_i' class='latex' /> variables, seems better suited for biological data.</strong></p>
<p>The statistical interpretation of least squares can be extended to a similar framework for PCA. Recall that we first considered a statistical interpretation for least squares where an unknown line <img src='http://s0.wp.com/latex.php?latex=y%3Dm%5E%7B%2A%7Dx%2Bb%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='y=m^{*}x+b^{*}' title='y=m^{*}x+b^{*}' class='latex' /> &#8220;generated&#8221; the observed points, in the sense that each observed point <em>i </em>was obtained by perturbing the point <img src='http://s0.wp.com/latex.php?latex=m%5E%7B%2A%7Dx_i+%2Bb%5E%7B%2A%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m^{*}x_i +b^{*}' title='m^{*}x_i +b^{*}' class='latex' /> <em>vertically</em> by a random amount from a single Gaussian distribution with mean 0 and variance <img src='http://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sigma^2' title='&#92;sigma^2' class='latex' />. PCA can be understood analogously by replacing <em>vertically</em> by <em>orthogonally </em>(this is the probabilistic model of <a href="http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf" target="_blank">Collins <em>et al.</em>, NIPS 2001 </a>for PCA). However this approach is not completely satisfactory as the orthogonality of the perturbation is is not readily interpretable. Stated differently, it is not obvious what physical processes would generate points orthogonal to a linear affine subspace by perturbations that are always orthogonal to the subspace. In the case of least squares, the &#8220;vertical&#8221; perturbation corresponds to noise in one measurement (represented by one coordinate). The problem is in naturally interpreting orthogonal perturbations in terms of a noise model for measurements. This difficulty is resolved by a model called <em>probabilistic PCA (pPCA), </em>first proposed by <a href="http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196/abstract" target="_blank">Tipping and Bishop in a Tech Report in 1997, and published in the J. of the Royal Statistical Society B 2002</a>,  and independently by <a href="http://www.cs.nyu.edu/~roweis/papers/empca.pdf" target="_blank">Sam Roweis, NIPS 1998</a>, that is illustrated visually in the figure below, and that we now explain:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_figure3.jpg"><img class="aligncenter size-full wp-image-2899" src="http://liorpachter.files.wordpress.com/2014/05/pca_figure3.jpg?w=490&#038;h=490" alt="PCA_Figure3" width="490" height="490" /></a></p>
<p>&nbsp;</p>
<p>In the pPCA model there is an (unknown) line (affine space in higher dimension) on which (hidden) points (blue) are generated at random according to a Gaussian distribution (represented by gray streak in the figure above, where the mean of the Gaussian is the green point). Observed points (red) are then generated from the hidden points by addition of isotropic Gaussian noise (blue smear), meaning that the Gaussian has a diagonal covariance matrix with equal entries. Formally, in the notation of Tipping and Bishop, this is a linear Gaussian model described as follows:</p>
<p>Observed random variables <em>t </em>are given by <img src='http://s0.wp.com/latex.php?latex=t+%3D+Wx+%2B+%5Cmu+%2B+%5Cepsilon&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='t = Wx + &#92;mu + &#92;epsilon' title='t = Wx + &#92;mu + &#92;epsilon' class='latex' /> where <em>x</em> are latent (hidden) random variables, <em>W</em> is a matrix describing a subspace and <img src='http://s0.wp.com/latex.php?latex=Wx%2B%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='Wx+&#92;mu' title='Wx+&#92;mu' class='latex' /> are the latent points on an affine subspace (<img src='http://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mu' title='&#92;mu' class='latex' /> corresponds to a translation). Finally, <img src='http://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;epsilon' title='&#92;epsilon' class='latex' /> is an error term, given by a Gaussian random variable with mean 0 and covariance matrix <img src='http://s0.wp.com/latex.php?latex=%5Csigma%5E2+I&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sigma^2 I' title='&#92;sigma^2 I' class='latex' />. The parameters of the model are <img src='http://s0.wp.com/latex.php?latex=W%2C%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='W,&#92;mu' title='W,&#92;mu' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sigma^2' title='&#92;sigma^2' class='latex' />. Equivalently, the observed random variables are themselves Gaussian, described by the distribution <img src='http://s0.wp.com/latex.php?latex=t+%5Csim+%5Cmathcal%7BN%7D%28%5Cmu%2CWW%5ET+%2B+%5Cpsi%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='t &#92;sim &#92;mathcal{N}(&#92;mu,WW^T + &#92;psi)' title='t &#92;sim &#92;mathcal{N}(&#92;mu,WW^T + &#92;psi)' class='latex' /> where <img src='http://s0.wp.com/latex.php?latex=%5Cpsi+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Csigma%5E2I%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;psi &#92;sim &#92;mathcal{N}(0,&#92;sigma^2I)' title='&#92;psi &#92;sim &#92;mathcal{N}(0,&#92;sigma^2I)' class='latex' />. Tipping and Bishop prove an analogy of the Gauss-Markov theorem, namely that the affine subspace given by the maximum likelihood estimates of <img src='http://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='W' title='W' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mu' title='&#92;mu' class='latex' /> is the PCA subspace (the proof is not difficult but I omit it and refer interested readers to their paper, or <a href="http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;qid=1400588836&amp;sr=8-1&amp;keywords=bishop+machine+learning" target="_blank">Bishop&#8217;s Pattern Recognition and Machine Learning book</a>).</p>
<p>It is important to note that although the maximum likelihood estimates of <img src='http://s0.wp.com/latex.php?latex=W%2C%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='W,&#92;mu' title='W,&#92;mu' class='latex' /> in the pPCA model correspond to the PCA subspace, only posterior distributions can be obtained for the latent data (points on the subspace). Neither the mode nor the mean of those distributions corresponds to the PCA points (orthogonal projections of the observations onto the subspace). However what is true, is that the posterior distributions converge to the PCA points as <img src='http://s0.wp.com/latex.php?latex=%5Csigma%5E2+%5Crightarrow+0&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sigma^2 &#92;rightarrow 0' title='&#92;sigma^2 &#92;rightarrow 0' class='latex' />. In other words, the relationship between pPCA and PCA is a bit more subtle than that between least squares and regression.</p>
<p>The relationship between regression and (p)PCA is shown in the figure below:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_figure4.jpg"><img class="aligncenter size-full wp-image-2898" src="http://liorpachter.files.wordpress.com/2014/05/pca_figure4.jpg?w=490&#038;h=490" alt="PCA_Figure4" width="490" height="490" /></a>In the figure, points have been generated randomly according to the pPCA model. the black smear shows the affine space on which the points were generated, with the smear indicating the Gaussian distribution used. Subsequently the latent points (light blue on the gray line) were used to make observed points (red) by the addition of isotropic Gaussian noise. The green line is the maximum likelihood estimate for the space, or equivalently by the theorem of Tipping and Bishop the PCA subspace. The projection of the observed points onto the PCA subspace (blue) are the PCA points. The purple line is the least squares line, or equivalently the affine space obtained by regression (<em>y</em> observed as a noisy function of <i>x</i>). The pink line is also a regression line, except where <em>x </em>is observed as a noisy function of <em>y.</em></p>
<p>A natural question to ask is why the probabilistic interpretation of PCA (pPCA) is useful or necessary? One reason it is beneficial is that maximum likelihood inference for pPCA involves hidden random variables, and therefore the EM algorithm immediately comes to mind as a solution (the strategy was suggested by both Tipping &amp; Bishop and Roweis). I have not yet discussed <em>how</em> to find the PCA subspace, and the EM algorithm provides an intuitive and direct way to see how it can be done, without the need for writing down any linear algebra:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_figure61.jpg"><img class="aligncenter size-full wp-image-2943" src="http://liorpachter.files.wordpress.com/2014/05/pca_figure61.jpg?w=490&#038;h=1892" alt="PCA_Figure6" width="490" height="1892" /></a></p>
<p>The exact version of the EM shown above is due to Roweis. In it, one begins with a random affine subspace passing through the centroid of the points. The &#8220;E&#8221; step (expectation) consists of projecting the points to the subspace. The projected points are considered fixed to the subspace. The &#8220;M&#8221; step (maximization) then consists of rotating the space so that the total squared distance of the fixed points on the subspace to the observed points is minimized. This is repeated until convergence. Roweis points out that this approach to finding the PCA subspace is equivalent to <a href="http://en.wikipedia.org/wiki/Power_iteration" target="_blank">power iteration</a> for (efficiently) finding eigenvalues of the the sample covariance matrix without computing it directly. This is our first use of the word eigenvalue in describing PCA, and we elaborate on it, and the linear algebra of computing PCA subspaces later in the post.</p>
<p>Another point of note is that pPCA can be viewed as a special case of factor analysis, and this connection provides an immediate starting point for thinking about generalizations of PCA. Specifically, factor analysis corresponds to the model <img src='http://s0.wp.com/latex.php?latex=t+%5Csim+%5Cmathcal%7BN%7D%28%5Cmu%2CWW%5ET+%2B+%5Cpsi%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='t &#92;sim &#92;mathcal{N}(&#92;mu,WW^T + &#92;psi)' title='t &#92;sim &#92;mathcal{N}(&#92;mu,WW^T + &#92;psi)' class='latex' /> where the covariance matrix <img src='http://s0.wp.com/latex.php?latex=%5Cpsi&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;psi' title='&#92;psi' class='latex' /> is less constrained, and only required to be diagonal. This is connected to a comment made above about when the PCA subspace might be more useful as a linear fit to data than regression. To reiterate, unlike physics, where some coordinate measurements have very little noise in comparison to others, biological measurements are frequently noisy in all coordinates. In such settings factor analysis is preferable, as the variance in each coordinate is estimated as part of the model. PCA is perhaps a good compromise, as PCA subspaces are easier to find than parameters for factor analysis, yet PCA, via its pPCA interpretation, accounts for noise in all coordinates.</p>
<p>A final comment about pPCA is that it provides a natural framework for thinking about hypothesis testing. The book <a href="http://www.amazon.com/Statistical-Methods-Geometric-Approach-Statistics/dp/0387975179" target="_blank">Statistical Methods: A Geometric Approach</a> by <span style="color:#545454;">Saville and Wood is essentially about (the geometry of) pPCA and its connection to hypothesis testing. The authors do not use the term pPCA but their starting point is exactly the linear Gaussian model of Tipping and Bishop. The idea is to consider single samples from <em>n </em>independent identically distributed independent Gaussian random variables as one single sample from a high-dimensional multivariate linear Gaussian model with isotropic noise. From that point of view pPCA provides an interpretation for <a title="Bessel’s correction and the dangers of MOOCs" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/" target="_blank">Bessel&#8217;s correction</a>. The details are interesting but tangential to our focus on PCA.</span></p>
<p>We are therefore ready to return to <strong><em>Jeopardy!</em></strong>, where we have &#8220;Normality for $600&#8243; with the answer &#8220;A generalization of linear regression in which the Gaussian noise is isotropic&#8221; and the question &#8220;What is PCA?&#8221;</p>
<p><strong>3. An orthogonal projection of points onto an affine space that maximizes the retained sample variance.</strong></p>
<p>In the previous two interpretations of PCA, the focus was on the PCA affine subspace. However in many uses of PCA the output of interest is the <em>projection of the given points onto the PCA affine space</em>. The projected points have three useful related interpretations:</p>
<ol>
<li>As seen in in section 1, the (orthogonally) projected points (red -&gt; blue) are those whose total squared distance to the observed points is minimized.</li>
<li>What we focus on in this section, is the interpretation that the PCA subspace is the one onto which the (orthogonally) projected points maximize the retained sample variance.</li>
<li>The topic of the next section, namely that the squared distances between the (orthogonally) projected points are on average (in the <img src='http://s0.wp.com/latex.php?latex=l_2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='l_2' title='l_2' class='latex' /> metric) closest to the original distances between the points.</li>
</ol>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_figure5.jpg"><img class="aligncenter size-medium wp-image-2901" src="http://liorpachter.files.wordpress.com/2014/05/pca_figure5.jpg?w=300&#038;h=225" alt="PCA_Figure5" width="300" height="225" /></a></p>
<p>&nbsp;</p>
<p>The <em>sample variance </em>of a set of points is the average squared distance from each point to the centroid. Mathematically, if the observed points are translated so that their centroid is at zero (known as zero-centering), and then represented by an <img src='http://s0.wp.com/latex.php?latex=n+%5Ctimes+p&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='n &#92;times p' title='n &#92;times p' class='latex' /> matrix <em>X</em>, then the sample covariance matrix is given by <img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn-1%7DX%5EtX&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{1}{n-1}X^tX' title='&#92;frac{1}{n-1}X^tX' class='latex' /> and the sample variance is given by the trace of the matrix. The point is that the <i>j</i>th diagonal entry of <img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn-1%7DX%5EtX&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{1}{n-1}X^tX' title='&#92;frac{1}{n-1}X^tX' class='latex' /> is just <img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi%3D1%7D%5En+%28x%5Ei_j%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{1}{n-1}&#92;sum_{i=1}^n (x^i_j)^2' title='&#92;frac{1}{n-1}&#92;sum_{i=1}^n (x^i_j)^2' class='latex' />, which is the <a title="Bessel’s correction and the dangers of MOOCs" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/">sample variance</a> of the <em>j</em>th variable. The PCA subspace can be viewed as that subspace with the property that <em>the sample variance of the projections of the observed points onto the subspace</em> is maximized. This is easy to see from the figure above. For each point (blue), Pythagoras&#8217; theorem implies that <img src='http://s0.wp.com/latex.php?latex=d%28red%2Cblue%29%5E2%2Bd%28blue%2Cgreen%29%5E2+%3D+d%28red%2Cgreen%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='d(red,blue)^2+d(blue,green)^2 = d(red,green)^2' title='d(red,blue)^2+d(blue,green)^2 = d(red,green)^2' class='latex' />. Since the PCA subspace is the one minimizing the total squared red-blue distances, and since the solid black lines (red-green distances) are fixed, it follows that the PCA subspace also maximizes the total squared green-blue distances. In other words, PCA maximizes the retained sample variance.</p>
<p>The explanation above is informal, and uses a 1-dimensional PCA subspace in dimension 2 to make the argument. However the argument extends easily to higher dimension, which is typically the setting where PCA is used. In fact, PCA is typically used to &#8220;visualize&#8221; high dimensional points by projection into dimensions two or three, precisely because of the interpretation provided above, namely that it retains the sample variance. I put visualize in quotes because<a href="http://en.wikipedia.org/wiki/Borsuk's_conjecture" target="_blank"> intuition in two or three dimensions does not always hold in high dimensions</a>. However PCA can be useful for visualization, and one of my favorite examples is the evidence for genes mirroring geography in humans. This was first alluded to by Cavalli-Sforza, but definitively shown by <a href="http://www.cell.com/current-biology/abstract/S0960-9822(08)00956-1" target="_blank">Lao <em>et al., </em>2008</a>, who analyzed 2541 individuals and showed that PCA of the SNP matrix (approximately) recapitulates geography:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/pca_lao.jpg"><img class="aligncenter size-full wp-image-2906" src="http://liorpachter.files.wordpress.com/2014/05/pca_lao.jpg?w=490&#038;h=319" alt="PCA_Lao" width="490" height="319" /></a></p>
<p style="text-align:center;">Genes mirror geography from Lao <em>et al. </em>2008: (Left) PCA of the SNP matrix (2541 individuals x 309,790 SNPs) showing a density map of projected points. (Right) Map of Europe showing locations of the populations .</p>
<p>In the picture above, it is useful to keep in mind that the emergence of geography is occurring in that projection in which the sample variance is maximized. As far as interpretation goes, it is useful to look back at Cavalli-Sforza&#8217;s work. He and collaborators who worked on the problem in the 1970s, were unable to obtain a dense SNP matrix due to limited technology of the time. Instead, in <a href="http://www.sciencemag.org/content/201/4358/786.short" target="_blank">Menozzi <em>et al., </em>1978</a> they performed PCA of an <em>allele-frequency </em>matrix, i.e. a matrix indexed by populations and allele frequencies instead of individuals and genotypes. Unfortunately they fell into the trap of misinterpreting the biological meaning of the eigenvectors in PCA. Specifically, they inferred migration patterns from contour plots in geographic space obtained by plotting the relative contributions from the eigenvectors, but the effects they observed turned out to be an <a href="http://www.nature.com/ng/journal/v40/n5/abs/ng.139.html" target="_blank">artifact of PCA</a>. However as we discussed above, PCA <em>can</em> be used quantitatively via the stochastic process for which it solves maximum likelihood inference. It just has to be properly understood.</p>
<p>To conclude this section in <strong><em>Jeopardy!</em></strong><em> </em>language, we have &#8220;Normality for $800&#8243; with the answer &#8220;A set of points in an affine space obtained via projection of a set of given points so that the sample variance of the projected points is maximized&#8221; and the question &#8220;What is PCA?&#8221;</p>
<p><strong>4. Principal component analysis of Euclidean distance matrices<strong>.</strong></strong></p>
<p>In the preceding interpretations of PCA, I have focused on what happens to individual points when projected to a lower dimensional subspace, but it is also interesting to consider what happens to <em>pairs</em> of points. One thing that is clear is that if a pair of points are projected orthogonally onto a low-dimensional affine subspace then the distance between the points in the projection is smaller than the original distance between the points. This is clear because of Pythagoras&#8217; theorem, which implies that the squared distance will shrink unless the points are parallel to the subspace in which case the distance remains the same. An interesting observation is that in fact the PCA subspace is the one with the property where the average (or total) squared distances between the points is maximized. To see this it again suffices to consider only projections onto one dimension (the general case follows by Pythagoras&#8217; theorem). The following <a title="Bessel’s correction and the dangers of MOOCs" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/">lemma, discussed in my previous blog post</a>, makes the connection to the previous discussion:</p>
<p><em><strong>Lemma:</strong></em><strong> </strong>Let <img src='http://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_1,&#92;ldots,x_n' title='x_1,&#92;ldots,x_n' class='latex' /> be numbers with mean <img src='http://s0.wp.com/latex.php?latex=%5Coverline%7Bx%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D%5Csum_i+x_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;overline{x} = &#92;frac{1}{n}&#92;sum_i x_i' title='&#92;overline{x} = &#92;frac{1}{n}&#92;sum_i x_i' class='latex' />. If the average squared distance between pairs of points is denoted <img src='http://s0.wp.com/latex.php?latex=D+%3D+%5Cfrac%7B1%7D%7Bn%5E2%7D%5Csum_%7Bi%2Cj%7D+%28x_i-x_j%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='D = &#92;frac{1}{n^2}&#92;sum_{i,j} (x_i-x_j)^2' title='D = &#92;frac{1}{n^2}&#92;sum_{i,j} (x_i-x_j)^2' class='latex' /> and the variance is denoted <img src='http://s0.wp.com/latex.php?latex=V%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_i+%28x_i-%5Coverline%7Bx%7D%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='V=&#92;frac{1}{n}&#92;sum_i (x_i-&#92;overline{x})^2' title='V=&#92;frac{1}{n}&#92;sum_i (x_i-&#92;overline{x})^2' class='latex' /> then <img src='http://s0.wp.com/latex.php?latex=V%3D%5Cfrac%7B1%7D%7B2%7DD&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='V=&#92;frac{1}{2}D' title='V=&#92;frac{1}{2}D' class='latex' />.</p>
<p>What the lemma says is that the sample variance is equal to the average squared difference between the numbers (i.e. it is a scalar multiple that does not depend on the numbers). I have already discussed that the PCA subspace maximizes the retained variance, and it therefore follows that it also maximizes the average (or total) projected squared distance between the points. Alternately, PCA can be interpreted as <em>minimizing </em>the total (squared) distance that is lost, i,e. if the original distances between the points are given by a distance matrix <img src='http://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='D' title='D' class='latex' /> and the projected distances are given by <img src='http://s0.wp.com/latex.php?latex=%5Ctilde%7BD%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;tilde{D}' title='&#92;tilde{D}' class='latex' />, then the PCA subspace minimizes <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bij%7D+%28D%5E2_%7Bij%7D+-+%5Ctilde%7BD%7D%5E2_%7Bij%7D%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sum_{ij} (D^2_{ij} - &#92;tilde{D}^2_{ij})' title='&#92;sum_{ij} (D^2_{ij} - &#92;tilde{D}^2_{ij})' class='latex' />, where each term in the sum is positive as discussed above.</p>
<p>This interpretation of PCA leads to an interesting application of the method to (Euclidean) <em>distance matrices</em> rather than <em>points. </em>The idea is based on a theorem of <a href="http://en.wikipedia.org/wiki/Isaac_Jacob_Schoenberg" target="_blank">Isaac Schoenberg</a> that characterizes Euclidean distance matrices and provides a method for realizing them. The theorem is well-known to structural biologists who work with NMR, because it is one of the foundations used to reconstruct coordinates of structures from distance measurements. It requires a bit of notation: <em>D </em>is a distance matrix with entries <img src='http://s0.wp.com/latex.php?latex=d_%7Bij%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='d_{ij}' title='d_{ij}' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;Delta' title='&#92;Delta' class='latex' /> is the matrix with entries <img src='http://s0.wp.com/latex.php?latex=-%5Cfrac%7B1%7D%7B2%7Dd%5E2_%7Bij%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='-&#92;frac{1}{2}d^2_{ij}' title='-&#92;frac{1}{2}d^2_{ij}' class='latex' />. <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+1%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf 1}' title='{&#92;bf 1}' class='latex' /> denotes the vector of all ones, and <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+s%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf s}' title='{&#92;bf s}' class='latex' /> denotes a vector.</p>
<p><em><strong>Theorem</strong></em> (Schoenberg, 1938): A matrix <em>D </em>is a Euclidean distance matrix if and only if the matrix <img src='http://s0.wp.com/latex.php?latex=B%3D%28I-%7B%5Cbf+1%7D%7B%5Cbf+s%7D%27%29%5CDelta%28I-%7B%5Cbf+s%7D%7B%5Cbf+1%7D%27%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='B=(I-{&#92;bf 1}{&#92;bf s}&#039;)&#92;Delta(I-{&#92;bf s}{&#92;bf 1}&#039;)' title='B=(I-{&#92;bf 1}{&#92;bf s}&#039;)&#92;Delta(I-{&#92;bf s}{&#92;bf 1}&#039;)' class='latex' /> is positive semi-definite where <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+s%7D%27%7B%5Cbf+1%7D+%3D+1&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf s}&#039;{&#92;bf 1} = 1' title='{&#92;bf s}&#039;{&#92;bf 1} = 1' class='latex' />.</p>
<p>For the case when <img src='http://s0.wp.com/latex.php?latex=%7B%5Cbf+s%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='{&#92;bf s}' title='{&#92;bf s}' class='latex' /> is chosen to be a unit vector, i.e. all entries are zero except one of them equal to 1, the matrix <em>B</em> can be viewed as the Gromov transform (known as the <a href="http://link.springer.com/article/10.1007%2Fs00026-007-0302-5#page-1" target="_blank">Farris transform</a> in phylogenetics) of the matrix with entries <img src='http://s0.wp.com/latex.php?latex=d%5E2_%7Bij%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='d^2_{ij}' title='d^2_{ij}' class='latex' />. Since the matrix <em>B </em>is positive semidefinite it can be written as <img src='http://s0.wp.com/latex.php?latex=B%3DXX%5Et&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='B=XX^t' title='B=XX^t' class='latex' />, where the matrix <em>X</em> provides coordinates for points that realize <em>D. </em>At this point PCA can be applied resulting in a principal subspace and points on it (the orthogonal projections of <em>X). </em>A point of note is that eigenvectors of <img src='http://s0.wp.com/latex.php?latex=XX%5Et&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='XX^t' title='XX^t' class='latex' /> can be computed directly, avoiding the need to compute <img src='http://s0.wp.com/latex.php?latex=X%5EtX&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='X^tX' title='X^tX' class='latex' /> which may be a larger matrix if <img src='http://s0.wp.com/latex.php?latex=n+%3C+p&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='n &lt; p' title='n &lt; p' class='latex' />.</p>
<p>The procedure just described is called classic multidimensional scaling (MDS) and it returns a set of points on a Euclidean subspace with distance matrix <img src='http://s0.wp.com/latex.php?latex=%5Ctilde%7BD%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;tilde{D}' title='&#92;tilde{D}' class='latex' /> that best represent the original distance matrix <em>D</em> in the sense that <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bij%7D+%28D%5E2_%7Bij%7D+-+%5Ctilde%7BD%7D%5E2_%7Bij%7D%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sum_{ij} (D^2_{ij} - &#92;tilde{D}^2_{ij})' title='&#92;sum_{ij} (D^2_{ij} - &#92;tilde{D}^2_{ij})' class='latex' /> is minimized. The term multidimensional scaling without the &#8220;classic&#8221; has taken on an expanded meaning, namely it encapsulates all methods that seek to approximately realize a distance matrix by points in a low dimensional Euclidean space. Such methods are generally not related to PCA, but classic multidimensional scaling <em>is </em>PCA. This is a general source of confusion and error on the internet. In fact, most articles and course notes I found online describing the connection between MDS and PCA are incorrect. In any case classic multidimensional scaling is a very useful instance of PCA, because it extends the utility of the method to cases where points are not available but distances between them are.</p>
<p>Now we return to <strong><em>Jeopardy!</em></strong><em> </em>one final time with the final question in the category: &#8220;Normality for $1000&#8243;. The answer is &#8220;Principal component analysis of Euclidean distance matrices&#8221; and the question is &#8220;What is classic multidimensional scaling?&#8221;</p>
<p><strong>An example</strong></p>
<p>To illustrate the interpretations of PCA I have highlighted, I&#8217;m including an example in <a href="http://www.r-project.org" target="_blank">R</a> inspired by an <a href="http://www.cerebralmastication.com/2010/09/principal-component-analysis-pca-vs-ordinary-least-squares-ols-a-visual-explination/" target="_blank">example from another blog post</a> (all commands can be directly pasted into an R console). I&#8217;m also providing the example because missing in the discussion above is a description of <em>how</em> to compute PCA subspaces and the projections of points onto them. I therefore explain some of this math in the course of working out the example:</p>
<p>First, I generate a set of points (in <img src='http://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mathbb{R}^2' title='&#92;mathbb{R}^2' class='latex' />). I&#8217;ve chosen a low dimension so that pictures can be drawn that are compatible with some of the examples above. Comments following commands appear after the # character.</p>
<pre> set.seed(2)             #sets the <a title="Random numbers, reproducibility and SAMtools" href="http://liorpachter.wordpress.com/2013/10/25/random-numbers-reproducibility-and-samtools/" target="_blank">seed for random number generation</a>.
 x &lt;- 1:100              #creates a vector x with numbers from 1 to 100
 ex &lt;- rnorm(100, 0, 30) #100 normally distributed rand. nos. w/ mean=0, s.d.=30
 ey &lt;- rnorm(100, 0, 30) # " " 
 y &lt;- 30 + 2 * x         #sets y to be a vector that is a linear function of x
 x_obs &lt;- x + ex         #adds "noise" to x
 y_obs &lt;- y + ey         #adds "noise" to y
 P &lt;- cbind(x_obs,y_obs) #places points in matrix
 plot(P,asp=1,col=1) #plot points
 points(mean(x_obs),mean(y_obs),col=3, pch=19) #show center</pre>
<p>At this point a full PCA analysis can be undertaken in R using the command &#8220;prcomp&#8221;, but in order to illustrate the algorithm I show all the steps below:</p>
<pre> M &lt;- cbind(x_obs-mean(x_obs),y_obs-mean(y_obs))#centered matrix
 MCov &lt;- cov(M)          #creates covariance matrix</pre>
<p>Note that the covariance matrix is proportional to the matrix $M^tM$. Next I turn to computation of the principal axes:</p>
<pre> eigenValues &lt;- eigen(MCov)$values       #compute eigenvalues
 eigenVectors &lt;- eigen(MCov)$vectors     #compute eigenvectors</pre>
<p>The eigenvectors of the covariance matrix provide the principal axes, and the eigenvalues quantify the fraction of variance explained in each component. This math is explained in many papers and books so we omit it here, except to say that the fact that eigenvalues of the sample covariance matrix are the principal axes follows from recasting the PCA optimization problem as maximization of the <a href="http://en.wikipedia.org/wiki/Rayleigh_quotient" target="_blank">Raleigh quotient</a>. A key point is that although I&#8217;ve computed the sample covariance matrix explicitly in this example, it is not necessary to do so in practice in order to obtain its eigenvectors. In fact, it is <a href="http://link.springer.com/article/10.1007%2FBF01386022?LI=true" target="_blank">inadvisable to do so</a>. Instead, it is computationally more efficient, and also more stable to directly compute the <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">singular value decomposition</a> of <em>M</em>. The singular value decomposition of <em>M</em> decomposes it into <img src='http://s0.wp.com/latex.php?latex=M%3DUDV%5Et&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='M=UDV^t' title='M=UDV^t' class='latex' /> where <em>D </em>is a diagonal matrix and<i> </i>both <img src='http://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='U' title='U' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=V%5Et&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='V^t' title='V^t' class='latex' /> are orthogonal matrices. I will also not explain in detail the linear algebra of singular value decomposition and its relationship to eigenvectors of the sample covariance matrix (there is plenty of material elsewhere), and only show how to compute it in R:</p>
<pre> d &lt;- svd(M)$d          #the singular values
 v &lt;- svd(M)$v          #the right singular vectors</pre>
<p>The right singular vectors are the eigenvectors of <img src='http://s0.wp.com/latex.php?latex=M%5EtM&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='M^tM' title='M^tM' class='latex' />.  Next I plot the principal axes:</p>
<pre> lines(x_obs,eigenVectors[2,1]/eigenVectors[1,1]*M[x]+mean(y_obs),col=8)</pre>
<p>This shows the first principal axis. Note that it passes through the mean as expected. The ratio of the eigenvectors gives the slope of the axis. Next</p>
<pre> lines(x_obs,eigenVectors[2,2]/eigenVectors[1,2]*M[x]+mean(y_obs),col=8)</pre>
<p>shows the second principal axis, which is orthogonal to the first (recall that the matrix <img src='http://s0.wp.com/latex.php?latex=V%5Et&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='V^t' title='V^t' class='latex' /> in the singular value decomposition is orthogonal). This can be checked by noting that the second principal axis is also</p>
<pre> lines(x_obs,-1/(eigenVectors[2,1]/eigenVectors[1,1])*M[x]+mean(y_obs),col=8)</pre>
<p>as the product of orthogonal slopes is -1. Next, I plot the projections of the points onto the first principal component:</p>
<pre> trans &lt;- (M%*%v[,1])%*%v[,1] #compute projections of points
 P_proj &lt;- scale(trans, center=-cbind(mean(x_obs),mean(y_obs)), scale=FALSE) 
 points(P_proj, col=4,pch=19,cex=0.5) #plot projections
 segments(x_obs,y_obs,P_proj[,1],P_proj[,2],col=4,lty=2) #connect to points</pre>
<p>The linear algebra of the projection is simply a rotation followed by a projection (and an extra step to recenter to the coordinates of the original points). Formally, the matrix <em>M</em> of points is rotated by the matrix of eigenvectors <em>W </em>to produce <img src='http://s0.wp.com/latex.php?latex=T%3DMW&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='T=MW' title='T=MW' class='latex' />. This is the rotation that has all the optimality properties described above. The matrix <em>T</em> is sometimes called the PCA score matrix. All of the above code produces the following figure, which should be compared to those shown above:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/rplot1.jpg"><img class="aligncenter size-full wp-image-3062" src="http://liorpachter.files.wordpress.com/2014/05/rplot1.jpg?w=490&#038;h=490" alt="Rplot" width="490" height="490" /></a></p>
<p>There are many generalizations and modifications to PCA that go far beyond what has been presented here. The first step in generalizing probabilistic PCA is <a href="http://en.wikipedia.org/wiki/Factor_analysis" target="_blank">factor analysis</a>, which includes estimation of variance parameters in each coordinate. Since it is rare that &#8220;noise&#8221; in data will be the same in each coordinate, factor analysis is almost always a better idea than PCA (although the numerical algorithms are more complicated). In other words, I just explained PCA in detail, now I&#8217;m saying don&#8217;t use it! There are other aspects that have been generalized and extended. For example the Gaussian assumption can be relaxed to <a href="http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf" target="_blank">other members of the exponential family</a>, an important idea if the data is discrete (as in genetics). <a href="http://www.nature.com/ng/journal/v44/n6/abs/ng.2285.html" target="_blank">Yang <em>et al.</em> 2012</a> exploit this idea by replacing PCA with logistic PCA for analysis of genotypes. There are also many constrained and regularized versions of PCA, all improving on the basic algorithm to deal with numerous issues and difficulties. Perhaps more importantly, there are issues in <em>using</em> PCA that I have not discussed. A big one is how to choose the PCA dimension to project to in analysis of high-dimensional data. But I am stopping here as I am certain no one is reading at this far into the post anyway&#8230;</p>
<p class="default prettyprint prettyprinted">The take-home message about PCA? <a href="https://www.youtube.com/watch?v=rTyN-vvFIkE" target="_blank">Always be thinking when using it!</a></p>
<p><strong>Acknowledgment</strong>: The exposition of PCA in this post began with notes I compiled for my course <a href="http://math.berkeley.edu/~lpachter/courses/239_Spring_2013/" target="_blank">MCB/Math 239: 14 Lessons in Computational Genomics</a> taught in the Spring of 2013. I thank students in that class for their questions and feedback. None of the material presented in class was new, but the exposition was intended to clarify when PCA ought to be used, and how. I was inspired by the papers of Tipping, Bishop and Roweis on probabilistic PCA in the late 1990s that provided the needed statistical framework for its understanding. Following the class I taught, I benefited greatly from conversations with <a href="http://math.berkeley.edu/~nbray/" target="_blank">Nicolas Bray</a>, <a href="http://www.eecs.berkeley.edu/~brielin/" target="_blank">Brielin Brown</a>, <a href="http://berkeley.academia.edu/IsaacJoseph" target="_blank">Isaac Joseph</a> and <a href="http://bio.math.berkeley.edu/smccurdy/index.html" target="_blank">Shannon McCurdy</a> who helped me to further frame PCA in the way presented in this post.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2640"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2640"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2640"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/?share=facebook" title="Share on Facebook" id="sharing-facebook-2640"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2640-53b3e85755fc8' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2640&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2640-53b3e85755fc8' data-name='like-post-frame-12758541-2640-53b3e85755fc8'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2978 post type-post status-publish format-standard hentry category-education category-expository tag-bessels-correction tag-charles-ii tag-coursera tag-inbreeding tag-mds tag-miriada-x tag-mooc tag-multi-dimensional-scaling tag-pca tag-principal-component-analysis tag-spanish tag-udacity tag-variance entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2978"><a href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/" rel="bookmark">Bessel&#8217;s correction and the dangers of&nbsp;MOOCs</a></h2>
			<p class="post-metadata">May 25, 2014 in <a href="http://liorpachter.wordpress.com/category/education/" rel="category tag">education</a>, <a href="http://liorpachter.wordpress.com/category/expository/" rel="category tag">expository</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/bessels-correction/" rel="tag">Bessel's correction</a>, <a href="http://liorpachter.wordpress.com/tag/charles-ii/" rel="tag">Charles II</a>, <a href="http://liorpachter.wordpress.com/tag/coursera/" rel="tag">Coursera</a>, <a href="http://liorpachter.wordpress.com/tag/inbreeding/" rel="tag">inbreeding</a>, <a href="http://liorpachter.wordpress.com/tag/mds/" rel="tag">MDS</a>, <a href="http://liorpachter.wordpress.com/tag/miriada-x/" rel="tag">Miriada X</a>, <a href="http://liorpachter.wordpress.com/tag/mooc/" rel="tag">MOOC</a>, <a href="http://liorpachter.wordpress.com/tag/multi-dimensional-scaling/" rel="tag">multi-dimensional scaling</a>, <a href="http://liorpachter.wordpress.com/tag/pca/" rel="tag">PCA</a>, <a href="http://liorpachter.wordpress.com/tag/principal-component-analysis/" rel="tag">principal component analysis</a>, <a href="http://liorpachter.wordpress.com/tag/spanish/" rel="tag">Spanish</a>, <a href="http://liorpachter.wordpress.com/tag/udacity/" rel="tag">Udacity</a>, <a href="http://liorpachter.wordpress.com/tag/variance/" rel="tag">Variance</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/#comments" title="Comment on Bessel&#8217;s correction and the dangers of&nbsp;MOOCs">11 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>The Habsburg rulership of Spain ended with an <a href="http://en.wikipedia.org/wiki/Coefficient_of_inbreeding" target="_blank">inbreeding coefficient</a> of F=0.254. The last king, <a href="http://en.wikipedia.org/wiki/Charles_II_of_Spain#cite_note-inbred-4" target="_blank">Charles II</a> (1661-1700), suffered an unenviable life. He was unable to chew. His tongue was so large he could not speak clearly, and he constantly drooled. Sadly, his mouth was the least of his problems. He suffered seizures, had intellectual disabilities, and was frequently vomiting. He was also impotent and infertile, which meant that even his death was a curse in that <a href="http://en.wikipedia.org/wiki/War_of_the_Spanish_Succession" target="_blank">his lack of heirs led to a war</a>.</p>
<p>None of these problems prevented him from being married (twice). His first wife, princess Henrietta of England, died at age 26 after becoming deeply depressed having being married to the man for a decade. Only a year later, he married another princess, 23 year old Maria Anna of Neuberg. To put it mildly, his wives did not end up living the charmed life of <a href="http://princess.disney.com" target="_blank">Disney princesses</a>, nor were they presumably smitten by young Charles II who apparently aged prematurely and looked the part of his horrific homozygosity. The princesses married Charles II because they were forced to. Royals organized marriages to protect and expand their power, money and influence. Coupled to this were primogeniture rules which ensured that the sons of kings, their own flesh and blood and therefore presumably the best-suited to be in power, would indeed have the opportunity to succeed their fathers. The family tree of Charles II shows how this worked in Spain:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/05/charles.jpg"><img class="aligncenter size-full wp-image-2982" src="http://liorpachter.files.wordpress.com/2014/05/charles.jpg?w=490&#038;h=431" alt="Charles" width="490" height="431" /></a></p>
<p><a href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0005174" target="_blank">It is believed that the inbreeding in Charles II&#8217;s family led to two genetic disorders, combined pituitary hormone deficiency and distal renal tubular acidosis</a>, that explained many of his physical and mental problems. In other words, genetic diversity is important, and the point of this blog post is to highlight the fact that diversity is important in education as well.</p>
<p>The problem of inbreeding in academia has been studied previously, albeit to a limited extent. One interesting article is <a href="http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1090.1109" target="_blank">Navel Grazing: Academic Inbreeding and Scientific Productivity</a> by Horta <em>et</em> <em>al </em>published in 2010 (my own experience with<a title="The network nonsense of Manolis Kellis" href="http://liorpachter.wordpress.com/2014/02/11/the-network-nonsense-of-manolis-kellis/" target="_blank"> an inbred academic</a> from a <a href="http://jeffhuang.com/computer_science_professors.html#composition" target="_blank">department where 39% of the faculty are self-hires</a> anecdotally confirms the claims made in the paper). But here I focus on the downsides of inbreeding of <em>ideas</em> rather than of faculty. For example home-schooling, the educational equivalent of primogeniture, <a href="http://www.examiner.com/article/florida-genius-teen-graduates-college-and-high-school-at-sixteen" target="_blank">can be fantastic</a> if the parents happen to be good teachers, but <a href="http://www.salon.com/2012/03/15/homeschooled_and_illiterate/" target="_blank">can fail miserably</a> if they are not. One thing that is guaranteed in a school or university setting is that learning happens by exposure to many teachers (different faculty, students, tutors, the internet, etc.) Students frequently complain when there is high variance in teaching quality, but one thing such variance ensures is that is is very unlikely that any student is exposed <em>only</em> to <em>bad</em> teachers. Diversity in teaching also helps to foster the development of new ideas. Different teachers, by virtue of insight or error, will occasionally &#8220;mutate&#8221; ideas or concepts for better or for worse. In other words, one does not have to fully embrace the theory of <a href="http://en.wikipedia.org/wiki/Meme" target="_blank">memes</a> to acknowledge that there are benefits to variance in teaching styles, methods and pedagogy. Conversely, there is danger in homogeneity.</p>
<p>This brings me to MOOCs. One of the great things about MOOCs is that they reach millions of people. Udacity claims it has 1.6 million &#8220;users&#8221; (students?). Coursera claims 7.1 million. These companies are greatly expanding the accessibility of education. Starving children in India can now take courses in mathematical methods for quantitative finance, and for the first time in history, a president of the United States can discreetly take a freshman course on economics together with its high school algebra prerequisites (highly recommended). But when I am asked whether I would be interested in offering a MOOC I hesitate, paralyzed at the thought that any error I make would immediately be embedded in the brains of millions of innocent victims. My concern is this: MOOCs can greatly reduce the variance in education. For example, Coursera currently offers 641 courses, which means that each courses is or has been taught to over 11,000 students. Many college courses may have less than a few dozen students, and even large college courses rarely have more than a few hundred students. This means that on average, through MOOCs, individual professors reach many more (2 orders of magnitude!) students. A great lecture can end up positively impacting a large number of individuals, but at the same time, a MOOC can be a vehicle for infecting the brains of millions of people with nonsense. If that nonsense is then propagated and reaffirmed via the interactions of the people who have learned it from the same source, then the inbreeding of ideas has occurred.</p>
<p>I mention MOOCs because I was recently thinking about intuition behind <a href="http://en.wikipedia.org/wiki/Bessel%27s_correction#Proof_that_Bessel.27s_correction_yields_an_unbiased_estimator_of_the_population_variance" target="_blank">Bessel&#8217;s correction</a> replacing <em>n</em> with <em>n-1</em> in the formula for sample variance. Formally, Bessel&#8217;s correction replaces the biased formula</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=s%5E2_n+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En+%28x_i+-+%5Coverline%7Bx%7D%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s^2_n = &#92;frac{1}{n} &#92;sum_{i=1}^n (x_i - &#92;overline{x})^2' title='s^2_n = &#92;frac{1}{n} &#92;sum_{i=1}^n (x_i - &#92;overline{x})^2' class='latex' /></p>
<p>for estimating the variance of a random variable from samples <img src='http://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_1,&#92;ldots,x_n' title='x_1,&#92;ldots,x_n' class='latex' /> with</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=s%5E2_%7Bn-1%7D+%3D+%5Cfrac%7B1%7D%7Bn-1%7D+%5Csum_%7Bi%3D1%7D%5En+%28x_i-%5Coverline%7Bx%7D%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s^2_{n-1} = &#92;frac{1}{n-1} &#92;sum_{i=1}^n (x_i-&#92;overline{x})^2' title='s^2_{n-1} = &#92;frac{1}{n-1} &#92;sum_{i=1}^n (x_i-&#92;overline{x})^2' class='latex' />.</p>
<p>The switch from <em>n </em>to <em>n-1</em> is a bit mysterious and surprising, and in introductory statistics classes it is frequently just presented as a &#8220;fact&#8221;. When an explanation is provided, it is usually in the form of algebraic manipulation that establishes the result. The issue came up as a result of a blog post I&#8217;m writing about principal components analysis (PCA), and I thought I would check for an intuitive explanation online. I googled &#8220;intuition sample variance&#8221; and the top link was a MOOC from the <a href="https://www.khanacademy.org" target="_blank">Khan Academy</a>:</p>
<p><span class='embed-youtube' style='text-align:center; display: block;'><iframe class='youtube-player' type='text/html' width='490' height='306' src='http://www.youtube.com/embed/KkaU2ur3Ymw?version=3&#038;rel=1&#038;fs=1&#038;showsearch=0&#038;showinfo=1&#038;iv_load_policy=1&#038;wmode=transparent' frameborder='0'></iframe></span></p>
<p>The video has over 51,000 views with over 100 &#8220;likes&#8221; and only 6 &#8220;dislikes&#8221;. Unfortunately, in this case, popularity is not a good proxy for quality. Despite the title promising &#8220;review&#8221; and &#8220;intuition&#8221; for &#8220;why we divide by <em>n-1 </em>for the unbiased sample variance&#8221; there is no specific reason given why <em>n </em>is replaced by <em>n-1 </em>(as opposed to another correction). Furthermore, the intuition provided has to do with the fact that <img src='http://s0.wp.com/latex.php?latex=x_i-%5Coverline%7Bx%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i-&#92;overline{x}' title='x_i-&#92;overline{x}' class='latex' /> underestimates <img src='http://s0.wp.com/latex.php?latex=x_i-%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='x_i-&#92;mu' title='x_i-&#92;mu' class='latex' /> (where <img src='http://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mu' title='&#92;mu' class='latex' /> is the mean of the random variable and <img src='http://s0.wp.com/latex.php?latex=%5Coverline%7Bx%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;overline{x}' title='&#92;overline{x}' class='latex' /> is the sample mean) but the explanation is confusing and not quantitative (which it can easily be). In fact, the <a href="http://en.wikipedia.org/wiki/Bessel%27s_correction#Proof_that_Bessel.27s_correction_yields_an_unbiased_estimator_of_the_population_variance" target="_blank">wikipedia page for Bessel&#8217;s correction</a> provides three different mathematical explanations for the correction together with the intuition that motivates them, but it is difficult to find with Google unless one knows that the correction is called &#8220;Bessel&#8217;s correction&#8221;.</p>
<p>Wikipedia is also not perfect, and this example is a good one for why teaching by humans is important. Among the three alternative derivations, I think that one stands out as &#8220;better&#8221; but one would not know by just looking at the wikipedia page. Specifically, I refer to &#8220;Alternate 1&#8243; on the wikipedia page, that is essentially explaining that <strong>variance can be rewritten as a double sum corresponding to the average squared distance between points and the diagonal terms of the sum are zero in expectation</strong><strong>.</strong> An explanation of why this fact leads to the <em>n-1</em> in the unbiased estimator is as follows:</p>
<p>The first step is to notice that the variance of a random variable is equal to half of the expected squared difference of two independent identically distributed random variables of that type. Specifically, the definition of variance is:</p>
<p><img src='http://s0.wp.com/latex.php?latex=var%28X%29+%3D+%5Cmathbb%7BE%7D%28X+-+%5Cmu%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='var(X) = &#92;mathbb{E}(X - &#92;mu)^2' title='var(X) = &#92;mathbb{E}(X - &#92;mu)^2' class='latex' /> where <img src='http://s0.wp.com/latex.php?latex=%5Cmu+%3D+%5Cmathbb%7BE%7D%28X%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mu = &#92;mathbb{E}(X)' title='&#92;mu = &#92;mathbb{E}(X)' class='latex' />. Equivalently, <img src='http://s0.wp.com/latex.php?latex=var%28X%29+%3D+%5Cmathbb%7BE%7D%28X%5E2%29+-%5Cmu%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='var(X) = &#92;mathbb{E}(X^2) -&#92;mu^2' title='var(X) = &#92;mathbb{E}(X^2) -&#92;mu^2' class='latex' />. Now suppose that <em>Y</em> is another random variable identically distributed to <em>X</em> and with <em>X,Y </em>independent. Then <img src='http://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%28X-Y%29%5E2+%3D+2+var%28X%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mathbb{E}(X-Y)^2 = 2 var(X)' title='&#92;mathbb{E}(X-Y)^2 = 2 var(X)' class='latex' />. This is easy to see by using the fact that</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%28X-Y%29%5E2+%3D+%5Cmathbb%7BE%7D%28X%5E2%29+%2B+%5Cmathbb%7BE%7D%28Y%5E2%29+-+2%5Cmathbb%7BE%7D%28X%29%5Cmathbb%7BE%7D%28Y%29+%3D+2%5Cmathbb%7BE%7D%28X%5E2%29-2%5Cmu%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;mathbb{E}(X-Y)^2 = &#92;mathbb{E}(X^2) + &#92;mathbb{E}(Y^2) - 2&#92;mathbb{E}(X)&#92;mathbb{E}(Y) = 2&#92;mathbb{E}(X^2)-2&#92;mu^2' title='&#92;mathbb{E}(X-Y)^2 = &#92;mathbb{E}(X^2) + &#92;mathbb{E}(Y^2) - 2&#92;mathbb{E}(X)&#92;mathbb{E}(Y) = 2&#92;mathbb{E}(X^2)-2&#92;mu^2' class='latex' />.</p>
<p>This identity motivates a rewriting of the (uncorrected) sample variance <img src='http://s0.wp.com/latex.php?latex=s_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s_n' title='s_n' class='latex' /> in a way that is computationally less efficient, but mathematically more insightful:</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=s_n+%3D+%5Cfrac%7B1%7D%7B2n%5E2%7D+%5Csum_%7Bi%2Cj%3D1%7D%5En+%28x_i-x_j%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s_n = &#92;frac{1}{2n^2} &#92;sum_{i,j=1}^n (x_i-x_j)^2' title='s_n = &#92;frac{1}{2n^2} &#92;sum_{i,j=1}^n (x_i-x_j)^2' class='latex' />.</p>
<p>Of note is that in this summation exactly <i>n</i> of the terms are zero, namely the terms when <em>i=j</em>. These terms are zero independently of the original distribution, and remain so <em>in expectation</em> thereby biasing the estimate of the variance, specifically leading to an underestimate. Removing them fixes the estimate and produces</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=s_%7Bn-1%7D%5E2+%3D+%5Cfrac%7B1%7D%7B2n%28n-1%29%7D+%5Csum_%7Bi%2Cj%3D1%2C+i+%5Cneq+j%7D%5En+%28x_i-x_j%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s_{n-1}^2 = &#92;frac{1}{2n(n-1)} &#92;sum_{i,j=1, i &#92;neq j}^n (x_i-x_j)^2' title='s_{n-1}^2 = &#92;frac{1}{2n(n-1)} &#92;sum_{i,j=1, i &#92;neq j}^n (x_i-x_j)^2' class='latex' />.</p>
<p>It is easy to see that this is indeed Bessel&#8217;s correction. In other words, the correction boils down to the fact that <img src='http://s0.wp.com/latex.php?latex=n%5E2-n+%3D+n%28n-1%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='n^2-n = n(n-1)' title='n^2-n = n(n-1)' class='latex' />, hence the appearance of <em>n-1</em>.</p>
<p>Why do I like this particular derivation of Bessel&#8217;s correction? There are two reasons: first, <em>n-1</em> emerges naturally and obviously from the derivation. The denominator in <img src='http://s0.wp.com/latex.php?latex=s_%7Bn-1%7D%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s_{n-1}^2' title='s_{n-1}^2' class='latex' /> matches exactly the number of terms being summed, so that it can be understood as a true average (this is not apparent in its standard form as <img src='http://s0.wp.com/latex.php?latex=s_%7Bn-1%7D%5E2+%3D+%5Cfrac%7B1%7D%7Bn-1%7D+%5Csum_%7Bi%3D1%7D%5En+%28x_i-%5Coverline%7Bx%7D%29%5E2&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='s_{n-1}^2 = &#92;frac{1}{n-1} &#92;sum_{i=1}^n (x_i-&#92;overline{x})^2' title='s_{n-1}^2 = &#92;frac{1}{n-1} &#92;sum_{i=1}^n (x_i-&#92;overline{x})^2' class='latex' />. There is really nothing mysterious anymore, its just that some terms having been omitted from the sum because they were non-inofrmative. Second, as I will show in my forthcoming blog post on PCA, the fact that the variance of a random variable is half of the expectation of the squared difference of two instances, is key to understanding the connection between multi-dimensional scaling (MDS) and PCA. In other words, as my student <a href="http://math.berkeley.edu/~nbray/" target="_blank">Nicolas Bray</a> is fond of saying, although most people think a proof is either right or wrong, in fact some proofs are more right than others. The connection between Bessel&#8217;s correction and PCA goes even deeper: as explained by Saville and Wood in their book <a href="http://www.amazon.com/Statistical-Methods-Geometric-Approach-Statistics/dp/0387975179" target="_blank">Statistical Methods: A Geometric Approach</a> <em>n-1</em> can be understood to be a reduction in one <em>dimension </em>from the point of view of probabilistic PCA (Saville and Wood do not explicitly use the term probabilistic PCA but as I will explain in my PCA post it is implicit in their book). Finally, there are many subtleties to Bessel&#8217;s correction, for example it is an unbiased estimator for <i>variance </i>and not <em>standard deviation</em>. These issues ought to be mentioned in a good lecture about the topic. In other words, the Khan lecture is neither necessary nor sufficient, but unlike a standard lecture where the damage is limited to a small audience of students, it has been viewed more than 50,000 times and those views cannot be unviewed.</p>
<div id="watch-description-clip">
<div id="watch-description-text">
<p>In writing this blog post I pondered the irony of my call for added diversity in teaching while I preach my own idea (this post) to a large number of readers via a medium designed for maximal outreach. I can only ask that others blog as well to offer alternative points of view <span class='wp-smiley emoji emoji-smile' title=':)'>:)</span> and that readers inform themselves on the issues I raise by fact-checking elsewhere. As far as the statistics goes, if someone finds the post confusing, they should go and register for one of the many fantastic <a href="https://www.coursera.org/courses?search=statist" target="_blank">MOOCs on statistics</a>! But I reiterate that in the rush to MOOCdom, providers must offer diversity in their offerings (even multiple lectures on the same topic) to ensure a healthy population of memes. This is especially true in Spain, where already <a href="http://www.nature.com/nature/journal/v410/n6824/full/410014b0.html" target="_blank">inbred faculty</a> are now inbreeding what they teach <a href="http://www.catedratelefonica.upf.edu/?page_id=4531" target="_blank">by MOOCing</a> via <a href="http://es.wikipedia.org/wiki/Mir%C3%ADada_X" target="_blank">Miriada X</a>. Half of the MOOCs being offered in Spain originate from just 3 universities, while the number of potential viewers is enormous as Spanish is now the second most spoken language in the world (thanks to Charles II&#8217;s great-great-grandfather, Charles I).</p>
<p>May Charles II rest in peace.</p>
</div>
</div>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2978"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2978"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2978"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/?share=facebook" title="Share on Facebook" id="sharing-facebook-2978"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2978-53b3e85766f53' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2978&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2978-53b3e85766f53' data-name='like-post-frame-12758541-2978-53b3e85766f53'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2842 post type-post status-publish format-standard hentry category-talks tag-arrows-impossibility-theorem tag-authorship tag-card-catalog tag-dewey-decimal-classification tag-hardy-littlewood-rule entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2842"><a href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/" rel="bookmark">Time to end ordered&nbsp;authorship</a></h2>
			<p class="post-metadata">May 16, 2014 in <a href="http://liorpachter.wordpress.com/category/talks/" rel="category tag">talks</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/arrows-impossibility-theorem/" rel="tag">Arrow's impossibility theorem</a>, <a href="http://liorpachter.wordpress.com/tag/authorship/" rel="tag">authorship</a>, <a href="http://liorpachter.wordpress.com/tag/card-catalog/" rel="tag">card catalog</a>, <a href="http://liorpachter.wordpress.com/tag/dewey-decimal-classification/" rel="tag">Dewey decimal classification</a>, <a href="http://liorpachter.wordpress.com/tag/hardy-littlewood-rule/" rel="tag">Hardy-Littlewood rule</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/#comments" title="Comment on Time to end ordered&nbsp;authorship">11 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>One of my distinct memories from elementary school is going to &#8220;library class&#8221; to learn about the <a href="http://en.wikipedia.org/wiki/Dewey_Decimal_Classification" target="_blank">Dewey decimal classification</a> and how to use a <a href="http://en.wikipedia.org/wiki/Library_catalog#Catalog_card" target="_blank">card catalog</a> to find books. Searching for books efficiently was possible because cards in the catalog were sorted lexicographically.</p>
<p>It didn&#8217;t occur to me at the time, but the system <em>required</em> authors of books to be <a href="http://en.wikipedia.org/wiki/Total_order" target="_blank">totally ordered</a>. Without an ordering of authors in a book with multiple authors, there would be no way to decide where to place the card for the book in a catalog searchable by author. The practice of ordering authors on publications is evident in the oldest printed texts and has persisted to this day. I have never thought that it could be any other way.</p>
<p>However this past Wednesday I was visiting the University of Washington to deliver a seminar, and among the highlights of the visit was my meeting with the graduate students. I met 12 for lunch and two more came for dinner. Meeting with students is always my favorite part of a visit to a university. They have original and creative ideas, and most importantly, are not bound in their thought by archaic tradition. They frequently don&#8217;t know what one is supposed to think and how one is supposed to say it. They just think and speak!</p>
<p>One of the students I met on Wednesday was <a href="http://faculty.washington.edu/dfowler/people.html" target="_blank">Vanessa Gray, a student of Doug Fowler</a>, who in a conversation on authorship practices suggested to me the radical and brilliant idea that <strong>papers should be published without an ordering of authors</strong>.</p>
<p>Many journals now have a section called &#8220;Author contributions&#8221; where roles of individuals in collaborative projects can be described (many journals now require such descriptions). So why bother ordering the authors for a list underneath the title? As far as indexing and searching goes, Google and other search engines require only a set of authors, and not a specific ordering.</p>
<p>I agree with Vanessa that ending author ordering on publications would greatly improve fairness in the biological sciences, where many current projects involve complex assemblies of teams with complementary skills. &#8220;First authorship&#8221; is not well-defined when one author performed a large number of difficult experiments, and another developed novel algorithms and wrote complex software for analyzing the experiments. Similarly, &#8220;last authorship&#8221; fails as a concept when students are co-advised, or one principal investigator provides substantial funding on a project, while another is participating in doing the work. And recently, large consortium projects have completely destroyed any meaning of &#8220;author&#8221; by having hundreds, or even thousands of authors on projects. Even when there are relatively few authors people rarely credit anyone except the first and last authors, even if others did substantial work. In the recent ENCODE paper published in PNAS with 30 authors, it appears to me from the responses to <a title="Estimating number of transcripts from RNA-Seq measurements (and why I believe in paywall)" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/" target="_blank">my previous blog post about the paper</a> that the 5th and 6th authors did a lot (majority?) of the work in putting together figures and results, yet I suspect the &#8220;credit&#8221; for the paper will go to the first author (the flip side in that case is that the first author is where blame is assigned as well).</p>
<p>There is also a theoretical justification for <em>not </em>ordering authors. Ordering of authors on a publication can be thought of as a ranking produced by &#8220;votes&#8221; of the participants in the project. Of course in practice not all votes are equal. In what is called dictatorship in social choice theory, PIs frequently make the decisions independently of how specific authors feel they may have contributed. This may work on a paper where there is a single PI (although it may be considered unfair by the graduate students and postdocs), however dictatorship as a system for determining authorship certainly breaks down when multiple PIs collaborate on a project. <a href="http://en.wikipedia.org/wiki/Arrow's_impossibility_theorem" target="_blank">Arrow&#8217;s impossibility theorem</a> explains that in the absence of dictatorship, there is a problem in producing a single ordering satisfying two other seemingly basic and essential fairness criteria. Informally, the theorem states that there is no authorship ordering <span style="color:#252525;">system based on voting of contributing authors that can satisfy the following three criteria:</span></p>
<ul>
<li>If every author thinks that X should be ordered before Y, then the author list should have X placed before Y.</li>
<li>For a fixed list of voting preferences regarding the ordering of X vs. Y, the ordering between X and Y in the author list will remain unchanged even if does not depend on the ordering of other pairs such as X and Z, Y and Z, or Z and W.</li>
<li>There is no &#8220;dictator&#8221;, i.e. no single author possesses the power to determine the author ordering.</li>
</ul>
<p><a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1764586/" target="_blank">Authors frequently have differing opinions about the impact of their own contribution to a publication</a>, and therefore their preferences (votes) for author ordering are discordant. This means that any system for ordering authors will not satisfy everyone&#8217;s preferences, and in the sense of Arrow&#8217;s impossibility theorem will be unfair. One way around Arrow&#8217;s impossibility theorem is to specify authorship order without regard to authors&#8217; preferences, for example by always ordering authors alphabetically (the Hardy-Littlewood rule). This method, usually the one used in the mathematical sciences, is also <a href="http://www.globalsciencebooks.info/JournalsSup/images/2013/AAJPSB_7(SI1)/AAJPSB_7(SI1)72-75o.pdf" target="_blank">fraught with problems</a>. Of course, listing author contributions for what they are is not entirely trivial. For example, different authors may have conflicting views about what it means to have &#8220;written the text of the paper&#8221;. But using words to describe contributions allow for much more detail about what each author did, and allows for nuanced contributions to be described (e.g., John and Jane were in the room when the initial idea for the project was discussed, but did not contribute anything afterwards).</p>
<p>To summarize, in the modern era of electronic publishing ordering of authors is unnecessary, and if it is unnecessary, then why confront Arrow&#8217;s theorem and inevitably produce orderings unfairly? Publications should just explain the author contributions. <strong>Time to end ordered authorship</strong>.</p>
<p style="text-align:center;"><a href="http://liorpachter.files.wordpress.com/2014/05/yale_card_catalog.jpg"><img class="aligncenter size-full wp-image-2844" src="http://liorpachter.files.wordpress.com/2014/05/yale_card_catalog.jpg?w=490&#038;h=490" alt="Yale_card_catalog" width="490" height="490" /></a> The card catalog at Yale University&#8217;s Sterling Memorial Library (from Wikipedia).</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2842"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2842"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2842"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/?share=facebook" title="Share on Facebook" id="sharing-facebook-2842"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2842-53b3e85770fbb' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2842&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2842-53b3e85770fbb' data-name='like-post-frame-12758541-2842-53b3e85770fbb'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2683 post type-post status-publish format-standard hentry category-rna-seq tag-encode tag-fpkm tag-rna-seq tag-rpkm tag-tpm entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2683"><a href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/" rel="bookmark">Estimating number of transcripts from RNA-Seq measurements (and why I believe in&nbsp;paywall)</a></h2>
			<p class="post-metadata">April 30, 2014 in <a href="http://liorpachter.wordpress.com/category/seq/rna-seq/" rel="category tag">RNA-Seq</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/encode/" rel="tag">ENCODE</a>, <a href="http://liorpachter.wordpress.com/tag/fpkm/" rel="tag">FPKM</a>, <a href="http://liorpachter.wordpress.com/tag/rna-seq/" rel="tag">RNA-Seq</a>, <a href="http://liorpachter.wordpress.com/tag/rpkm/" rel="tag">RPKM</a>, <a href="http://liorpachter.wordpress.com/tag/tpm/" rel="tag">TPM</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/#comments" title="Comment on Estimating number of transcripts from RNA-Seq measurements (and why I believe in&nbsp;paywall)">43 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>I was recently reading the latest <a href="http://www.pnas.org/content/early/2014/04/18/1318948111.abstract?sid=e8ecb518-0777-4829-a8b8-5cd083bdc528" target="_blank">ENCODE paper published in PNAS </a>when a sentence in the caption of Figure 2 caught my attention:</p>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
<blockquote><p>&#8220;Depending on the total amount of RNA in a cell, one transcript copy per cell corresponds to between 0.5 and 5 FPKM in PolyA+ whole-cell samples according to current estimates (with the upper end of that range corresponding to small cells with little RNA and vice versa).&#8221;</p></blockquote>
</div>
</div>
</div>
</div>
<p>Although very few people actually care about ENCODE, many people do care about the interpretation of RNA-Seq FPKM measurements and to them this is likely to be a sentence of interest. In fact, there have been a number of attempts to provide intuitive meaning for RPKM (and FPKM) in terms of copy numbers of transcripts per cell. Even though the ENCODE PNAS paper provides no citation for the statement (or methods section explaining the derivation), I believe its source is the<i> </i>RNA-Seq paper by Mortazavi <em>et al.</em> In that paper, the authors write that</p>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
<blockquote><p>&#8220;&#8230;absolute transcript levels per cell can also be calculated. For example, on the basis of literature values for the mRNA content of a liver cell [Galau <em>et al.</em> 1977] and the RNA standards, we estimated that 3 RPKM corresponds to about one transcript per liver cell. For C2C12 tissue culture cells, for which we know the starting cell number and RNA preparation yields needed to make the calculation, a transcript of 1 RPKM corresponds to approximately one transcript per cell. &#8220;</p></blockquote>
<p>This statement has been picked up on in a number of publications (e.g., <a href="http://msb.embopress.org/content/7/1/497" target="_blank">Hebenstreit <em>et al.</em>, 2011, </a><a href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001102" target="_blank">van Bakel <em>et al.</em>, 2011</a>). However the inference of transcript copies per cell directly from RPKM or FPKM estimates is not possible and conversion factors such as 1 RPKM = 1 transcript per cell are <strong>incoherent.</strong> At the same time, the estimates of Mortazavi <em>et al.</em> and the range provided in the ENCODE PNAS paper are<strong> informative. </strong>The &#8220;incoherence&#8221; stems from a subtle issue in the normalization of RPKM/FPKM that I have discussed in a <a href="https://www.youtube.com/watch?v=5NiFibnbE8o" target="_blank">talk I gave at CSHL</a>, and is the reason why TPM is a better unit for RNA abundance. Still, the estimates turn out to be &#8220;informative&#8221;, in the sense that the effect of (lack of) normalization appears to be smaller than variability in the amount of RNA per cell. I explain these issues below:</p>
<p><strong>Why is the sentence incoherent?</strong></p>
</div>
</div>
</div>
</div>
<p>RNA-Seq can be used to estimate transcript abundances in an RNA sample. Formally, a sample consists of <em>n</em> distinct types of transcripts, and each occurs with different multiplicity (copy number), so that transcript <em>i </em>appears <img src='http://s0.wp.com/latex.php?latex=m_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m_i' title='m_i' class='latex' /> times in the sample. By &#8220;abundance&#8221; we mean the relative amounts <img src='http://s0.wp.com/latex.php?latex=%5Crho_1%2C%5Cldots%2C%5Crho_n&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho_1,&#92;ldots,&#92;rho_n' title='&#92;rho_1,&#92;ldots,&#92;rho_n' class='latex' /> where <img src='http://s0.wp.com/latex.php?latex=%5Crho_i+%3D+%5Cfrac%7Bm_i%7D%7B%5Csum_%7Bi%3D1%7D%5En+m_i%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho_i = &#92;frac{m_i}{&#92;sum_{i=1}^n m_i}' title='&#92;rho_i = &#92;frac{m_i}{&#92;sum_{i=1}^n m_i}' class='latex' />. Note that  <img src='http://s0.wp.com/latex.php?latex=0+%5Cleq+%5Crho_i+%5Cleq+1&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='0 &#92;leq &#92;rho_i &#92;leq 1' title='0 &#92;leq &#92;rho_i &#92;leq 1' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Crho_i+%3D+1&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;sum_{i=1}^n &#92;rho_i = 1' title='&#92;sum_{i=1}^n &#92;rho_i = 1' class='latex' />. Suppose that <img src='http://s0.wp.com/latex.php?latex=m_j%3D1&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='m_j=1' title='m_j=1' class='latex' /> for some <em>j. </em>The corresponding <img src='http://s0.wp.com/latex.php?latex=%5Crho_j&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho_j' title='&#92;rho_j' class='latex' /> is therefore <img src='http://s0.wp.com/latex.php?latex=%5Crho_j+%3D+%5Cfrac%7B1%7D%7BM%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho_j = &#92;frac{1}{M}' title='&#92;rho_j = &#92;frac{1}{M}' class='latex' /> where <img src='http://s0.wp.com/latex.php?latex=M+%3D+%5Csum_%7Bi%3D1%7D%5En+m_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='M = &#92;sum_{i=1}^n m_i' title='M = &#92;sum_{i=1}^n m_i' class='latex' />. The question is what does this <img src='http://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho' title='&#92;rho' class='latex' /> value correspond to in RPKM (or FPKM).</p>
<p>RPKM stands for &#8220;reads per kilobase  of transcript per million reads mapped&#8221; and FPKM is the same except with &#8220;fragment&#8221; replacing read (initially reads were not paired-end, but with the advent of paired-end sequencing it makes more sense to speak of fragments, and hence FPKM). As a unit of measurement for an estimate, what FPKM really refers to is the <em>expected</em> number of fragments per kilboase of transcript per million reads. Formally, if we let <img src='http://s0.wp.com/latex.php?latex=l_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='l_i' title='l_i' class='latex' /> be the length of transcript <em>i </em>and define <img src='http://s0.wp.com/latex.php?latex=%5Calpha_i+%3D+%5Cfrac%7B%5Crho_i+l_i%7D%7B%5Csum_%7Bj%3D1%7D%5En+%5Crho_j+l_j%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;alpha_i = &#92;frac{&#92;rho_i l_i}{&#92;sum_{j=1}^n &#92;rho_j l_j}' title='&#92;alpha_i = &#92;frac{&#92;rho_i l_i}{&#92;sum_{j=1}^n &#92;rho_j l_j}' class='latex' /> then abundance in FPKM for transcript <em>i </em>is abundance measured as <img src='http://s0.wp.com/latex.php?latex=FPKM_i+%3D+%5Cfrac%7B%5Calpha_i+%5Ccdot+10%5E%7B6%7D%7D%7Bl_i%2F%2810%5E3%29%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='FPKM_i = &#92;frac{&#92;alpha_i &#92;cdot 10^{6}}{l_i/(10^3)}' title='FPKM_i = &#92;frac{&#92;alpha_i &#92;cdot 10^{6}}{l_i/(10^3)}' class='latex' />. In terms of <img src='http://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho' title='&#92;rho' class='latex' />, we obtain that</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=FPKM_i+%3D+%5Cfrac%7B%5Crho_i+%5Ccdot+10%5E9%7D%7B%5Csum_%7Bj%3D1%7D%5En+%5Crho_j+l_j%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='FPKM_i = &#92;frac{&#92;rho_i &#92;cdot 10^9}{&#92;sum_{j=1}^n &#92;rho_j l_j}' title='FPKM_i = &#92;frac{&#92;rho_i &#92;cdot 10^9}{&#92;sum_{j=1}^n &#92;rho_j l_j}' class='latex' />.</p>
<p style="text-align:left;">The term in the denominator can be considered a kind of normalization factor, that while identical for each transcript, depends on the abundances of each transcript (unless all lengths are equal). It is in essence an average of lengths of transcripts weighted by abundance. Moreover, the length of each transcript should be taken to be taken to be its &#8220;effective&#8221; length, i.e. the length with respect to fragment lengths, or equivalently, the number of positions where fragments can start.</p>
<p style="text-align:left;">The implication for finding a relationship between FPKM and relative abundance constituting one transcript copy per cell is that one cannot. Mathematically, the latter is equivalent to setting <img src='http://s0.wp.com/latex.php?latex=%5Crho_i+%3D+%5Cfrac%7B1%7D%7BM%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho_i = &#92;frac{1}{M}' title='&#92;rho_i = &#92;frac{1}{M}' class='latex' /> in the formula above and then trying to determine <img src='http://s0.wp.com/latex.php?latex=FPKM_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='FPKM_i' title='FPKM_i' class='latex' />. Unfortunately, all the remaining <img src='http://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho' title='&#92;rho' class='latex' /> are still in the formula, and must be known in order to calculate the corresponding FPKM value.</p>
<p style="text-align:left;">The argument above makes clear that it does not make sense to estimate transcript copy counts per cell in terms of RPKM or FPKM. Measurements in RPKM or FPKM units depend on the abundances of transcripts in the specific sample being considered, and therefore the connection to copy counts is incoherent. The obvious and correct solution is to work directly with the <img src='http://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;rho' title='&#92;rho' class='latex' />. This is the rationale of TPM (transcripts per million) used by Bo Li and Colin Dewey in the <a href="http://www.biomedcentral.com/1471-2105/12/323/" target="_blank">RSEM paper</a> (the argument for TPM is also made in <a href="http://lynchlab.uchicago.edu/publications/Wagner,%20Kin,%20and%20Lynch%20(2012).pdf" target="_blank">Wagner <em>et al.</em> 2012</a>).</p>
<p><strong>Why is the sentence informative?</strong></p>
<p>Even though incoherent, it turns out there is some truth to the ranges and estimates of copy count per cell in terms of RPKM and FPKM that have been circulated. To understand why requires noting that there are in fact two factors that come into play in estimating the FPKM corresponding to abundance of one transcript copy per cell. First, is <em>M </em>as defined above to be the total number of transcripts in a cell. This depends on the amount of RNA in a cell. Second are the relative abundances of all transcripts and their contribution to the denominator in the <img src='http://s0.wp.com/latex.php?latex=FPKM_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='FPKM_i' title='FPKM_i' class='latex' /> formula.</p>
<p>The best paper to date on the connection between transcript copy numbers and RNA-Seq measurements is the careful work of Marinov <em>et al.</em> in &#8220;<a href="http://genome.cshlp.org/content/24/3/496" target="_blank">From single-cell to cell-pool transcriptomes: stochasticity in gene expression and RNA splicing</a>&#8221; published in Genome Research earlier this year. First of all, the paper describes careful estimates of RNA quantities in different cells, and concludes that (at least for the cells studied in the paper) amounts vary by approximately one order of magnitude. Incidentally, the estimates in Marinov <em>et al.</em> confirm and are consistent with rough estimates of <a href="http://www.sciencedirect.com/science/article/pii/0003986177901473" target="_blank">Galau <em>et al.</em> from 1977,</a> of 300,000 transcripts per cell. Marinov <em>et al. </em>also use spike-in measurements are used to conclude that in &#8220;<span style="color:#403838;">GM12878 single cells, one transcript copy corresponds to ∼10 FPKM on average.&#8221;. The main value of the paper lies in its confirmation that RNA quantities can vary by an order of magnitude, and I am guessing this factor of 10 is the basis for the range provided in the ENCODE PNAS paper (0.5 to 5 FPKM).</span></p>
<p>In order to determine the relative importance of the denominator in <img src='http://s0.wp.com/latex.php?latex=FPKM_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='FPKM_i' title='FPKM_i' class='latex' /> I looked at a few RNA-Seq datasets we are currently examining. In the GEUVADIS data, the weighted average can vary by as much as 20% between samples. In a rat RNA-Seq dataset we are analyzing, the difference is a factor of two (and interestingly very dependent on the exact annotation used for quantification). The point here is that even the denominator in <img src='http://s0.wp.com/latex.php?latex=FPKM_i&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='FPKM_i' title='FPKM_i' class='latex' /> <em>does</em> vary, but less, it seems, than the variability in RNA quantity. In other words, the estimate of 0.5 to 5 FPKM corresponding to one transcript per cell is incoherent albeit probably not too far off.</p>
<p>One consequence of all of the above discussion is that while differential analysis of experiments can be performed based on FPKM units (as done for example in Cufflinks, where the normalization factors are appropriately accounted for), it does <em>not</em> make sense to compare raw FPKM values across experiments. This is precisely what is done in Figure 2 of the ENCODE PNAS paper. What the analysis above shows, is that actual abundances may be off by amounts much larger than the differences shown in the figure. In other words, while the caption turns out to to containing an interesting comment the overall figure doesn&#8217;t really make sense. Specifically, I&#8217;m not sure the relative RPKM values shown in the figure deliver the correct relative amounts, an issue that ENCODE can and should check. Which brings me to the last part of this post&#8230;</p>
<p><b>What is ENCODE doing?</b></p>
<p>Having realized the possible issue with RPKM comparisons in Figure 2, I took a look at Figure 3 to try to understand whether there were potential implications for it as well. That exercise took me to a whole other level of ENCODEness. To begin with, I was trying to make sense of the <em>x-</em>axis, which is labeled &#8220;biochemical signal strength (log10)&#8221; when I realized that the different curves on the plot all come from different, completely unrelated <em>x-</em>axes. If this sounds confusing, it is. The green curves are showing graphs of functions whose domain is in log 10 RPKM units. However the histone modification curves are in <em>log (-10 log p), </em>where <em>p</em> is a <em>p</em>-value that has been computed. I&#8217;ve never seen anyone plot log(log(p-values)); what does it mean?! Nor do I understand how such graphs can be placed on a common x-axis (?!). What is &#8220;biochemical signal strength&#8221; (?) Why in the bottom panel is the grey H3K9me3 showing %nucleotides conserved <em>decreasing </em>as &#8220;biochemical strength&#8221; is increasing (?!) Why is the green RNA curves showing conservation <em>below</em> genome average for low expressed transcripts (?!) and why in the top panel is the red H3K4me3 an &#8220;M&#8221; shape (?!) What does any of this mean (?!) What I&#8217;m supposed to understand from it, or frankly, what is going on at all ??? I know many of the authors of this ENCODE PNAS paper and I simply cannot believe they saw and approved this figure. It is truly beyond belief&#8230; see below:</p>
<p><a href="https://liorpachter.files.wordpress.com/2014/04/encode_pnas_fig3.jpg"><img class="aligncenter size-full wp-image-2735" src="https://liorpachter.files.wordpress.com/2014/04/encode_pnas_fig3.jpg?w=490&#038;h=487" alt="ENCODE_PNAS_Fig3" width="490" height="487" /></a></p>
<p>All of these figures are of course to support the main point of the paper. Which is that even though <a href="http://blogs.discovermagazine.com/notrocketscience/2012/09/05/encode-the-rough-guide-to-the-human-genome/#ENCODEfunctional" target="_blank">80% of the genome is functional</a> it is also true that<a href="https://www.youtube.com/watch?v=s7_UtWJfFBY" target="_blank"> this is not what was meant to be said</a> , and that what is true is that &#8220;survey of biochemical activity led to a significant increase in genome coverage and thus accentuated the discrepancy between biochemical and evolutionary estimates&#8230; where function is ascertained independently of cellular state but is dependent on environment and evolutionary niche therefore resulting in estimates that  differ widely in their false-positive and false-negative rates and the resolution with which elements can be defined&#8230; [unlike] genetic approaches that rely on sequence alterations to establish the biological relevance of a DNA segment and are often considered a gold standard for defining function.&#8221;</p>
<p><a href="http://judgestarling.tumblr.com/post/83591181083/encode-nih-in-pnas-2014-in-2012-the-dog-ate-our-lab" target="_blank">Alright then</a>.</p>
<p>The ENCODE PNAS paper was first published behind a paywall. However after some public criticism, the authors relented and paid for it to be open access. This was a mistake. Had it remained behind a paywall not only would the consortium have saved money, I and others might have been spared the experience of reading the paper. I hope the consortium will afford me the courtesy of paywall next time.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2683"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2683"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2683"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/?share=facebook" title="Share on Facebook" id="sharing-facebook-2683"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2683-53b3e8577fea7' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2683&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2683-53b3e8577fea7' data-name='like-post-frame-12758541-2683-53b3e8577fea7'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2656 post type-post status-publish format-standard hentry category-sophistry tag-cannabis tag-causality tag-correlation tag-hans-breiter tag-marijuana tag-multiple-testing tag-p-hacking tag-study-design tag-trend-towards-significance entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2656"><a href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/" rel="bookmark">Does researching casual marijuana use cause brain&nbsp;abnormalities?</a></h2>
			<p class="post-metadata">April 17, 2014 in <a href="http://liorpachter.wordpress.com/category/sophistry/" rel="category tag">sophistry</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/cannabis/" rel="tag">cannabis</a>, <a href="http://liorpachter.wordpress.com/tag/causality/" rel="tag">causality</a>, <a href="http://liorpachter.wordpress.com/tag/correlation/" rel="tag">correlation</a>, <a href="http://liorpachter.wordpress.com/tag/hans-breiter/" rel="tag">Hans Breiter</a>, <a href="http://liorpachter.wordpress.com/tag/marijuana/" rel="tag">marijuana</a>, <a href="http://liorpachter.wordpress.com/tag/multiple-testing/" rel="tag">multiple testing</a>, <a href="http://liorpachter.wordpress.com/tag/p-hacking/" rel="tag">p-hacking</a>, <a href="http://liorpachter.wordpress.com/tag/study-design/" rel="tag">study design</a>, <a href="http://liorpachter.wordpress.com/tag/trend-towards-significance/" rel="tag">trend towards significance</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/#comments" title="Comment on Does researching casual marijuana use cause brain&nbsp;abnormalities?">80 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>In reading the news yesterday I came across multiple reports claiming that <a href="http://www.washingtonpost.com/news/morning-mix/wp/2014/04/16/even-casually-smoking-marijuana-can-change-your-brain-study-says/?tid=pm_national_pop" target="_blank">even casually smoking marijuana can change your brain.</a> I usually don&#8217;t pay much attention to such articles; <strong>I&#8217;ve never smoked a joint in my life. In fact, I&#8217;ve never even smoked a cigarette. </strong>So even though as a scientist I&#8217;ve been interested in cannabis from the molecular biology point of view, and as a citizen from a legal point of view, the issues have not been personal. However reading a <a href="http://www.usatoday.com/story/news/nation/2014/04/15/marijuana-brain-changes/7749309/" target="_blank">USA Today article</a> about the paper, I noticed that the principal investigator Hans Breiter was claiming to be a psychiatrist and <em>mathematician</em>. That is an unusual combination so I decided to take a closer look. I immediately found out the claim was a lie. In fact, the totality of math credentials of Hans Breiter consist of some <a href="http://health.usnews.com/doctors/hans-breiter-278814" target="_blank">logic/philosophy courses during a year abroad</a> at St. Andrews while he was a pre-med student at Northwestern. Even being an undergraduate major in mathematics does not make one a mathematician, just as being an undergraduate major in biology does not makes one a doctor. Thus, with his outlandish claim, Hans Breiter had succeeded in personally offending me! So, I decided to take a look at his paper underlying the multiple news reports:</p>
<ul>
<li>J.M. Gilman <em>et al.</em>, <a href="http://www.jneurosci.org/content/34/16/5529.abstract" target="_blank">Cannabis Use Is Quantitatively Associated with Nucleus Accumbens and Amygdala Abnormalities in Young Adult Recreational Users</a>, Journal of Neuroscience (Neurobiology of Disease section), 34 (2014), 5529&#8211;5538.</li>
</ul>
<p><strong>This is quite possibly the worst paper I&#8217;ve read all year</strong> (as some of my <a title="The network nonsense of Manolis Kellis" href="http://liorpachter.wordpress.com/2014/02/11/the-network-nonsense-of-manolis-kellis/" target="_blank">previous blog posts</a> show I am saying something with this statement). Here is a breakdown of some of the issues with the paper:</p>
<h3>1. Study design</h3>
<p>First of all, the study has a very small sample size, with only 20 &#8220;cases&#8221; (marijuana users), a fact that is important to keep in mind in what follows. The title uses the term &#8220;recreational users&#8221; to describe them, and in the press release accompanying the article Breiter says that &#8220;Some of these people only used marijuana to get high once or twice a week. People think a little recreational use shouldn&#8217;t cause a problem, if someone is doing OK with work or school. Our data directly says this is not the case.&#8221; In fact,<strong> the majority of users in the study were smoking more than 10 joints per week. </strong>There is even a person in the study smoking more than 30 joints per week (as disclosed above, I&#8217;m not an expert on this stuff but if 30 joints per week is &#8220;recreation&#8221; then it seems to me that person is having a lot of fun). More importantly, <strong>Breiter&#8217;s statement in the press release is a lie</strong>. There is no evidence in the paper whatsoever, not even a tiny shred, that the users who were getting high once or twice a week were having any problems. There are also other issues with the study design. For example, the paper claims the users are not &#8220;abusing&#8221; other drugs, but it is quite possible that they are getting high on cocaine, heroin, or ??? as well, an issue that could quite possibly affect the study. The experiment consisted of an MRI scan of each user/control, but only a single scan was done. Given the variability in MRI scans this also seems problematic.</p>
<h3>2. Multiple testing</h3>
<p>The study looked at three aspects of brain morphometry in the study participants: gray matter density, volume and shape. Each of these morphometric analyses constituted multiple tests. In the case of gray matter density, estimates were based on small clusters of voxels, resulting in 123 tests (association of each voxel cluster with marijuana use). Volumes were estimated for four regions: left and right nucleus accumbens and amygdala. Shape was also tested in the same four regions. What the authors should have done is to correct the <em>p</em>-values computed for each of these tests by accounting for the <em>total </em>number of tests performed. Instead, (Bonferroni) corrections were performed separately for each type of analysis. For example, in the volume analysis <em>p</em>-values were required to be less than 0.0125 = 0.05/4. In other words, <strong>the extent of testing was not properly accounted for</strong>. Even so, many of the results were <strong>not</strong> significant. For example, the volume analysis showed no significant association for any of the four tested regions. The best case was the left nucleus accumbens (Figure 1C) with a corrected <em>p</em>-value of 0.015 which is over the authors&#8217; own stated required threshold of 0.0125 (see caption). They use the language &#8220;The association with drug use, after correcting for 4 comparisons, was determined to be a <a href="http://mchankins.wordpress.com/2013/04/21/still-not-significant-2/" target="_blank">trend toward significance</a>&#8221; to describe this non-effect. It is worth noting that the removal of the outlier at a volume of over <img src='http://s0.wp.com/latex.php?latex=800+mm%5E3&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='800 mm^3' title='800 mm^3' class='latex' /> would almost certainly flatten the line altogether and remove even the slight effect. It would have been nice to test this hypothesis but <strong>the authors did not release any of their data</strong>.</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/04/fig1c_cannabis.jpg"><img class="aligncenter size-full wp-image-2666" src="http://liorpachter.files.wordpress.com/2014/04/fig1c_cannabis.jpg?w=490" alt="Fig1c_cannabis"   /></a></p>
<p style="text-align:center;">Figure 1c.</p>
<p>In the Fox News article about the paper, Breiter is quoted saying &#8220;<span style="color:#222222;">“For the NAC [nucleus accumbens], all three measures were abnormal, and they were abnormal in a dose-dependent way, meaning the changes were greater with the amount of marijuana used,” Breiter said.  “The amygdala had abnormalities for shape and density, and only volume correlated with use.  But if you looked at all three types of measures, it showed the relationships between them were quite abnormal in the marijuana users, compared to the normal controls.&#8221; The result above shows this to be a lie. Volume did not significantly correlate with use.</span></p>
<p>This is all very bad, but things get uglier the more one looks at the paper. In the tables reporting the <em>p</em>-values, the authors do something I have never seen before in a published paper. <strong>They report the <em>uncorrected</em> <em>p</em>-values, indicating those that are significant (prior to correction) in boldface, and then put an asterisk next to those that are significant after their (incomplete) correction</strong>. I realize my own use of boldface is controversial&#8230; but what they are doing is truly insane. The fact that they put an asterisk next to the values significant after correction indicates they are aware that multiple testing is required. So why bother boldfacing <i>p</i>-values that they know are not significant? The overall effect is an impression that more tests are significant than is actually the case. See for yourself in their Table 4:</p>
<p style="text-align:center;"><a href="http://liorpachter.files.wordpress.com/2014/04/table4_cannabis.jpg"><img class="aligncenter size-full wp-image-2664" src="http://liorpachter.files.wordpress.com/2014/04/table4_cannabis.jpg?w=490&#038;h=203" alt="Table4_cannabis" width="490" height="203" /></a>Table 4.</p>
<p> The fact that there are multiple columns is also problematic. Separate tests were performed for smoking occasions per day, joints per occasion, joints per week and smoking days per week. These measures are highly correlated, but even so multiply testing them requires multiple test correction. The authors simply didn&#8217;t perform it. They say &#8220;We did not correct for the number of drug use measures because these measures tend not be independent of each other&#8221;. In other words, they multiplied the number of tests by four, and chose to not worry about that. Unbelievable.</p>
<p>Then there is Table 5, where the authors did not report the <em>p</em>-values at all, only whether they were significant or not&#8230; <em>without correction:</em></p>
<p><a href="http://liorpachter.files.wordpress.com/2014/04/table5_cannabis.jpg"><img class="aligncenter size-full wp-image-2673" src="http://liorpachter.files.wordpress.com/2014/04/table5_cannabis.jpg?w=490&#038;h=235" alt="Table5_cannabis" width="490" height="235" /></a></p>
<p style="text-align:center;">Table 5.</p>
<h3>3. Correlation vs. causation</h3>
<p>This issue is one of the oldest in the book. There is even <a href="http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation" target="_blank">a wikipedia entry about it</a>. <strong>Correlation does not imply causation.</strong> Yet despite the fact the every result in the paper is directed at testing for association, in the last sentence of the abstract they say &#8220;These data suggest that marijuana exposure, even in young recreational users, is associated with exposure-dependent alterations of the neural matrix of core reward structures and is consistent with animal studies of changes in dendritic arborization.&#8221; At a minimum, such a result would require doing a longitudinal study. Breiter takes this language to an extreme in the press release accompanying the article. I repeat the statement he made that I quoted above where I boldface the causal claim: &#8220;&#8221;Some of these people only used marijuana to get high once or twice a week. People think <strong>a little recreational use shouldn&#8217;t cause a problem</strong>, if someone is doing OK with work or school. <strong>Our data directly says this is not the case</strong>.&#8221; I believe that scientists should be sanctioned for making public statements that directly contradict the content of their papers, as appears to be the case here. <a href="http://www.washingtonpost.com/national/health-science/the-press-release-crime-of-a-biotech-ceo-and-its-impact-on-scientific-research/2013/09/23/9b4a1a32-007a-11e3-9a3e-916de805f65d_story.html" target="_blank">There is precedent for this</a>.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2656"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2656"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2656"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/?share=facebook" title="Share on Facebook" id="sharing-facebook-2656"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2656-53b3e857880eb' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2656&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2656-53b3e857880eb' data-name='like-post-frame-12758541-2656-53b3e857880eb'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2608 post type-post status-publish format-standard hentry category-reviews tag-phylogenetics tag-satyan-devadoss tag-straight-skeleton entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2608"><a href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/" rel="bookmark">Designing roofs and drawing phylogenetic&nbsp;trees</a></h2>
			<p class="post-metadata">March 25, 2014 in <a href="http://liorpachter.wordpress.com/category/reviews/" rel="category tag">reviews</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/phylogenetics/" rel="tag">phylogenetics</a>, <a href="http://liorpachter.wordpress.com/tag/satyan-devadoss/" rel="tag">Satyan Devadoss</a>, <a href="http://liorpachter.wordpress.com/tag/straight-skeleton/" rel="tag">Straight skeleton</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/#comments" title="Comment on Designing roofs and drawing phylogenetic&nbsp;trees">4 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>A few years ago after the birth of our second daughter and in anticipation of our third, I started designing a one-room addition for our house. One of the problems I faced was figuring out the shape of the roof. I learned of the concept of the <a href="http://en.wikipedia.org/wiki/Straight_skeleton" target="_blank">straight skeleton</a> of a polygon, first defined by Oswin Aichholzer and Franz Aurenhammer in a book chapter &#8220;<a href="http://download.springer.com/static/pdf/323/chp%253A10.1007%252F3-540-61332-3_144.pdf?auth66=1395861501_20c7036eb939135c14e18ddbc3002c13&amp;ext=.pdf" target="_blank">Straight Skeletons for General Polygonal Figures in the Plane</a>&#8221; in 1996. Wikipedia provides an intuitive definition:</p>
<p>&#8220;The straight skeleton of a polygon is defined by a continuous shrinking process in which the edges of the polygon are moved inwards parallel to themselves at a constant speed. As the edges move in this way, the vertices where pairs of edges meet also move, at speeds that depend on the angle of the vertex. If one of these moving vertices collides with a nonadjacent edge, the polygon is split in two by the collision, and the process continues in each part. The straight skeleton is the set of curves traced out by the moving vertices in this process. In the illustration the top figure shows the shrinking process and the middle figure depicts the straight skeleton in blue.&#8221;</p>
<p>but the concept is best understood by picture:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/03/straightskeletondefinition.png"><img class="aligncenter size-full wp-image-2610" src="http://liorpachter.files.wordpress.com/2014/03/straightskeletondefinition.png?w=490" alt="StraightSkeletonDefinition"   /></a></p>
<p>The fact that straight skeletons fit &#8220;symmetrically&#8221; into the polygons that generated them, made me think about whether they could constitute aesthetic representations of phylogenetic trees. So I asked the inverse question: given a phylogenetic tree, i.e. a graph that is a tree with weighted edges, together with a cyclic orientation on its vertices, is there a convex polygon such that the tree is the straight skeleton of that polygon? A few google searches didn&#8217;t reveal anything, but fortunately and coincidentally, <a title="Satyan Devadoss" href="http://web.williams.edu/Mathematics/devadoss/" target="_blank">Satyan Devadoss</a>, who is a topologist and computational geometer was visiting my group on his sabbatical (2009&#8211;2010).</p>
<p>Now, a few years later, Satyan and coauthors have written a paper providing a partial answer to my question. Their paper is about to appear in the next issue of Discrete Applied Mathematics:</p>
<ul>
<li>Howard Cheng, Satyan L. Devadoss, Brian Li, and Andrej Risteski, <a href="http://www.sciencedirect.com/science/article/pii/S0166218X14000171" target="_blank">Skeletal configurations of ribbon trees</a>, <em>Discrete Applied Mathematics</em> 170, 19 June 2014, 46–54.</li>
</ul>
<p>The main theorem is formally about <em>ribbon trees</em><em>:</em></p>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
<p><strong>Definition</strong>. A ribbon tree is a tree (a connected graph with no cycles) for which each edge is assigned a nonnegative length, each internal vertex has degree at least three, and the edges incident to each vertex are cyclically ordered.</p>
<p>The authors prove the interesting result that there exists only a finite set of planar embeddings of a tree appearing as straight skeletons of convex polygons. Specifically, they show that:</p>
<p><strong>Theorem. </strong>A ribbon tree with <em>n</em> leaves has at most <em>2n−5</em> suitable convex polygons.</p>
<p>Its fun to work out by hand the case of a star tree with three leaves:</p>
<p><a href="http://liorpachter.files.wordpress.com/2014/03/traingle_skeleton.jpg"><img class="aligncenter size-full wp-image-2624" src="http://liorpachter.files.wordpress.com/2014/03/traingle_skeleton.jpg?w=490" alt="traingle_skeleton"   /></a></p>
<p>The algebra works out to solving a cubic system of three equations that can be seen to have one unique positive solution (Lemma 5 in the paper).</p>
<p>The proof of the main theorem relies on some elementary trigonometry and algebra, as well as an interesting analogy of Cauchy&#8217;s &#8220;arm lemma&#8221; (if one increases one of the angles of a convex polygonal chain, then the distance between the endpoints will only increase). Furthermore, a few interesting cases and connections are pointed out along the way. For example, some ribbons, even caterpillar ribbons, are not realized by any polygon. There is also a related conference publication by many of the same authors, although including Aichholzer himself, that provides some interesting constructions in special cases:</p>
<ul>
<li>Oswin Aichholzer, Howard Cheng, Satyan L. Devadoss, Thomas Hackl, Stefan Huber, Brian Li, Andrej Risteski, <a href="https://web.williams.edu/Mathematics/devadoss/files/acdhhlr.pdf" target="_blank">What makes a tree a straight skeleton?</a>, Canadian Conference on Computational Geometry, 2012.</li>
</ul>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
<p>Straight skeletons may yet find application in drawing phylogenetic trees, but for now the best out there are <a href="http://download.springer.com/static/pdf/286/chp%253A10.1007%252F11602613_110.pdf?auth66=1395937792_a1f3a09693ed6f1e2ca9b1fb0ee571c8&amp;ext=.pdf" target="_blank">radial or circular representations optimizing various layout considerations</a>.</p>
<p>My addition is now complete and the roof is absolutely beautiful.</p>
<p style="text-align:center;"> <a href="http://liorpachter.files.wordpress.com/2014/03/photo-14.jpg"><img class="aligncenter size-full wp-image-2630" src="http://liorpachter.files.wordpress.com/2014/03/photo-14.jpg?w=490&#038;h=295" alt="photo-14" width="490" height="295" /></a> The roof of the addition</p>
</div>
</div>
</div>
<p>&nbsp;</p>
</div>
</div>
</div>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2608"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2608"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2608"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/?share=facebook" title="Share on Facebook" id="sharing-facebook-2608"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2608-53b3e8578ff2d' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2608&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2608-53b3e8578ff2d' data-name='like-post-frame-12758541-2608-53b3e8578ff2d'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2569 post type-post status-publish format-standard hentry category-expository category-reviews tag-aupr tag-auroc tag-daniel-marbach tag-dream5 tag-magnitude-of-effect tag-manolis-kellis tag-muriel-medard tag-network-deconvolution tag-reproducibility tag-soheil-feizi tag-statistical-significance tag-usability entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2569"><a href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/" rel="bookmark">Reproducibility vs. usability</a></h2>
			<p class="post-metadata">March 18, 2014 in <a href="http://liorpachter.wordpress.com/category/expository/" rel="category tag">expository</a>, <a href="http://liorpachter.wordpress.com/category/reviews/" rel="category tag">reviews</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/aupr/" rel="tag">AUPR</a>, <a href="http://liorpachter.wordpress.com/tag/auroc/" rel="tag">AUROC</a>, <a href="http://liorpachter.wordpress.com/tag/daniel-marbach/" rel="tag">Daniel Marbach</a>, <a href="http://liorpachter.wordpress.com/tag/dream5/" rel="tag">DREAM5</a>, <a href="http://liorpachter.wordpress.com/tag/magnitude-of-effect/" rel="tag">magnitude of effect</a>, <a href="http://liorpachter.wordpress.com/tag/manolis-kellis/" rel="tag">Manolis Kellis</a>, <a href="http://liorpachter.wordpress.com/tag/muriel-medard/" rel="tag">Muriel Médard</a>, <a href="http://liorpachter.wordpress.com/tag/network-deconvolution/" rel="tag">network deconvolution</a>, <a href="http://liorpachter.wordpress.com/tag/reproducibility/" rel="tag">reproducibility</a>, <a href="http://liorpachter.wordpress.com/tag/soheil-feizi/" rel="tag">Soheil Feizi</a>, <a href="http://liorpachter.wordpress.com/tag/statistical-significance/" rel="tag">statistical significance</a>, <a href="http://liorpachter.wordpress.com/tag/usability/" rel="tag">usability</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/#comments" title="Comment on Reproducibility vs. usability">18 comments</a>			</p>
		</div>

		<div class="post-content">
			<p><span style="line-height:1.5em;">Reproducibility has become a major issue in scientific publication, it is <a href="http://reproducibility.cs.arizona.edu" target="_blank">under scrutiny</a> by many, making <a href="http://news.sciencemag.org/asiapacific/2014/03/irreproducibility-dogs-new-reprogramming-method" target="_blank">headlines in the news</a>, is on the <a href="http://www.nature.com/nature/focus/reproducibility/" target="_blank">minds of journals</a>, and there are <a href="http://www.ploscollections.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1003285;jsessionid=19DD6FADBDE5A23504BF6011C3A6FC77" target="_blank">guidelines for achieving it</a>. Reproducibility is certainly important for scientific research, but I think that lost in the debate is the notion of <em>usability</em>. Reproducibility only ensures that the results of a paper can be recreated by others, but usability ensures that researchers can build on scientific work, and explore hypotheses and ideas unforeseen by authors of the original papers. Here I describe a case study in reproducibility and usability, that emerged from </span><span style="line-height:1.5em;">a </span><a style="line-height:1.5em;" title="The network nonsense of Manolis Kellis" href="http://liorpachter.wordpress.com/2014/02/11/the-network-nonsense-of-manolis-kellis/">previous post</a><span style="line-height:1.5em;"> I wrote about the paper</span></p>
<ul>
<li>Soheil Feizi, Daniel Marbach, Muriel Médard &amp; Manolis Kellis, <a href="http://www.nature.com/nbt/journal/v31/n8/abs/nbt.2635.html" target="_blank">Network deconvolution as a general method to distinguish direct dependencies in networks</a>, <em>Nature Biotechnology</em> 31(8), 2013, p 726&#8211;733.</li>
</ul>
<p>Feizi <em>et al. </em>describe a method called network deconvolution, that they claim improves the inference results for 8 out of 9 network inference methods, out of the 35 that were tested in the DREAM5 challenge. In DREAM5, participants were asked to examine four chip-based <em>gene x expression</em> matrices, and were also provided a list of transcription factors for each. They were then asked to provide ranked lists of transcription factor &#8211; gene interactions for each of the four datasets. The four datasets consisted of one computer simulation (the &#8220;<em>in silico</em>&#8221; dataset) and expression measurements in  <em>E. coli, S. cerevisiae </em>and <em>S. aureus. </em>The consortium received submission from 29 different groups and ran 6 other &#8220;off-the-shelf&#8221; methods, while also developing its own &#8220;community&#8221; method (for a total of 36=29+6+1). The community method consisted of applying the <a href="http://en.wikipedia.org/wiki/Borda_count" target="_blank">Borda count</a> to the 35 methods being tested, to produce a new consensus, or community, network. <a href="http://math.berkeley.edu/~nbray/" target="_blank">Nicolas Bray</a> and I tried to replicate the results of Feizi <em>et al. </em>so that we could test for ourselves the performance of network deconvolution with different parameters and on other DREAM5 methods ( Feizi <em>et al.</em> tested only 9 methods; there were 36 in total).  But despite contacting the authors for help we were unable to do so. In desperation, I even offered $100 for someone to replicate all of the figures in the paper. Perhaps as a result of my blogging efforts, or possibly due to a spontaneous change of heart, the authors finally released <em>some</em> of the code and data needed to reproduce <em>some</em> of the figures in their paper. In particular, I am pleased to say that the <a href="http://compbio.mit.edu/nd/try_it_out.html" target="_blank">released material</a> is sufficient to almost replicate Figure 2 of their paper which describes their results on a portion of the DREAM5 data. I say almost because the results for one of the networks is off, but to the authors credit it does appear that the distributed data and code are close to what was used to produce the figure in the paper (note: there is still not enough disclosure to replicate <em>all</em> of the figures of the paper, including the suspicious Figure S4 before and after revision of the supplement, and I am therefore not yet willing to concede the $100). What Feizi <em>et al.</em> did accomplish was to make their methods <em>usable</em>. That is to say, with the distributed code and data I was able to test the method with different parameters and on new datasets. In other words, <strong>Feizi <em>et al.</em> is still not completely reproducible, but it is usable. </strong>In this post, I&#8217;ll demonstrate why usability is important, and make the case that it is too frequently overlooked or confused with reproducibility. With usable network deconvolution code in hand, I was finally able to test some of the claims of Feizi <em>et al.  </em>First, I identify the relationship between the DREAM methods and the methods Feizi <em>et al.</em> applied network deconvolution to. In the figure below, I have reproduced Figure 2 from Feizi <em>et al.</em> together with Figure 2 from Marbach <em>et al.:</em> <a href="http://liorpachter.files.wordpress.com/2014/03/figure1_dream_nd.jpg"><img class="aligncenter" src="http://liorpachter.files.wordpress.com/2014/03/figure1_dream_nd.jpg?w=490&#038;h=270" alt="Figure1_DREAM_ND" width="490" height="270" /></a></p>
<p style="text-align:center;">Figure 2 from Feizi <em>et al.</em> aligned to Figure 2 from Marbach <em>et al.</em></p>
<p>The mapping is more complex than appears at first sight. For example, in the case of Spearman correlation (method Corr #2 in Marbach <em>et al., </em>method #5 in Feizi <em>et al.</em>), Feizi <em>et al.</em> ran network deconvolution on the method after taking absolute values. This makes no sense, as throwing away the sign is to throw away a significant amount of information, not to mention it destroys any hope of connecting the approach to the intuition of inferring directed interactions from the observed via the idealized &#8220;model&#8221; described in the paper. On the other hand, Marbach <em>et al.</em> evaluated Spearman correlation <em>with</em> sign. Without taking the absolute value before evaluation negative edges, strong (negative) interactions, are ignored. This is the reason for the very poor performance of Spearman correlation and the reason for the discrepancy in bar heights between Marbach <em>et al.</em><em> </em>and Feizi <em>et al.</em> for that method. The caption of Figure 2 in Feizi <em>et al.</em> begins &#8220;Network deconvolution applied to the inferred networks of top-scoring methods [1] from DREAM5..&#8221; This is obviously not true. Moreover, one network they did not test on was the community network of Marbach <em>et al.</em> which was the best method and the point of the whole paper. However the methods they did test on were ranked 2,3,4,6,8,12,14,16,28 (out of 36 methods). The 10th &#8220;community&#8221; method of Feizi <em>et al.</em> is actually the result of applying the community approach to the ND output from all the methods, so it is not in and of itself a test of ND. Of the nine tested methods, arguably only a handful were &#8220;top&#8221; methods. I do think its sensible to consider &#8220;top&#8221; to be the best methods for each category (although Correlation is so poor I would discard it altogether). That leaves four top methods. So instead of creating the illusion that network deconvolution improves 9/10 top scoring methods, what Feizi <em>et al.</em> should have reported is that <strong>3 out of 4 of the top methods that were tested were improved by network deconvolution</strong>. That is the result of running network deconvolution with the default parameters. I was curious what happens when using the parameters that Feizi <em>et al.</em> applied to the protein interaction data (alpha=1, beta=0.99). Fortunately, because they have made the code usable, I was able to test this. The overall result as well as the scores on the individual datasets are shown below: <a href="http://liorpachter.files.wordpress.com/2014/03/reverse_params.png"><img class="aligncenter" src="http://liorpachter.files.wordpress.com/2014/03/reverse_params.png?w=490&#038;h=447" alt="protein params" width="490" height="447" /></a> The Feizi <em>et al. </em>results on gene regulatory networks using parameters different from the default. The results are very different. Despite the claims of Feizi <em>et al.</em> that network deconvolution is robust to choice of parameters,<strong> now only 1 out of 4 of the top methods are improved by network deconvolution</strong>. Strikingly, the top three methods tested have their quality degraded. In fact, the top method in two out of the three datasets tested is made worse by network deconvolution. <strong>Network deconvolution is certainly not robust to parameter choice. </strong>What was surprising to me was the <em>improved</em> performance of network deconvolution on the <em>S. cerevisae</em> dataset, especially for the mutual information and correlation methods. In fact, the improvement of network deconvolution over the methods is appears extraordinary. At this point I started to wonder about what the improvements really mean, i.e. what is the &#8220;score&#8221; that is being measured. The y-axis, called the &#8220;score&#8221; by Feizi <em>et al.</em> and Marbach <em>et al.</em> seemed to be changing drastically between runs. I wondered&#8230; what exactly is the score? What do the improvements mean? It turns out that &#8220;score&#8221; is defined as follows:</p>
<p style="text-align:center;"><img src='http://s0.wp.com/latex.php?latex=score+%3D+%5Cfrac%7B1%7D%7B2%7D+%28+%5Cfrac%7B1%7D%7B3%7D+%5Csum_%7Bi%3D1%7D%5E3+-log_%7B10%7D+p_%7BAUROC%2Ci%7D+%2B+%5Cfrac%7B1%7D%7B3%7D+%5Csum_%7Bi%3D1%7D%5E3+-log_%7B10%7D+p_%7BAUPR%2Ci%7D%29&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='score = &#92;frac{1}{2} ( &#92;frac{1}{3} &#92;sum_{i=1}^3 -log_{10} p_{AUROC,i} + &#92;frac{1}{3} &#92;sum_{i=1}^3 -log_{10} p_{AUPR,i})' title='score = &#92;frac{1}{2} ( &#92;frac{1}{3} &#92;sum_{i=1}^3 -log_{10} p_{AUROC,i} + &#92;frac{1}{3} &#92;sum_{i=1}^3 -log_{10} p_{AUPR,i})' class='latex' />.</p>
<p>This formula requires some untangling: First of all, AUROC is shorthand for area under the <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank">ROC</a> (receiver operator curve), and AUPR for area under the PR (precision recall) curve. For context, ROC is a standard concept in engineering/statistics. Precision and recall are used frequently, but the PR curve is used much less than ROC . Both are measures for judging the quality of a binary classifier. In the DREAM5 setting, this means the following: there is a gold standard of &#8220;positives&#8221;, namely a set of edges in a network that should be predicted by a method, and the remainder of the edges will be considered &#8220;negatives&#8221;, i.e. they should not be predicted. A method generates a list of edges, sorted (ranked) in some way. As one proceeds through the list, one can measure the fraction of positives and false positives predicted. The ROC and PR curves measure the performance. A ROC is simply a plot showing the true positive rate for a method as a function of the false positive rate. Suppose that there are <em>m </em>positives in the gold standard out of a goal of <em>n </em>edges. If one examines the top <i>k</i> predictions of a method, then among them there will be <em>t</em><em> &#8220;</em>true&#8221; positives as well as <em>k</em><em>-t &#8220;</em>false&#8221; positives. This will result in a single point on the ROC, i.e. the point (<img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7Bk-t%7D%7Bn-m%7D%2C%5Cfrac%7Bt%7D%7Bm%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{k-t}{n-m},&#92;frac{t}{m}' title='&#92;frac{k-t}{n-m},&#92;frac{t}{m}' class='latex' />). This can be confusing at first glance for a number of reasons. First, the points do not necessarily form a function, e.g. there can be points with the same <em>x-</em>coordinate. Second, as one varies <em>k </em>one obtains a set of points, not a curve. The ROC is a <em>curve,</em> and<em> </em>is obtained by taking the envelope of all of the points for <img src='http://s0.wp.com/latex.php?latex=k+%5Cin+%5C%7B1%2C%5Cldots%2Cn%5C%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='k &#92;in &#92;{1,&#92;ldots,n&#92;}' title='k &#92;in &#92;{1,&#92;ldots,n&#92;}' class='latex' />. The following intuition is helpful in understanding ROC:</p>
<ol>
<li>The <em>x</em> coordinate in the ROC is the false positive rate. If one doesn&#8217;t make any predictions of edges at all, then the false positive rate is 0 (in the notation above <em>k=0, t=0). </em>On the other hand, if all edges are considered to be &#8220;true&#8221;, then the false positive rate is 1 and the corresponding point on the ROC is (1,1), which corresponds to <em>k=n, t=m</em>.</li>
<li>If a method has no predictive power, i.e. the ranking of the edges tells you nothing about which edges really are true, then the ROC is the line <em>y=x. </em>This is because lack of predictive power means that truncating the list at any <em>k</em>, results in the same proportion of true positives above and below the <em>k</em>th edge. And a simple calculation shows that this will correspond to the point (<img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7Bk%7D%7Bn%7D%2C%7Bk%7D%7Bn%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{k}{n},{k}{n}' title='&#92;frac{k}{n},{k}{n}' class='latex' />) on the ROC curve.</li>
<li>ROC curves can be summarized by a single number that has meaning: the area under the ROC (AUROC). The observation above means that a method without any predictive power will have an AUROC of 1/2. Similarly, a &#8220;perfect&#8221; method, where he true edges are all ranked at the top will have an AUROC of 1. AUROC is widely used to summarize the content of a ROC curve because it has an intuitive meaning: the AUROC is the probability that if a positive and a negative edge are each picked at random from the list of edges, the positive will rank higher than the negative.</li>
</ol>
<p>An alternative to ROC is the precision-recall curve. Precision, in the mathematics notation above, is the value <img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7Bt%7D%7Bk%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{t}{k}' title='&#92;frac{t}{k}' class='latex' />, i.e., the number of true positives divided by the number of true positives plus false positives. Recall is the same as sensitivity, or true positive rate: it is <img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7Bt%7D%7Bm%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{t}{m}' title='&#92;frac{t}{m}' class='latex' />. In other words, the PR curve contains the points (<img src='http://s0.wp.com/latex.php?latex=%5Cfrac%7Bt%7D%7Bm%7D%2C%5Cfrac%7Bt%7D%7Bk%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;frac{t}{m},&#92;frac{t}{k}' title='&#92;frac{t}{m},&#92;frac{t}{k}' class='latex' />), as recall is usually plotted on the <em>x-</em>axis. The area under the precision-recall curve (AUPR) has an intuitive meaning just like AUROC. It is the average of precision across all recall values, or alternatively, the probability that if a &#8220;positive&#8221; edge is selected from the ranked list of the method, then an edge above it on the list will be &#8220;positive&#8221;. Neither precision-recall curves, nor AUPR are widely used. There is one problem with AUPR, which is that its value is dependent on the number of positive examples in the dataset. For this reason, it doesn&#8217;t make sense to average AUPR across datasets (while it does make sense for AUROC). For all of these reasons, I&#8217;m slightly uncomfortable with AUPR but that is not the main issue in the DREAM5 analysis. I have included an example of ROC and PR curves below. I generated them for the method &#8220;GENIE3&#8243; tested by Feizi <em>et al.</em>. This was the method with the best overall score. The figure below is for the <em>S. cerevisiae </em>dataset: <a href="http://liorpachter.files.wordpress.com/2014/03/four_panel1.png"><img class="aligncenter size-full wp-image-2583" src="http://liorpachter.files.wordpress.com/2014/03/four_panel1.png?w=490&#038;h=392" alt="ROC and PR before and after network deconvolution" width="490" height="392" /></a></p>
<p style="text-align:center;">The ROC and a PR curves before (top) and after (bottom) applying network deconvolution to the GENIE3 network.</p>
<p>The red curve in the ROC plots is what one would see for a method without any predictive power (point #2 above). In this case, what the plot shows is that GENIE3 is effectively ranking the edges of the network randomly. The PR curve is showing that at all recall levels there is very little precision. <strong>The difference between GENIE3 before and after network deconvolution is so small, that it is indistinguishable in the plots. </strong>I had to create separate plots before and after network deconvolution because the curves literally overlapped and were not visible together. <strong>The conclusion from plots such as these, should not be that there is statistically significance (in the difference between methods with/without network deconvolution, or in comparison to random), but rather that there is negligible effect.</strong> There is a final ingredient that is needed to constitute &#8220;score&#8221;. Instead of just averaging AUROC and AUPR, both are first converted into <em>p</em>-values that measure the statistical significance of the method being different from random. The way this was done was to create random networks from the edges of the 35 methods, and then to measure their quality (by AUROC or AUPR) to obtain a distribution. The <em>p-</em>value for a given method was then taken to be the area under the probability density function to the right of the method&#8217;s value. The graph below shows the pdf for AUROC from the <em>S. cerevisae </em>DREAM5 data that was used by Feizi <em>et al. </em>to generate the scores:</p>
<p style="text-align:center;"><a href="http://liorpachter.files.wordpress.com/2014/03/pdf_auroc.png"><img src="http://liorpachter.files.wordpress.com/2014/03/pdf_auroc.png?w=490&#038;h=367" alt="pdf_AUROC" width="490" height="367" /></a></p>
<p style="text-align:center;">Distribution of AUROC for random methods generated from the <em>S. cerevisiae </em>submissions in Marbach <em>et al.</em></p>
<p>In other words, almost all random methods had an AUROC of around 0.51, so <strong>any slight deviation from that was magnified in the computation of <em>p-</em>value, and then by taking the (negative) logarithm of that number a very high &#8220;score&#8221; was produced</strong>. The scores were then taken to be the average of the AUROC and AUPR scores. I can understand why Feizi <em>et al.</em> might be curious whether the difference between a method&#8217;s performance (before and after network deconvolution) is significantly different from random, but to <a title="Magnitude of effect vs. statistical significance" href="http://liorpachter.wordpress.com/2013/08/26/magnitude-of-effect-vs-statistical-significance/">replace magnitude of effect with statistical significance</a> in this setting, with such small effect sizes, is to completely mask the fact that the methods are hardly distinguishable from random in the first place. To make concrete the implication of reporting the statistical significance instead of effect size, I examined the &#8220;significant&#8221; improvement of network deconvolution on the <em>S. cerevisae </em>and other datasets when run with the protein parameters rather than the default (second figure above). Below I show the AUROC and AUPR plots for the dataset.</p>
<p style="text-align:center;"><a href="http://liorpachter.files.wordpress.com/2014/03/auroc_nd_proteinparams.png"><img src="http://liorpachter.files.wordpress.com/2014/03/auroc_nd_proteinparams.png?w=490&#038;h=432" alt="AUROC_ND_proteinparams" width="490" height="432" /></a></p>
<p style="text-align:center;">The Feizi <em>et al.</em> results before and after network deconvolution using alpha=1, beta=0.99 (shown with AUROC).</p>
<p style="text-align:center;"><a href="http://liorpachter.files.wordpress.com/2014/03/aupr_nd_proteinparams.png"><img src="http://liorpachter.files.wordpress.com/2014/03/aupr_nd_proteinparams.png?w=490&#038;h=539" alt="AUPR_ND_proteinparams" width="490" height="539" /></a></p>
<p style="text-align:center;">The Feizi <em>et al.</em> results before and after network deconvolution using alpha=1, beta=0.99 (shown with AUPR).</p>
<p>My conclusion was that <strong>the use of &#8220;score&#8221; was basically a red herring</strong>. <strong>What looked like major differences between methods disappears into tiny effects in the experimental datasets, and even the <em>in silico </em>variations are greatly diminished</strong>. The differences in AUROC of one part in 1000 hardly seem reasonable for concluding that network deconvolution works. Biologically, both results are that the methods cannot reliably predict edges in the network. With usable network deconvolution code at hand, I was curious about one final question. The main result of the DREAM5 paper</p>
<ul>
<li>D. Marbach <em>et al.</em>, <a href="http://www.nature.com/nmeth/journal/v9/n8/abs/nmeth.2016.html" target="_blank">Wisdom of Crowds for Robust Gene Network Inference,</a> Nature Methods 9 (2012), 796&#8211;804.</li>
</ul>
<p>was that the community method was best. So I wondered whether network deconvolution would improve it. In particular, the community result shown in Feizi <em>et al. </em>was not a test of network deconvolution, it was simply a construction of the community from the 9 methods tested (two communities were constructed, one before and one after network deconvolution). To perform the test, I went to examine the DREAM5 data, available as supplementary material with the paper. I was extremely impressed with reproducibility. The participant submissions are all available, together with scripts that can be used to quickly obtain the results of the paper. H<span style="line-height:1.5em;">owever the data is not very usable. For example, what is provided is the top 100,000 edges </span>that<span style="line-height:1.5em;"> each method produced. But if one wants to use the full prediction of a method, it is not available. The implication of this in the context of network deconvolution is that it is not possible to test network deconvolution on the DREAM5 data without thresholding. Furthermore, in order to evaluate edges absolute value was applied to all the edge weights. Again, this makes the data much less useful for further experiments one may wish to conduct. In other words, DREAM5 is reproducible but not very usable.</span> But since Feizi <em>et al.</em> suggest that network deconvolution can literally be run on anything with &#8220;indirect effect&#8221;, I decided to give it a spin. I did have to threshold the input (although fortunately, Feizi <em>et al.</em> have assured us that this is a fine way to run network deconvolution), so actually the experiment is entirely reasonable in terms of their paper. The figure is below (produced with the default network deconvolution parameters),  but before looking at it, please accept my apology for making it. I really think its the most incoherent, illogical, meaningless and misleading figure I&#8217;ve ever made. But it does abide by the spirit of network deconvolution: <a href="http://liorpachter.files.wordpress.com/2014/03/nd_dream5_final.png"><img class="aligncenter" src="http://liorpachter.files.wordpress.com/2014/03/nd_dream5_final.png?w=490&#038;h=442" alt="ND_DREAM5_final" width="490" height="442" /></a></p>
<p style="text-align:center;">The DREAM5 results before and after network deconvolution.</p>
<p>Alas, <strong>network deconvolution <em>decreases</em> the quality of the best method, namely the community method</strong>. <strong>The wise crowds have been dumbed down. </strong>In fact, 19/36 methods become worse, 4 stay the same, and only 13 improve. Moreover, network deconvolution decreases the quality of the top method in each dataset. The only methods with consistent improvements when network deconvolution is applied are the mutual information and correlation methods, poor performers overall, that Feizi <em>et al.</em> ended up focusing on. I will acknowledge that one complaint (of the many possible) about my plot is that the overall results are dominated by the <em>in silico</em> dataset. True- and I&#8217;ve tried to emphasize that by setting the y-axis to be the same in each dataset (unlike Feizi <em>et al.</em>) But I think its clear that no matter how the datasets are combined into an overall score, the result is that network deconvolution is <strong>not</strong> consistently improving methods. All of the analyses I&#8217;ve done were made possible thanks to the improved usability of network deconvolution. It is unfortunate that the result of the analyses is that network deconvolution should not be used. Still, I think this examples makes a good case for the fact that <strong>r</strong><span style="line-height:1.5em;"><strong>eproducibility is essential, but usability is more important.</strong> </span></p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2569"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2569"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2569"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/?share=facebook" title="Share on Facebook" id="sharing-facebook-2569"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2569-53b3e857a1bd6' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2569&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2569-53b3e857a1bd6' data-name='like-post-frame-12758541-2569-53b3e857a1bd6'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2448 post type-post status-publish format-standard hentry category-guest-post tag-anova tag-jingyi-jessica-li tag-mark-biggin tag-peter-bickel tag-schwanhausser tag-transcription tag-translation entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2448"><a href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/" rel="bookmark">Using statistical methods to estimate and take into account experimental measurement errors: a case study using high throughput proteomics&nbsp;data</a></h2>
			<p class="post-metadata">March 1, 2014 in <a href="http://liorpachter.wordpress.com/category/guest-post/" rel="category tag">guest post</a> | Tags: <a href="http://liorpachter.wordpress.com/tag/anova/" rel="tag">ANOVA</a>, <a href="http://liorpachter.wordpress.com/tag/jingyi-jessica-li/" rel="tag">Jingyi Jessica Li</a>, <a href="http://liorpachter.wordpress.com/tag/mark-biggin/" rel="tag">Mark Biggin</a>, <a href="http://liorpachter.wordpress.com/tag/peter-bickel/" rel="tag">Peter Bickel</a>, <a href="http://liorpachter.wordpress.com/tag/schwanhausser/" rel="tag">Schwanhäusser</a>, <a href="http://liorpachter.wordpress.com/tag/transcription/" rel="tag">transcription</a>, <a href="http://liorpachter.wordpress.com/tag/translation/" rel="tag">translation</a> | by <a href="http://liorpachter.wordpress.com/author/jlistat/" title="Posts by jlistat" rel="author">jlistat</a> | <a href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/#comments" title="Comment on Using statistical methods to estimate and take into account experimental measurement errors: a case study using high throughput proteomics&nbsp;data">5 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>Jingyi Jessica Li and Mark D. Biggin</p>
<p>We published a paper titled “System wide analyses have underestimated protein abundances and the importance of transcription in mammals” in PeerJ on Feb 27, 2014 (<a title="System wide analyses have underestimated protein abundances and the importance of transcription in mammals" href="https://peerj.com/articles/270/" target="_blank">https://peerj.com/articles/270/</a>). In our paper we use statistical methods to reanalyze the data of several proteomics papers to assess the relative importance that each step in gene expression plays in determining the variance in protein amounts expressed by each gene. Historically transcription was viewed as the dominant step. More recently, though, system wide analyses have claimed that translation plays the dominant role and that differences in mRNA expression between genes explain only 10-40% of the differences in protein levels. We find that when measurement errors in mRNA and protein abundance data is taken into account, transcription again appears to be the dominant step.</p>
<p>Our study was initially motivated by our observation that the system wide label-free mass spectrometry data of 61 housekeeping proteins in <a title="Global quantification of mammalian gene expression control" href="http://www.nature.com/nature/journal/v473/n7347/full/nature10098.html" target="_blank">Schwanhäusser et al (2011)</a> have lower expression estimates than their corresponding individual protein measurements based on SILAC mass spectrometry or western blot data. The underestimation bias is especially obvious for proteins with expression levels lower than 10<sup>6</sup> molecules per cell. We therefore corrected this non linear bias to determine how more accurately scaled data impacts the relationship between protein and mRNA abundance data. We found that a two-part spline model fits well on the 61 housing keep protein data and applied this model to correct the system-wide protein abundance estimates in <a title="Global quantification of mammalian gene expression control" href="http://www.nature.com/nature/journal/v473/n7347/full/nature10098.html" target="_blank">Schwanhäusser et al (2011)</a>. After this correction, our corrected protein abundance estimates show a significantly higher correlation with mRNA abundances than do the uncorrected protein data.</p>
<p>We then investigated if other sources of experimental error could further explain the relatively poor correlation between protein and mRNA levels. We employed two strategies that both use Analysis of Variance (ANOVA) to determine the percent of the variation in measured protein expression levels that is due to each of the four steps: transcription, mRNA degradation, translation, and protein degradation, as well as estimating the measurement errors in each step. ANOVA is a classic statistical method developed by RA Fisher in the 1920s. Despite the fact that this is a well-regarded and standard approach in some fields, its usefulness has not been widely appreciated in genomics and proteomics. In our first strategy, we estimated the variances of errors in mRNA and protein abundances using direct experimental measurements provided by control experiments in the Schwanhäusser et al. paper. Plugging these variances into ANOVA, we found that mRNA levels explain at least 56% of the differences in protein abundance for the 4,212 genes detected by <a title="Global quantification of mammalian gene expression control" href="http://www.nature.com/nature/journal/v473/n7347/full/nature10098.html" target="_blank">Schwänhausser et al (2011)</a>. However,  because one major source of error—systematic error of protein measurements—could not be estimated, the true percent contribution of mRNA to protein expression should be higher. We also employed a second, independent strategy to determine the contribution of mRNA levels to protein expression. We show that the variance in translation rates directly measured by ribosome profiling is only 12% of that inferred by <a title="Global quantification of mammalian gene expression control" href="http://www.nature.com/nature/journal/v473/n7347/full/nature10098.html" target="_blank">Schwanhäusser et al (2011)</a>, and that the measured and inferred translation rates correlate poorly. Based on this, our second strategy suggests that mRNA levels explain ∼81% of the variance in protein levels. While the magnitudes of our two estimates vary, they both suggest that transcription plays a more important role than the earlier studies implied and translation a much smaller role.</p>
<p>Finally, we noted that all of the published estimates, as welll as ours given above, only apply to those genes whose mRNA and protein expression was detected. Based on a detailed analysis by <a title="Duel of the fates: the role of transcriptional circuits and noise in CD4+ cells" href="http://www.sciencedirect.com/science/article/pii/S0955067412000385" target="_blank">Hebenstreit et al. (2012)</a>, we estimate that approximately 40% of genes in a given cell within a population express no mRNA. Since there can be no translation in the absence of mRNA, we argue that differences in translation rates can play no role in determining the expression levels for the ∼40% of genes that are non-expressed.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2448"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2448"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2448"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/?share=facebook" title="Share on Facebook" id="sharing-facebook-2448"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2448-53b3e857aa79b' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2448&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2448-53b3e857aa79b' data-name='like-post-frame-12758541-2448-53b3e857aa79b'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
	<div class="post-2330 post type-post status-publish format-standard hentry category-expository entry">
		<div class="post-meta">
			<h2 class="post-title" id="post-2330"><a href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/" rel="bookmark">Number deconvolution</a></h2>
			<p class="post-metadata">February 18, 2014 in <a href="http://liorpachter.wordpress.com/category/expository/" rel="category tag">expository</a> | by <a href="http://liorpachter.wordpress.com/author/math4bio/" title="Posts by Lior Pachter" rel="author">Lior Pachter</a> | <a href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/#comments" title="Comment on Number deconvolution">31 comments</a>			</p>
		</div>

		<div class="post-content">
			<p>Given the reaction to our use of the word &#8220;fraud&#8221; in our <a title="The network nonsense of Manolis Kellis" href="http://liorpachter.wordpress.com/2014/02/11/the-network-nonsense-of-manolis-kellis/">blog post on Feizi <em>et al.</em></a>, we would like to remind readers how it was actually used:</p>
<blockquote><p>In academia the word fraudulent is usually reserved for outright forgery. However given what appears to be deliberate hiding, twisting and torturing of the facts by Feizi <em>et al.</em>, we think that fraud (“deception deliberately practiced in order to secure unfair or unlawful gain”) is a reasonable characterization. If the paper had been presented in a straightforward manner, would it have been accepted by Nature Biotechnology?</p></blockquote>
<p dir="ltr"><span style="line-height:1.5em;">While the reaction</span><span style="line-height:1.5em;"> has largely focused on reproducibility and the swapping of figures, we reiterate our stance: misleading one’s readers (and reviewers) is itself a form of scientific fraud. Regarding the other questions raised, the response from Feizi, Marbach, Médard and Kellis falls short. On their new website their code has changed, their explanations are in many cases incoherent, self-contradictory, and make false claims, and the newly added correction to Figure S4 turns out not to explain the difference between the figures in the revisions. We explain all of this below. </span>And yet for us the claim of fraud can stand on the basis of deception alone. The distance between the image created by the main text of the paper and the truth of their method is simply too great.</p>
<p dir="ltr">However, one unfortunate fact is that that judging that distance requires understanding some of the mathematics in the paper, and another common reaction has been “I don’t understand the math.”  For such readers, we explain network deconvolution in simple terms by providing an analogy using numbers instead of matrices. Understanding this requires no more than high school algebra. We follow that with a response to Feizi <em>et al.</em>&#8216;s rebuttal.</p>
<p><span style="text-decoration:underline;"><strong>Network deconvolution math made simple</strong></span></p>
<p>In what follows, <em>number deconvolution,</em> a simplified form of network deconvolution,<em> </em>is explained in red. <em>Ne</em><em>twork deconvolution </em>is explained in blue.</p>
<p><span style="color:#ff0000;"><span style="line-height:1.5em;">The main concept to understand is that of a </span><em style="line-height:1.5em;">geometric</em><span style="line-height:1.5em;"> series. This is a series with a constant ratio between successive terms, for example</span></span></p>
<p><span style="color:#ff0000;"><a name="eqgsbasic"></a></span></p>
<p align="center"><span style="color:#ff0000;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7B1%7D%7B2%7D%2B%5Cfrac%7B1%7D%7B4%7D%2B%5Cfrac%7B1%7D%7B8%7D+%2B+%5Ccdots+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle &#92;frac{1}{2}+&#92;frac{1}{4}+&#92;frac{1}{8} + &#92;cdots &#92; &#92; &#92; &#92; &#92; (1)' title='&#92;displaystyle &#92;frac{1}{2}+&#92;frac{1}{4}+&#92;frac{1}{8} + &#92;cdots &#92; &#92; &#92; &#92; &#92; (1)' class='latex' /></span></p>
<p><span style="color:#ff0000;"><a name="eqgsbasic"></a></span></p>
<p><span style="color:#ff0000;"><a name="eqgsbasic"></a> In this case, each term is one half the previous term, so that the ratio between successive terms is always <img src='http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;frac{1}{2}}' title='{&#92;frac{1}{2}}' class='latex' />. This sum converges to <img src='http://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{1}' title='{1}' class='latex' />, an intuitive notion that is formalized by defining the infinite sum to be</span></p>
<p align="center"><span style="color:#ff0000;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7D+%5Cleft%28+%5Cfrac%7B1%7D%7B2%7D%5Cright%29%5Ei+%3D+lim_%7Bn+%5Crightarrow+%5Cinfty%7D+%5Csum_%7Bi%3D1%7D%5En+%5Cleft%28+%5Cfrac%7B1%7D%7B2%7D+%5Cright%29%5Ei+%3D+1.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle &#92;sum_{i=1}^{&#92;infty} &#92;left( &#92;frac{1}{2}&#92;right)^i = lim_{n &#92;rightarrow &#92;infty} &#92;sum_{i=1}^n &#92;left( &#92;frac{1}{2} &#92;right)^i = 1. &#92; &#92; &#92; &#92; &#92; (2)' title='&#92;displaystyle &#92;sum_{i=1}^{&#92;infty} &#92;left( &#92;frac{1}{2}&#92;right)^i = lim_{n &#92;rightarrow &#92;infty} &#92;sum_{i=1}^n &#92;left( &#92;frac{1}{2} &#92;right)^i = 1. &#92; &#92; &#92; &#92; &#92; (2)' class='latex' /></span></p>
<p><span style="color:#ff0000;">This is the familiar math of the <span style="color:#ff0000;"><a href="http://en.wikipedia.org/wiki/Zeno's_paradoxes" target="_blank">Dichotomy paradox from Zeno&#8217;s paradoxes</a>.</span><br />
</span></p>
<p><span style="color:#ff0000;line-height:1.5em;">It is important to note that not every geometric series converges. If the number <img src='http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;frac{1}{2}}' title='{&#92;frac{1}{2}}' class='latex' /> in the sum above is replaced by <img src='http://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{2}' title='{2}' class='latex' />, then the series <img src='http://s0.wp.com/latex.php?latex=%7B2%2B4%2B8%2B16%2B%5Ccdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{2+4+8+16+&#92;cdots}' title='{2+4+8+16+&#92;cdots}' class='latex' /> is said to diverge. The are two basic questions about geometric series: (1) for which numbers do they converge and (2) when they do converge, what do they converge to. These two questions are answered in the following:</span></p>
<p><span style="color:#ff0000;">Let <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' /> be a real number. The geometric series converges if, and only if, <img src='http://s0.wp.com/latex.php?latex=%7B-1+%3C+a+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-1 &lt; a &lt; 1}' title='{-1 &lt; a &lt; 1}' class='latex' />, in which case <a name="eqconv"></a></span></p>
<p align="center"><span style="color:#ff0000;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+m%3D%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7D+a%5Ei+%3D+a%281-a%29%5E%7B-1%7D.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle m=&#92;sum_{i=1}^{&#92;infty} a^i = a(1-a)^{-1}. &#92; &#92; &#92; &#92; &#92; (3)' title='&#92;displaystyle m=&#92;sum_{i=1}^{&#92;infty} a^i = a(1-a)^{-1}. &#92; &#92; &#92; &#92; &#92; (3)' class='latex' /></span></p>
<p><span style="color:#ff0000;"><a name="eqconv"></a></span></p>
<p><span style="color:#ff0000;"><a name="eqconv"></a> It is not hard to see why this is true:</span></p>
<p align="center"><span style="color:#ff0000;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+m%3Da%2Ba%5E2%2B%5Ccdots+%3D+a%281%2Ba%2Ba%5E2%2B%5Ccdots%29+%3D+a%281%2Bm%29+%5CRightarrow+m%3Da%281-a%29%5E%7B-1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle m=a+a^2+&#92;cdots = a(1+a+a^2+&#92;cdots) = a(1+m) &#92;Rightarrow m=a(1-a)^{-1}. ' title='&#92;displaystyle m=a+a^2+&#92;cdots = a(1+a+a^2+&#92;cdots) = a(1+m) &#92;Rightarrow m=a(1-a)^{-1}. ' class='latex' /></span></p>
<p><span style="color:#ff0000;"><span style="line-height:1.5em;">Returning to the example of <img src='http://s0.wp.com/latex.php?latex=%7Ba%3D%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a=&#92;frac{1}{2}}' title='{a=&#92;frac{1}{2}}' class='latex' />, we see that it is a special case of this result. It converges because <img src='http://s0.wp.com/latex.php?latex=%7B-1+%3C+%5Cfrac%7B1%7D%7B2%7D+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-1 &lt; &#92;frac{1}{2} &lt; 1}' title='{-1 &lt; &#92;frac{1}{2} &lt; 1}' class='latex' />, and furthermore, <img src='http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B%5Cfrac%7B1%7D%7B2%7D%7D%7B1-%5Cfrac%7B1%7D%7B2%7D%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;frac{&#92;frac{1}{2}}{1-&#92;frac{1}{2}} = 1}' title='{&#92;frac{&#92;frac{1}{2}}{1-&#92;frac{1}{2}} = 1}' class='latex' />.</span></span></p>
<p><span style="color:#0000ff;">Matrices behave like numbers in some ways, for example they can be added and multiplied, yet quite differently in others. The generalization of geometric series as in Feizi <em>et al. </em>specifically deals with diagonalizable matrices, a class of matrices which has many convenient properties. Let <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> be such a matrix. The geometric series converges if, and only if, the eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> (numbers associated to the matrix) lie strictly between <img src='http://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-1}' title='{-1}' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{1}' title='{1}' class='latex' />, in which case</span></p>
<p><span style="color:#0000ff;"><a name="eqgeometricroot"></a></span></p>
<p align="center"><span style="color:#0000ff;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+M+%3D+%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7D+A%5Ei+%3D+A+%5Ccdot+%28I-A%29%5E%7B-1%7D+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle M = &#92;sum_{i=1}^{&#92;infty} A^i = A &#92;cdot (I-A)^{-1} &#92; &#92; &#92; &#92; &#92; (4)' title='&#92;displaystyle M = &#92;sum_{i=1}^{&#92;infty} A^i = A &#92;cdot (I-A)^{-1} &#92; &#92; &#92; &#92; &#92; (4)' class='latex' /></span></p>
<p><span style="color:#0000ff;"><a name="eqgeometricroot"></a></span></p>
<p><span style="color:#0000ff;"><a name="eqgeometricroot"></a></span></p>
<p><span style="color:#0000ff;">Notice that what has changed is that the condition that <img src='http://s0.wp.com/latex.php?latex=%7B-1%3Ca%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-1&lt;a&lt;1}' title='{-1&lt;a&lt;1}' class='latex' /> has been replaced by an eigenvalue restriction on <em>m</em> and the formula  <img src='http://s0.wp.com/latex.php?latex=A%28I-A%29%5E%7B-1%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='A(I-A)^{-1}' title='A(I-A)^{-1}' class='latex' /> is just like <img src='http://s0.wp.com/latex.php?latex=a%281-a%29%5E%7B-1%7D&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='a(1-a)^{-1}' title='a(1-a)^{-1}' class='latex' /> except the operation of matrix inversion is required instead of number inversion. </span><span style="color:#0000ff;">The result is obvious from elementary linear algebra because the assumption that <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> is diagonalizable means that <img src='http://s0.wp.com/latex.php?latex=%7BA%3DUDU%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A=UDU^{-1}}' title='{A=UDU^{-1}}' class='latex' /> for some matrix <img src='http://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{U}' title='{U}' class='latex' />, where <img src='http://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{D}' title='{D}' class='latex' /> is a diagonal matrix with the eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> on the diagonal. Therefore <img src='http://s0.wp.com/latex.php?latex=%7BA%5Ek%3DUD%5EkU%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A^k=UD^kU^{-1}}' title='{A^k=UD^kU^{-1}}' class='latex' /> and the series converges if, and only if, the geometric series formed by summing the powers of each eigenvalue converge.</span></p>
<p><span style="color:#ff0000;">In following Feizi <em>et al.</em>, we call the number <img src='http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;frac{1}{2}}' title='{&#92;frac{1}{2}}' class='latex' /> in the example above the &#8220;direct number&#8221;, and the remainder of the sum the &#8220;indirect number&#8221;. One should think of the indirect number as consisting of echoes of the direct number. Furthermore, following the language in Feizi et al., we call the ratio between terms, again the number <img src='http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;frac{1}{2}}' title='{&#92;frac{1}{2}}' class='latex' /> in the example above, the indirect flow. When it is strong, the terms decrease slowly. When it is weak, the terms decrease rapidly.</span></p>
<p><span style="color:#0000ff;">In Feizi <em>et al.</em>, the matrix <em>A</em> is called the &#8220;direct effect&#8221;, and the remainder of the sum the &#8220;indirect effect&#8221;. One should think of the indirect effect as consisting of echoes of the direct effect. Furthermore, following the language in Feizi et al., we call the rate at which the terms in the indirect effect decay the indirect flow. When it is strong, the terms decrease slowly. When it is weak, the terms decrease rapidly.</span></p>
<p><span style="color:#ff0000;">In <strong>number deconvolution</strong> the goal is to infer <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' /> from <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' />. That is, one would like to remove the indirect effect, the echoes, out of <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' />. If <img src='http://s0.wp.com/latex.php?latex=%7Bm%3Da%281-a%29%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m=a(1-a)^{-1}}' title='{m=a(1-a)^{-1}}' class='latex' />, then solving for <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' /> we obtain<a name="networkdec"></a></span></p>
<p align="center"><span style="color:#ff0000;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+a%3Dm%281%2Bm%29%5E%7B-1%7D.+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle a=m(1+m)^{-1}. &#92; &#92; &#92; &#92; &#92; (5)' title='&#92;displaystyle a=m(1+m)^{-1}. &#92; &#92; &#92; &#92; &#92; (5)' class='latex' /></span></p>
<p><span style="color:#ff0000;"><a name="networkdec"></a></span></p>
<p><span style="color:#ff0000;"><a name="networkdec"></a>That is the formula for number deconvolution. </span></p>
<p><span style="color:#0000ff;">In <strong>network deconvolution </strong>the goal is to infer  <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> from <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' />. That is, one would like to remove the indirect effect, the echoes, out of <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' /> as follows: <a name="lemroot"></a>If <img src='http://s0.wp.com/latex.php?latex=%7BM%3DA%28I-A%29%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M=A(I-A)^{-1}}' title='{M=A(I-A)^{-1}}' class='latex' /> then solving for <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> we obtain<a name="eqgeometricroot"></a></span></p>
<p align="center"><span style="color:#0000ff;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A+%3D+M+%5Ccdot+%28I%2BM%29%5E%7B-1%7D+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle A = M &#92;cdot (I+M)^{-1} &#92; &#92; &#92; &#92; &#92; (6)' title='&#92;displaystyle A = M &#92;cdot (I+M)^{-1} &#92; &#92; &#92; &#92; &#92; (6)' class='latex' /></span></p>
<p><span style="color:#0000ff;"><a name="eqgeometricroot"></a></span></p>
<p><span style="color:#0000ff;"><a name="eqgeometricroot"></a>That is the formula for network deconvolution.</span></p>
<p><span style="color:#ff0000;">Everything looks great but there is a problem. Even though one can plug any number <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' /> (except <img src='http://s0.wp.com/latex.php?latex=%7Bm%3D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m=-1}' title='{m=-1}' class='latex' />) into the formula and get out an <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' />, the formula only gives an <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' /> satisfying <img src='http://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7D+a%5Ei+%3D+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;sum_{i=1}^{&#92;infty} a^i = m}' title='{&#92;sum_{i=1}^{&#92;infty} a^i = m}' class='latex' /> when <img src='http://s0.wp.com/latex.php?latex=%7Bm+%3E+-%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m &gt; -&#92;frac{1}{2}}' title='{m &gt; -&#92;frac{1}{2}}' class='latex' />. F</span><span style="color:#ff0000;">or example, starting with <img src='http://s0.wp.com/latex.php?latex=%7Bm%3D-2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m=-2}' title='{m=-2}' class='latex' /> and then plugging it into number deconvolution one gets <img src='http://s0.wp.com/latex.php?latex=%7Ba%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a=2}' title='{a=2}' class='latex' />. But <img src='http://s0.wp.com/latex.php?latex=%7B2%2B4%2B+8%2B%5Ccdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{2+4+ 8+&#92;cdots}' title='{2+4+ 8+&#92;cdots}' class='latex' /> diverges.</span></p>
<p><span style="color:#0000ff;line-height:1.5em;"><span style="line-height:1.5em;">The restriction on <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' /> in number deconvolution, namely that it has to be bigger than <img src='http://s0.wp.com/latex.php?latex=%7B-%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-&#92;frac{1}{2}}' title='{-&#92;frac{1}{2}}' class='latex' />, can be translated into the same restriction on the eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' /> for network deconvolution. This follows from the fact that <img src='http://s0.wp.com/latex.php?latex=%7BM+%5C%2C+%3D+%5C%2C+%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7DA%5Ei+%5C%2C+%3D+%5C%2C+%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7DUD%5EiU%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M &#92;, = &#92;, &#92;sum_{i=1}^{&#92;infty}A^i &#92;, = &#92;, &#92;sum_{i=1}^{&#92;infty}UD^iU^{-1}}' title='{M &#92;, = &#92;, &#92;sum_{i=1}^{&#92;infty}A^i &#92;, = &#92;, &#92;sum_{i=1}^{&#92;infty}UD^iU^{-1}}' class='latex' /> means that if <img src='http://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;lambda}' title='{&#92;lambda}' class='latex' /> is an eigenvalue of <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> then <img src='http://s0.wp.com/latex.php?latex=%7B%5Cmu+%3A%3D%5Cfrac%7B%5Clambda%7D%7B1-%5Clambda%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;mu :=&#92;frac{&#92;lambda}{1-&#92;lambda}}' title='{&#92;mu :=&#92;frac{&#92;lambda}{1-&#92;lambda}}' class='latex' /> is an eigenvalue of <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' /> and all the eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' /> arise in this way. Therefore, <img src='http://s0.wp.com/latex.php?latex=%7B%5Clambda+%3D+%5Cfrac%7B%5Cmu%7D%7B1%2B%5Cmu%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;lambda = &#92;frac{&#92;mu}{1+&#92;mu}}' title='{&#92;lambda = &#92;frac{&#92;mu}{1+&#92;mu}}' class='latex' /> and the condition <img src='http://s0.wp.com/latex.php?latex=%7B-1+%3C+%5Clambda+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-1 &lt; &#92;lambda &lt; 1}' title='{-1 &lt; &#92;lambda &lt; 1}' class='latex' /> holds if, and only if, <img src='http://s0.wp.com/latex.php?latex=%7B%5Cmu%3E-%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;mu&gt;-&#92;frac{1}{2}}' title='{&#92;mu&gt;-&#92;frac{1}{2}}' class='latex' />. </span></span></p>
<p><span style="color:#ff0000;">But what if we wanted number deconvolution to work for all negative numbers? One idea is to follow Feizi et al.&#8217;s approach and introduce a scaling factor called <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma}' title='{&#92;gamma}' class='latex' /> to be applied to <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' />, so that the product <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma m}' title='{&#92;gamma m}' class='latex' /> can be deconvolved. For example, suppose we start with <img src='http://s0.wp.com/latex.php?latex=%7Bm%3D-12%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m=-12}' title='{m=-12}' class='latex' />. We&#8217;d like to find <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' />. But we can&#8217;t just apply number deconvolution. So we multiply <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' /> by <img src='http://s0.wp.com/latex.php?latex=%7B-%5Cfrac%7B1%7D%7B12%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-&#92;frac{1}{12}}' title='{-&#92;frac{1}{12}}' class='latex' /> to get <img src='http://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{1}' title='{1}' class='latex' />, and then deconvolve that to get <img src='http://s0.wp.com/latex.php?latex=%7Ba%3D%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a=&#92;frac{1}{2}}' title='{a=&#92;frac{1}{2}}' class='latex' />. We could have multiplied <img src='http://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{m}' title='{m}' class='latex' /> by something else, say <img src='http://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B-1%7D%7B6%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;frac{-1}{6}}' title='{&#92;frac{-1}{6}}' class='latex' /> in which case we&#8217;d get <img src='http://s0.wp.com/latex.php?latex=%7Ba%3D%5Cfrac%7B2%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a=&#92;frac{2}{3}}' title='{a=&#92;frac{2}{3}}' class='latex' />. In fact, there are infinitely many scaling factors, giving infinitely many solutions <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' />. In fact, we can get any <img src='http://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{a}' title='{a}' class='latex' /> between <img src='http://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{-1}' title='{-1}' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{1}' title='{1}' class='latex' />.</span></p>
<p><span style="color:#0000ff;">When <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' /> is not writable as a geometric sum (decomposable), there exist scaling factors <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma}' title='{&#92;gamma}' class='latex' /> such that the scaled matrix <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma M}' title='{&#92;gamma M}' class='latex' /> is decomposable. This is obvious because if <img src='http://s0.wp.com/latex.php?latex=%7B%5Cmu_%7B-%7D+%3D%5Cmu_1+%5Cleq+%5Cldots+%5Cleq+%5Cmu_n+%3D+%5Cmu_%7B%2B%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;mu_{-} =&#92;mu_1 &#92;leq &#92;ldots &#92;leq &#92;mu_n = &#92;mu_{+}}' title='{&#92;mu_{-} =&#92;mu_1 &#92;leq &#92;ldots &#92;leq &#92;mu_n = &#92;mu_{+}}' class='latex' /> are the eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' />, then <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+%5Cmu_1%2C%5Cldots%2C%5Cgamma+%5Cmu_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma &#92;mu_1,&#92;ldots,&#92;gamma &#92;mu_n}' title='{&#92;gamma &#92;mu_1,&#92;ldots,&#92;gamma &#92;mu_n}' class='latex' /> are the eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma M}' title='{&#92;gamma M}' class='latex' /> so by choosing <img src='http://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Cgamma+%3C+%7C%5Cfrac%7B1%7D%7B2%5Cmu_%7B-%7D%7D%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{0 &#92;leq &#92;gamma &lt; |&#92;frac{1}{2&#92;mu_{-}}|}' title='{0 &#92;leq &#92;gamma &lt; |&#92;frac{1}{2&#92;mu_{-}}|}' class='latex' /> we are guaranteed that <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma M}' title='{&#92;gamma M}' class='latex' /> is decomposable. <a name="lembeta"></a> Let <img src='http://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{M}' title='{M}' class='latex' /> be a real symmetric matrix with minimum/maximum eigenvalues <img src='http://s0.wp.com/latex.php?latex=%7B%5Cmu_%7B-%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;mu_{-}}' title='{&#92;mu_{-}}' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=%7B%5Cmu_%7B%2B%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;mu_{+}}' title='{&#92;mu_{+}}' class='latex' /> respectively and <img src='http://s0.wp.com/latex.php?latex=%7B0%3C%5Cbeta%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{0&lt;&#92;beta&lt;1}' title='{0&lt;&#92;beta&lt;1}' class='latex' />. If</span></p>
<p align="center"><span style="color:#0000ff;"><img src='http://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cgamma+%3D+%5Cfrac%7B%5Cbeta%7D%7Bmax%5Cleft%28%281-%5Cbeta%29%5Cmu_%7B%2B%7D%2C%28-1-%5Cbeta%29%5Cmu_%7B-%7D%5Cright%29%7D+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='&#92;displaystyle &#92;gamma = &#92;frac{&#92;beta}{max&#92;left((1-&#92;beta)&#92;mu_{+},(-1-&#92;beta)&#92;mu_{-}&#92;right)} &#92; &#92; &#92; &#92; &#92; (7)' title='&#92;displaystyle &#92;gamma = &#92;frac{&#92;beta}{max&#92;left((1-&#92;beta)&#92;mu_{+},(-1-&#92;beta)&#92;mu_{-}&#92;right)} &#92; &#92; &#92; &#92; &#92; (7)' class='latex' /></span></p>
<p><span style="color:#0000ff;"><a name="eqalpha"></a></span></p>
<p><span style="color:#0000ff;"><a name="eqalpha"></a> then the matrix <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma M}' title='{&#92;gamma M}' class='latex' /> is decomposable as <img src='http://s0.wp.com/latex.php?latex=%7B%5Cgamma+M+%3D+%5Csum_%7Bi%3D1%7D%5E%7B%5Cinfty%7DA%5Ei%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;gamma M = &#92;sum_{i=1}^{&#92;infty}A^i}' title='{&#92;gamma M = &#92;sum_{i=1}^{&#92;infty}A^i}' class='latex' />. Furthermore, if <img src='http://s0.wp.com/latex.php?latex=%7B%5Clambda_%7B-%7D%2C%5Clambda_%7B%2B%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{&#92;lambda_{-},&#92;lambda_{+}}' title='{&#92;lambda_{-},&#92;lambda_{+}}' class='latex' /> are the minimum/maximum eigenvalues of <img src='http://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{A}' title='{A}' class='latex' /> respectively then <img src='http://s0.wp.com/latex.php?latex=%7Bmax%28%7C%5Clambda_%7B-%7D%7C%2C%7C%5Clambda_%7B%2B%7D%7C%29+%3D+%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0' alt='{max(|&#92;lambda_{-}|,|&#92;lambda_{+}|) = &#92;beta}' title='{max(|&#92;lambda_{-}|,|&#92;lambda_{+}|) = &#92;beta}' class='latex' /> (we omit the derivation, but it is straightforward).</span><span style="color:#0000ff;"><br />
</span></p>
<p><span style="color:#0000ff;">Matrices do behave differently than numbers, and so it is not true that network deconvolution can produce <em>any</em> matrix as an output. <strong>However, as with number deconvolution, it remains true that network deconvolution can produce an <em>infinite</em> number of possible matrices. </strong></span></p>
<p><strong>Not only do Feizi <em>et al. </em>not mention this anywhere in the main text, instead they create the impression that there is a single output from any input.</strong></p>
<p dir="ltr"><span style="text-decoration:underline;"><strong>Feizi et al.&#8217;s response</strong></span></p>
<p dir="ltr">We now turn to the <a href="http://compbio.mit.edu/nd/Response_to_Nonsense_Blog_Post.pdf" target="_blank">response of Feizi <em>et al</em></a>. to our post. First, upon initial inspection of the “<a href="http://compbio.mit.edu/nd/try_it_out.html" target="_blank">Try It Out</a>” site, we were astounded to find that the code being distributed there is not the same as the code distributed with the paper (in particular, step 1 has been removed) and so it seems unlikely that it could be used to replicate the paper’s results. Unless, that is, the code originally distributed with the paper was not the code used to generate its results. Second, the “one click reproduction”s that the authors have now added do not actually start with the original data but instead merely produce plots from some results without showing how those results were generated. This is not what reproducibility means. Third, while the one matrix of results from a “one click reproduction” that we looked at (DI_ND_1bkr.mat) was very close to the matrix originally distributed with the paper, it was slightly different. It was close and hopefully it does generate basically the same figure, but as we explain below, we’ve spent a good bit of time on this and have no desire to spend any more. This is why we have tried to incentivize someone to simply reproduce the results.</p>
<p dir="ltr">In retrospect, we regret not explaining exactly how we came to be so skeptical about the reproducibility of the results in the Feizi <em>et al</em>. paper. While we very quickly (yet not instantly) recognized how misleading the paper is, our initial goal in looking at their results was not to verify reproducibility (which we assumed) but rather to explore how changing the parameters affected the results. Verifying that we could recover the results from the paper was only supposed to be the first step in this process. We downloaded the file of datasets and code provided by the authors and began examining the protein contact dataset.</p>
<p>After writing scripts that we verified exactly regenerated some figures from the paper from the output matrices distributed by the authors, we then checked whether we arrived at the same results when running the ND code on what we assumed was the input data (files with names like “MI_1bkr.txt” while the output was named “MI_ND_1bkr.txt”). We were surprised when the output did not match, however we were then informed by the authors that they had not distributed the input data but rather thresholded versions of the input. When we asked the authors to provide us with the actual data, we were told that it would violate scientific etiquette to send us another scientist’s dataset and that we would have to regenerate it ourselves. We had never heard this claimed point of etiquette before, but acquiesced and attempted to do so. However, the matrices produced from the original data were actually a different size than those used by the authors (suggesting that they had used a different alignment).</p>
<p>Stymied on that front, we turned to the DREAM5 dataset instead. While the actual scoring that goes into the DREAM analysis is fairly complicated, we decided that we would start by merely checking that we could regenerate the output provided by the authors. We eventually received step-by-step instructions for doing so and, because those steps did not produce the provided output, asked a simple question suggested by our experience with protein contact dataset: to generate the provided results, should we use as input the provided non-ND matrices? We asked this question four times in total without receiving a reply. We sent our script attempting to implement their steps and received no response.</p>
<p dir="ltr">At some point, we also discovered that the authors had been using different parameter values for different datasets without disclosing that to their readers. They could provide no coherent explanation for doing so. And sometime after this, we found that the authors had removed the data files from their website: readers were directed instead to acquire the datasets elsewhere and the results from the paper were no longer provided there.</p>
<p dir="ltr">At this point, we expect it is obvious why we were skeptical about the reproducibility of the results. Having said that, we never wanted to believe that the results of the paper were not reproducible. It was not our initial assumption and we still hope to be proven wrong. However, we continue to wait, and the fact that the code has been changed yet again, and that the authors’ explanation in regards to figure S4 does not appear to explain its changes (see below) do not fill us with confidence.</p>
<p dir="ltr">We repeat that while most of the discussion above has focused on replicability, we will quote again the definition of “fraud” used in the original post: “deception deliberately practiced in order to secure unfair or unlawful gain”. Our main point is this: would any honest reader of the main text of the original paper recognize the actual method implemented? We don&#8217;t think so, hence deception. And we believe the authors deliberately wrote the paper in this way to unfairly gain acceptance at Nature Biotechnology.</p>
<p dir="ltr">Now, in response to the authors rebuttal, we offer the following (Feizi <em>et al.</em> remarks in italics):</p>
<p dir="ltr"><span style="text-decoration:underline;"><strong>Point-by-point rebuttal</strong></span></p>
<p dir="ltr"><em><span style="line-height:1.5em;">Appendix A</span></em></p>
<p dir="ltr"><em>The pre-processing step that Bray and Pachter criticize (step 1 in their description of our work) has no effect on the performance of ND. Mathematically, because our matrices are non-negative with min zero, a linear scaling of the network results in a similar scaling of its eigenvalues, which are normalized during eigenvalue scaling, canceling out the linear scaling of step 1. Practically, removing step 1 from the code has little effect on the performance of ND on the DREAM5 regulatory networks, as we show in Figure 1 below.</em></p>
<p dir="ltr">We cannot imagine how the authors came to make the statement “our matrices are non-negative with min zero”. After all, one of the inputs they used in their paper was correlation matrices and they surely know that correlations can be negative. If the map in step 1 were linear rather than &#8211; as we correctly stated in the document they are responding to &#8211; affine (for those unfamiliar with the terms, “linear” here means only scaling values while “affine” means scaling and shifting as well), then this explanation would be correct and it would have no effect. This just makes it even stranger that immediately after giving this explanation, the authors produced a graph showing the (sometimes quite significant) effect of what we assume was the affine mapping.</p>
<p dir="ltr">Stranger still is that after this defense, Step 1 has now been removed from the code.</p>
<p dir="ltr"><em>Appendix B</em></p>
<p><em>The eigenvalue scaling (step 5) is essential both theoretically, to guarantee the convergence of the Taylor series, and practically, as performance decreases without it in practice. This step is clearly stated in both the </em><em>main text of our paper and in the supplement, despite the claims by Bray and Pachter that it somehow was maliciously concealed from our description of the method (just search the manuscript for the word &#8216;scaling&#8217;). Our empirical results confirm that this step is also necessary in practice, as ND without eigenvalue scaling is consistently performing worse, as we show in Figure 2 below:</em></p>
<p dir="ltr">We do not understand why the author’s felt the need to defend the use of eigenvalue scaling when we have never suggested it should be removed. Our objection was rather to how it was presented (or not) to the reader. Yes, there is a mention of the word “scaling”. But it is done so in a way that makes it sound as though it were some trivial issue: there’s an assumption, you scale, assumption satisfied, done. And this mention occurs in the context of the rest of the main text of the paper which presents a clear image of a simple, parameterless, 100% theoretically justified method: you have a matrix, you put it into the method, you get the output, done (and it is &#8220;globally optimal&#8221;). In other words, it is the math we discuss above prior to the arrival of the parameter <img src='http://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=545454&amp;s=0' alt='&#92;gamma' title='&#92;gamma' class='latex' />.</p>
<p dir="ltr">Yes, scaling is dealt with at length in the supplement. Perhaps Feizi et al. think it is fine for the main text of a paper to convey such a different image from the truth contained in its supplement and in the code. We do not and we would hope the rest of the scientific community agrees with us.</p>
<p dir="ltr"><em>Appendix C: Robustness to input parameters.</em></p>
<p dir="ltr"><em>Bray and Pachter claim that &#8220;the reason for covering up the existence of parameters is that the parameters were tuned to obtain the results&#8221;. Once again the claims are incorrect and unfounded. (1) First, we did not cover up the existence of the parameters. (2) Second, we used exactly the exact same set of parameters for all 27 tested regulatory networks in the DREAM challenge. (3) Third, we show here that our results are robust to the parameter values, using β = 0.5 and β = 0.99 for the DREAM5 network.</em></p>
<p>(1) Again: in the main text of the paper, there is not even a hint that the method could have parameters. This is highly relevant information that a reader who is looking at its performance on datasets should have access to. That reader could then ask, e.g. what parameters were used and how they were selected. They would even have the ability to wonder if the parameters were tuned to improve apparent performance. It is worth noting that when the paper was peer reviewed, the reviewers did not have access to the actual values of the parameters used.</p>
<p>(2) It is, of course, perfectly possible to tune parameters while using those same parameters on multiple datasets.</p>
<p>(3) The authors seem to have forgotten that there are <b>two</b> parameters to the method and that they gave the other parameter (alpha) different values on different datasets as well. They also have omitted their performance on the “Community” dataset for some reason.</p>
<p dir="ltr"><em>Appendix D: Network Deconvolution vs. Partial Correlation. </em></p>
<p dir="ltr"><em><span style="line-height:1.5em;">Bray and Pachter compare network deconvolution to partial correlation using a test dataset built using a partial correlation model. In this very artificial setting, it is thus not surprising that partial correlation performs better, as it exactly matches the process that generated the data in the first place. To demonstrate the superiority of partial correlation, Bray and Pachter should test it on real datasets, such as the ones provided as part of the DREAM5 benchmarks . In our experience, partial correlation performed very poorly in practice, but we encourage Bray and Pachter to try it for themselves.</span></em></p>
<p dir="ltr">We looked at partial correlations here for a very simple reason: the authors originally claimed that their method reduced to it in the context we considered here. Thus, comparing them seemed a natural thing to do.</p>
<p dir="ltr">The idea that we should look at the performance of partial correlations in other contexts makes no sense: we have never claimed that it is the right tool to solve every problem. Indeed, it was the authors who claimed that their tool was “widely applicable”. By arguing for domain specific tools, they seem to be making our point for us.</p>
<p dir="ltr"><em>Claim 1: &#8220;the method used to obtain the results in the paper is completely different than the idealized version sold in the main text of the paper&#8221;.</em></p>
<p dir="ltr"><em>The paper clearly describes both the key matrix operation (&#8220;step 6&#8243; in Lior Pachter&#8217;s blog post) which is shown in figure 1, and all the pre- and post-processing steps, which are all part of our method. There is nothing mischievous about including these pre- and post-processing steps that were clearly defined, </em><em>well described, and implemented in the provided code.</em></p>
<p dir="ltr">The statement that the paper describes all steps is simply false. For example, the affine mapping appears nowhere.</p>
<p dir="ltr"><em>Claim 2: &#8220;the method actually used has parameters that need to be set, yet no approach to setting them is provided&#8221;.</em></p>
<p dir="ltr"><em>It is unfortunate that so many methods in our field have parameters associated with them, but we are not the first method to have them. However, we do provide guidelines for setting these in the supplement.</em></p>
<p dir="ltr">It is strange to suggest that it is unfortunate that methods have parameters. It is also strange to describe their guidelines as such when they did not even follow them. Also, there is no guideline for setting alpha in the supplement. In their FAQ, they say “we used beta=0.5 as we expected indirect flows to be propagating more slowly”. We wonder where one derives these expectations from.</p>
<p dir="ltr"><em>Dr. Pachter also points out that a correction to Supplement Figure S4 on August 26th 2013 was not fully documented. </em><em>We apologize for the omission and provide additional details in an updated correction notice dated February 12, 2014.</em></p>
<p dir="ltr">The correction notice states that the original Figure S4 was plotted with the incorrect <em>x</em>-axis. In particular it states that the maximum eigenvalue of the observed network was used, instead of the maximum eigenvalue of the direct network. We checked this correction, and have produced below the old curve and the new curve plotted on the same <em>x</em>-axis:</p>
<p dir="ltr"><a href="http://liorpachter.files.wordpress.com/2014/02/new_and_old_s4.png"><img class="aligncenter size-full wp-image-2397" alt="new_and_old_S4" src="http://liorpachter.files.wordpress.com/2014/02/new_and_old_s4.png?w=490&#038;h=196" width="490" height="196" /></a></p>
<p dir="ltr" style="text-align:center;">The new and old Figure S4. The old curve, remapped into the correct <em>x-</em>axis coordinates is shown in red. The new curve is shown in blue. Raw data was extracted from the supplement PDFs using <a href="http://arohatgi.info/WebPlotDigitizer/" target="_blank">WebPlotDigitizer</a>.</p>
<p dir="ltr">As can be seen, the two curves are not the same. While it is expected that due to the transformation the red curve should not cover the whole <em>x-</em>axis, if the only difference was the choice of coordinates, the curves should overlap exactly. We cannot explain the discrepancy.</p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul><li class="share-email"><a rel="nofollow" class="share-email sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/?share=email" title="Click to email this to a friend"><span>Email</span></a></li><li class="share-print"><a rel="nofollow" class="share-print sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/" title="Click to print"><span>Print</span></a></li><li><a href="#" class="sharing-anchor sd-button share-more"><span>More</span></a></li><li class="share-end"></li></ul><div class="sharing-hidden"><div class="inner" style="display: none;"><ul><li class="share-twitter"><a rel="nofollow" class="share-twitter sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/?share=twitter" title="Click to share on Twitter" id="sharing-twitter-2330"><span>Twitter</span></a></li><li class="share-google-plus-1"><a rel="nofollow" class="share-google-plus-1 sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/?share=google-plus-1" title="Click to share on Google+" id="sharing-google-2330"><span>Google</span></a></li><li class="share-end"></li><li class="share-reddit"><a rel="nofollow" class="share-reddit sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/?share=reddit" title="Click to share on Reddit"><span>Reddit</span></a></li><li class="share-linkedin"><a rel="nofollow" class="share-linkedin sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/?share=linkedin" title="Click to share on LinkedIn" id="sharing-linkedin-2330"><span>LinkedIn</span></a></li><li class="share-end"></li><li class="share-facebook"><a rel="nofollow" class="share-facebook sd-button share-icon" href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/?share=facebook" title="Share on Facebook" id="sharing-facebook-2330"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div></div></div><div class='sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded' id='like-post-wrapper-12758541-2330-53b3e857beb0c' data-src='//widgets.wp.com/likes/#blog_id=12758541&amp;post_id=2330&amp;origin=liorpachter.wordpress.com&amp;obj_id=12758541-2330-53b3e857beb0c' data-name='like-post-frame-12758541-2330-53b3e857beb0c'><h3 class='sd-title'>Like this:</h3><div class='likes-widget-placeholder post-likes-widget-placeholder' style='height:55px'><span class='button'><span>Like</span></span> <span class="loading">Loading...</span></div><span class='sd-text-color'></span><a class='sd-link-color'></a></div></div>		</div>
			</div>
</div>
<div id="secondary">

<div class="widgets">

<div id="search-4" class="widget widget_search"><div class="searchbox">
	<form method="get" id="searchform" action="/"><fieldset>
		<input type="text" value="" name="s" id="s" tabindex="21" />
		<input type="submit" id="searchsubmit" value="Search" tabindex="22" />
	</fieldset></form>
</div></div><div id="recent-comments-2" class="widget widget_recent_comments"><h3>Recent Comments</h3>				<table class="recentcommentsavatar" cellspacing="0" cellpadding="0" border="0">
					<tr><td title="Bug10" class="recentcommentsavatartop" style="height:48px; width:48px;"><img alt='' src='http://1.gravatar.com/avatar/a858ea78d8573552df2403c9fa928e16?s=48&#038;d=http%3A%2F%2Fs0.wp.com%2Fi%2Fmu.gif&#038;r=G' class='avatar avatar-48' height='48' width='48' /></td><td class="recentcommentstexttop" style="">Bug10 on <a href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/#comment-1702">Nicholas Wade&#8217;s troubles&hellip;</a></td></tr><tr><td title="Bug10" class="recentcommentsavatarend" style="height:48px; width:48px;"><img alt='' src='http://1.gravatar.com/avatar/a858ea78d8573552df2403c9fa928e16?s=48&#038;d=http%3A%2F%2Fs0.wp.com%2Fi%2Fmu.gif&#038;r=G' class='avatar avatar-48' height='48' width='48' /></td><td class="recentcommentstextend" style="">Bug10 on <a href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/#comment-1701">Nicholas Wade&#8217;s troubles&hellip;</a></td></tr><tr><td title="GhoshP" class="recentcommentsavatarend" style="height:48px; width:48px;"><img alt='' src='http://0.gravatar.com/avatar/9f26f023d974cd0e48c6aa2d7f308756?s=48&#038;d=http%3A%2F%2Fs0.wp.com%2Fi%2Fmu.gif&#038;r=G' class='avatar avatar-48' height='48' width='48' /></td><td class="recentcommentstextend" style="">GhoshP on <a href="http://liorpachter.wordpress.com/seq/#comment-1662">*Seq</a></td></tr><tr><td title="All Things Illumina Sequencing Methods" class="recentcommentsavatarend" style="height:48px; width:48px;"><a href="http://nextgenseek.com/2014/06/all-things-illumina-sequencing-methods/" rel="nofollow"></a></td><td class="recentcommentstextend" style=""><a href="http://nextgenseek.com/2014/06/all-things-illumina-sequencing-methods/" rel="nofollow">All Things Illumina&hellip;</a> on <a href="http://liorpachter.wordpress.com/seq/#comment-1661">*Seq</a></td></tr><tr><td title="Mary Jane (@MaryJsDiary)" class="recentcommentsavatarend" style="height:48px; width:48px;"><a href="http://twitter.com/MaryJsDiary" rel="nofollow"><img alt='' src='http://i0.wp.com/pbs.twimg.com/profile_images/463024602130546688/PWlKi9gx_normal.png?resize=48%2C48' class='avatar avatar-48' height='48' width='48' /></a></td><td class="recentcommentstextend" style=""><a href="http://twitter.com/MaryJsDiary" rel="nofollow">Mary Jane (@MaryJsDi&hellip;</a> on <a href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/#comment-1643">Does researching casual mariju&hellip;</a></td></tr>				</table>
				</div>		<div id="recent-posts-3" class="widget widget_recent_entries">		<h3>Posts in chronological order</h3>		<ul>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/06/04/nicholas-wades-troublesome-inheritance/">Nicholas Wade&#8217;s troublesome&nbsp;inheritance</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/">What is principal component&nbsp;analysis?</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/05/25/bessels-correction-and-the-dangers-of-moocs/">Bessel&#8217;s correction and the dangers of&nbsp;MOOCs</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/05/16/time-to-end-ordered-authorship/">Time to end ordered&nbsp;authorship</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/04/30/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall/">Estimating number of transcripts from RNA-Seq measurements (and why I believe in&nbsp;paywall)</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/04/17/does-researching-casual-marijuana-use-cause-brain-abnormalities/">Does researching casual marijuana use cause brain&nbsp;abnormalities?</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/03/25/designing-roofs-and-drawing-phylogenetic-trees/">Designing roofs and drawing phylogenetic&nbsp;trees</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/03/18/reproducibility-vs-usability/">Reproducibility vs. usability</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/03/01/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data/">Using statistical methods to estimate and take into account experimental measurement errors: a case study using high throughput proteomics&nbsp;data</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/02/18/number-deconvolution/">Number deconvolution</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/02/12/why-i-read-the-network-nonsense-papers/">Why I read the network nonsense&nbsp;papers</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/02/11/the-network-nonsense-of-manolis-kellis/">The network nonsense of Manolis&nbsp;Kellis</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/02/10/the-network-nonsense-of-albert-laszlo-barabasi/">The network nonsense of Albert-László&nbsp;Barabási</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2014/01/19/why-do-you-look-at-the-speck-in-your-sisters-quilt-plot-and-pay-no-attention-to-the-plank-in-your-own-heat-map/">Why do you look at the speck in your sister&#8217;s quilt plot and pay no attention to the plank in your own heat&nbsp;map?</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/12/31/this-paper-is-full-of-sht/">This paper is full of&nbsp;sh*t</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/12/28/nature-is-irrational/">Nature is irrational</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/12/23/time-to-end-letter-grades/">Time to end letter&nbsp;grades</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/12/08/molecular-restoration-studies-of-extinct-forms-of-life/">Molecular &#8220;restoration studies&#8221; of extinct forms of&nbsp;life</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/11/30/23andme-genotypes-are-all-wrong/">Multiple testing an issue for&nbsp;23andme</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/11/17/imakaev_explained/">Iterative [proportional fitting of a log-linear model for] correction of Hi-C data reveals hallmarks of chromosome&nbsp;organization</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/11/09/linearity-of-expectation/">Linearity of expectation</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/11/07/zero-cell-rna-seq/">Zero-cell RNA-Seq</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/11/05/claude-shannon-population-geneticist/">Claude Shannon, population&nbsp;geneticist</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/11/02/stories-from-the-supplement/">Stories from the&nbsp;Supplement</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/10/31/response-to-gtex-is-throwing-away-90-of-their-data/">Response to: “GTEx is throwing away 90% of their&nbsp;data”</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/10/25/random-numbers-reproducibility-and-samtools/">Random numbers, reproducibility and&nbsp;SAMtools</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/10/21/gtex/">GTEx is throwing away 90% of their&nbsp;data</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/10/17/non-uniform-coverage-of-transcripts-in-rna-seq-experiments/">Non-uniform coverage of transcripts in RNA-Seq&nbsp;experiments</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/10/08/ukkonens-condition-for-assembly/">Ukkonen&#8217;s condition for&nbsp;assembly</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/09/28/finite-decimal-expansion-infinite-time-constants-and-structural-controllability-of-networks/">Finite decimal expansion, infinite time constants and structural controllability of&nbsp;networks</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/09/27/the-tabular-hall-of-shame/">The tabular hall of&nbsp;shame</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/09/21/the-needleman-wunsch-algorithm/">The Needleman-Wunsch algorithm</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/09/18/unifrac-revealed/">UniFrac revealed</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/09/06/dont-believe-the-affymetrix-anti-hype/">Don&#8217;t believe the Affymetrix&nbsp;anti-hype</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/09/04/fibonacci-numbers-and-splicing-complexity/">Fibonacci numbers and splicing&nbsp;complexity</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/08/26/magnitude-of-effect-vs-statistical-significance/">Magnitude of effect vs. statistical&nbsp;significance</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/08/26/the-phylogeny-of-orange-is-a-point-in-a-phylogenetic-orange/">The phylogeny of oranges is a point in a phylogenetic&nbsp;orange</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/08/22/catching-a-bus/">Catching a bus</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/08/20/sailfish/">Sailfish</a>
						</li>
					<li>
				<a href="http://liorpachter.wordpress.com/2013/08/19/genesis-of-seq/">Genesis of *Seq</a>
						</li>
				</ul>
		</div><div id="categories-2" class="widget widget_categories"><h3>Categories</h3><select name='cat' id='cat' class='postform' >
	<option value='-1'>Select Category</option>
	<option class="level-0" value="391498">*Seq&nbsp;&nbsp;(13)</option>
	<option class="level-1" value="23214864">&nbsp;&nbsp;&nbsp;RNA-Seq&nbsp;&nbsp;(11)</option>
	<option class="level-0" value="21325">arXiv&nbsp;&nbsp;(11)</option>
	<option class="level-1" value="3582">&nbsp;&nbsp;&nbsp;mathematics&nbsp;&nbsp;(1)</option>
	<option class="level-2" value="27083426">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;math.AG&nbsp;&nbsp;(1)</option>
	<option class="level-2" value="6133188">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;math.CO&nbsp;&nbsp;(1)</option>
	<option class="level-2" value="38668166">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;math.MG&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="1211">&nbsp;&nbsp;&nbsp;physics&nbsp;&nbsp;(2)</option>
	<option class="level-2" value="191967355">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;astro-ph.IM&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="13816077">&nbsp;&nbsp;&nbsp;quantitative biology&nbsp;&nbsp;(8)</option>
	<option class="level-2" value="191967173">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q-bio.GN&nbsp;&nbsp;(6)</option>
	<option class="level-2" value="192228372">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q-bio.PE&nbsp;&nbsp;(2)</option>
	<option class="level-2" value="194875636">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q-bio.QM&nbsp;&nbsp;(1)</option>
	<option class="level-1" value="192035513">&nbsp;&nbsp;&nbsp;statistics&nbsp;&nbsp;(3)</option>
	<option class="level-2" value="192230433">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stat.ME&nbsp;&nbsp;(3)</option>
	<option class="level-0" value="1342">education&nbsp;&nbsp;(9)</option>
	<option class="level-0" value="162892">expository&nbsp;&nbsp;(10)</option>
	<option class="level-0" value="415">guest post&nbsp;&nbsp;(2)</option>
	<option class="level-0" value="5281">papers&nbsp;&nbsp;(1)</option>
	<option class="level-0" value="309">reviews&nbsp;&nbsp;(16)</option>
	<option class="level-0" value="74131">sophistry&nbsp;&nbsp;(9)</option>
	<option class="level-0" value="42375">talks&nbsp;&nbsp;(5)</option>
	<option class="level-0" value="6">technology&nbsp;&nbsp;(3)</option>
</select>

<script type='text/javascript'>
/* <![CDATA[ */
	var dropdown = document.getElementById("cat");
	function onCatChange() {
		if ( dropdown.options[dropdown.selectedIndex].value > 0 ) {
			location.href = "http://liorpachter.wordpress.com/?cat="+dropdown.options[dropdown.selectedIndex].value;
		}
	}
	dropdown.onchange = onCatChange;
/* ]]> */
</script>

</div><div id="linkcat-191670002" class="widget widget_links"><h3>Biology</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://www.genomesunzipped.org" title="Collaborative blog on personal genomics" target="_blank">genomes unzipped</a></li>
<li><a href="http://www.michaeleisen.org/blog/?wref=bif" title="Michael Eisen&#8217;s blog" target="_blank">it is NOT junk</a></li>
<li><a href="http://nextgenseek.com/" target="_blank">Next Gen Seek</a></li>
<li><a href="http://ongenetics.blogspot.com" title="Steve Mount&#8217;s blog" target="_blank">On Genetics</a></li>
<li><a href="http://rrresearch.fieldofscience.com" title="Rosie Redfield&#8217;s blog" target="_blank">RRResearch</a></li>
<li><a href="http://phylogenomics.blogspot.com/?wref=bif" title="Jonathan Eisen&#8217;s blog" target="_blank">The Tree of Life</a></li>

	</ul>
</div>
<div id="linkcat-191674163" class="widget widget_links"><h3>Computational Biology</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://pmelsted.wordpress.com" title="Páll Melsted&#8217;s blog" target="_blank">Bits of Bioinformatics</a></li>
<li><a href="http://www.homolog.us/blogs/" target="_blank">Homologus</a></li>
<li><a href="http://flxlexblog.wordpress.com" title="Lex Nederbragt&#8217;s blog" target="_blank">In between lines of code</a></li>
<li><a href="http://judgestarling.tumblr.com" title="Dan Graur&#8217;s blog" target="_blank">Judge Starling</a></li>
<li><a href="http://ivory.idyll.org/blog/" title="Titus Brown’s blog" target="_blank">Living in an ivory basement</a></li>
<li><a href="http://biomickwatson.wordpress.com/2013/11/17/we-didnt-ask-for-it/" title="Mick Watson&#8217;s blog" target="_blank">opiniomics</a></li>
<li><a href="http://nickeriksson.blogspot.com" title="Nick Eriksson&#8217;s blog" target="_blank">The turning of the key</a></li>

	</ul>
</div>
<div id="linkcat-191670432" class="widget widget_links"><h3>Computer Science</h3>
	<ul class='xoxo blogroll'>
<li><a href="https://blogs.princeton.edu/imabandit/" title="Sébastien Bubeck&#8217;s blog" target="_blank">I&#039;m a bandit</a></li>
<li><a href="http://lucatrevisan.wordpress.com" title="Luca Trevisan&#8217;s blog" target="_blank">in theory</a></li>
<li><a href="http://tcsmath.wordpress.com" title="James Lee&#8217;s blog" target="_blank">tcs math</a></li>

	</ul>
</div>
<div id="linkcat-219207742" class="widget widget_links"><h3>Ideas</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://tunesforbears.com" title="Julian Jamison&#8217;s blog" target="_blank">Tunes for Bears</a></li>

	</ul>
</div>
<div id="linkcat-191667277" class="widget widget_links"><h3>Math</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://gilkalai.wordpress.com" title="Gil Kalai&#8217;s blog" target="_blank">Combinatorics and more</a></li>
<li><a href="http://www.math.rutgers.edu/~zeilberg/OPINIONS.html" title="Doron Zeilberger&#8217;s opinions" target="_blank">Dr. Z&#039;s Opinions</a></li>
<li><a href="http://haggstrom.blogspot.se" title="Olle Häggström&#8217;s blog" target="_blank">Häggström hävdar</a></li>
<li><a href="http://quomodocumque.wordpress.com" title="Jordan Ellenberg&#8217;s blog" target="_blank">Quomodocumque</a></li>
<li><a href="http://terrytao.wordpress.com/" title="Terence Tao&#8217;s blog" target="_blank">What&#039;s new</a></li>

	</ul>
</div>
<div id="linkcat-191970373" class="widget widget_links"><h3>Medicine</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://other-things-amanzi.blogspot.com" title="Neil Bonginkosi Lawrence Taverner’s blog" target="_blank">other things amanzi</a></li>
<li><a href="http://willandreason.com" title="William &#8220;Roy&#8221; Smythe&#8217;s blog" target="_blank">Will and Reason</a></li>

	</ul>
</div>
<div id="linkcat-191676884" class="widget widget_links"><h3>Statistics</h3>
	<ul class='xoxo blogroll'>
<li><a href="http://www.mii.ucla.edu/causality/" title="Judea Pearl&#8217;s blog" target="_blank">Causal Analysis in Theory and Practice</a></li>
<li><a href="http://normaldeviate.wordpress.com" title="Larry Wasserman&#8217;s blog" target="_blank">Normal Deviate</a></li>
<li><a href="http://simplystatistics.org" title="Jeff Leek, Roger Peng and Rafa Irizzary joint blog" target="_blank">Simply Statistics</a></li>
<li><a href="http://andrewgelman.com" title="Andrew Gelman&#8217;s blog" target="_blank">Statistical Modeling, Causal Inference, and Social Science</a></li>
<li><a href="http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/" title="Cosma Shalizi&#8217;s blog" target="_blank">Three-Toed Sloth</a></li>

	</ul>
</div>
<div id="blog-stats-2" class="widget widget_blog-stats"><h3>Blog Stats</h3>		<ul>
			<li>323,834 hits</li>
		</ul>
		</div>
</div>

</div>
</div>

<div id="footer">

	<div id="miscellany">

	<div class="widgets">

	<div id="follow_button_widget-2" class="widget widget_follow_button_widget">
		<a class="wordpress-follow-button" href="http://liorpachter.wordpress.com" data-blog="http://liorpachter.wordpress.com" data-lang="en" >Follow Bits of DNA on WordPress.com</a>
		<script type="text/javascript">(function(d){var f = d.getElementsByTagName('SCRIPT')[0], p = d.createElement('SCRIPT');p.type = 'text/javascript';p.async = true;p.src = '//widgets.wp.com/platform.js';f.parentNode.insertBefore(p,f);}(document));</script>

		</div>
	</div>

	</div>


	<div id="about">
		<div class="navigation">
							<div class="left"><a href="http://liorpachter.wordpress.com/page/2/" ><span>&laquo;</span> Previous Entries</a></div>
			<div class="right"></div>
				</div>
	</div>


	<div id="theme-info">
		<div class="primary content">
			<p><span class="generator"><a href="http://wordpress.com/?ref=footer_blog">Blog at WordPress.com</a>.</span><span class="designer"><a href="http://theme.wordpress.com/credits/liorpachter.wordpress.com/" title="Learn about customizing this theme with the Custom Design upgrade">Customized Tarski Theme</a>. </span></p>
		</div>
		<div class="secondary">
			<p><a class="feed" title="Subscribe to the Bits of DNA feed" href="http://liorpachter.wordpress.com/feed/">Subscribe to feed.</a></p>
		</div>
	</div>


</div>

</div>		<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = {"settings":{"id":"primary","ajaxurl":"http:\/\/liorpachter.wordpress.com\/?infinity=scrolling","type":"click","wrapper":true,"wrapper_class":"infinite-wrap","footer":true,"click_handle":"1","text":"Older posts","totop":"Scroll back to top","currentday":"18.02.14","order":"DESC","scripts":[],"styles":[],"google_analytics":false,"offset":0,"history":{"host":"liorpachter.wordpress.com","path":"\/page\/%d\/","use_trailing_slashes":true,"parameters":""},"query_args":{"error":"","m":"","p":0,"post_parent":"","subpost":"","subpost_id":"","attachment":"","attachment_id":0,"name":"","static":"","pagename":"","page_id":0,"second":"","minute":"","hour":"","day":0,"monthnum":0,"year":0,"w":0,"category_name":"","tag":"","cat":"","tag_id":"","author":"","author_name":"","feed":"","tb":"","paged":0,"comments_popup":"","meta_key":"","meta_value":"","preview":"","s":"","sentence":"","fields":"","menu_order":"","category__in":[],"category__not_in":[],"category__and":[],"post__in":[],"post__not_in":[],"tag__in":[],"tag__not_in":[],"tag__and":[],"tag_slug__in":[],"tag_slug__and":[],"post_parent__in":[],"post_parent__not_in":[],"author__in":[],"author__not_in":[],"posts_per_page":10,"ignore_sticky_posts":false,"suppress_filters":false,"cache_results":false,"update_post_term_cache":true,"update_post_meta_cache":true,"post_type":"","nopaging":false,"comments_per_page":"50","no_found_rows":false,"order":"DESC"},"last_post_date":"2014-02-18 04:47:51","stats":"blog=12758541&v=wpcom&tz=-7&user_id=0&subd=liorpachter&x_pagetype=infinite-click"}};
		//]]>
		</script>
		<script type='text/javascript' src='//0.gravatar.com/js/gprofiles.js?ver=201427x'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type='text/javascript' src='http://s2.wp.com/wp-content/mu-plugins/gravatar-hovercards/wpgroho.js?m=1380573781g'></script>

	<script>
		//initialize and attach hovercards to all gravatars
		jQuery( document ).ready( function( $ ) {
			if ( typeof Gravatar.init !== "function" ) {
				return;
			}			

			Gravatar.profile_cb = function( hash, id ) {
				WPGroHo.syncProfileData( hash, id );
			};
			Gravatar.my_hash = WPGroHo.my_hash;
			Gravatar.init( 'body', '#wp-admin-bar-my-account' );
		});
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-a858ea78d8573552df2403c9fa928e16">
	</div>
	<div class="grofile-hash-map-9f26f023d974cd0e48c6aa2d7f308756">
	</div>
	<div class="grofile-hash-map-5115db88c6509103474534c5c4006da1">
	</div>
	</div>

	<div id="bit" class="loggedout-follow-normal">
		<a class="bsub" href="javascript:void(0)"><span id='bsub-text'>Follow</span></a>
		<div id="bitsubscribe">

					<h3><label for="loggedout-follow-field">Follow &ldquo;Bits of DNA&rdquo;</label></h3>

			<form action="https://subscribe.wordpress.com" method="post" accept-charset="utf-8" id="loggedout-follow">
			<p>Get every new post delivered to your Inbox.</p>

			<p id="loggedout-follow-error" style="display: none;"></p>

						<p class="bit-follow-count">Join 2,510 other followers</p>
			<p><input type="email" name="email" value="Enter your email address" onfocus='this.value=(this.value=="Enter your email address") ? "" : this.value;' onblur='this.value=(this.value=="") ? "Enter email address" : this.value;'  id="loggedout-follow-field"/></p>

			<input type="hidden" name="action" value="subscribe"/>
			<input type="hidden" name="blog_id" value="12758541"/>
			<input type="hidden" name="source" value="http://liorpachter.wordpress.com/"/>
			<input type="hidden" name="sub-type" value="loggedout-follow"/>

			<input type="hidden" id="_wpnonce" name="_wpnonce" value="2cbc6f8659" /><input type="hidden" name="_wp_http_referer" value="/" />
			<p id='bsub-subscribe-button'><input type="submit" value="Sign me up" /></p>
			</form>
					<div id='bsub-credit'><a href="http://wordpress.com/signup/?ref=lof">Powered by WordPress.com</a></div>
		</div><!-- #bitsubscribe -->
	</div><!-- #bit -->

	<script type="text/javascript">
		WPCOM_sharing_counts = {"http:\/\/liorpachter.wordpress.com\/2014\/06\/04\/nicholas-wades-troublesome-inheritance\/":3101,"http:\/\/liorpachter.wordpress.com\/2014\/05\/26\/what-is-principal-component-analysis\/":2640,"http:\/\/liorpachter.wordpress.com\/2014\/05\/25\/bessels-correction-and-the-dangers-of-moocs\/":2978,"http:\/\/liorpachter.wordpress.com\/2014\/05\/16\/time-to-end-ordered-authorship\/":2842,"http:\/\/liorpachter.wordpress.com\/2014\/04\/30\/estimating-number-of-transcripts-from-rna-seq-measurements-and-why-i-believe-in-paywall\/":2683,"http:\/\/liorpachter.wordpress.com\/2014\/04\/17\/does-researching-casual-marijuana-use-cause-brain-abnormalities\/":2656,"http:\/\/liorpachter.wordpress.com\/2014\/03\/25\/designing-roofs-and-drawing-phylogenetic-trees\/":2608,"http:\/\/liorpachter.wordpress.com\/2014\/03\/18\/reproducibility-vs-usability\/":2569,"http:\/\/liorpachter.wordpress.com\/2014\/03\/01\/using-statistical-methods-to-estimate-and-take-into-account-experimental-measurement-errors-a-case-study-using-high-throughput-proteomics-data\/":2448,"http:\/\/liorpachter.wordpress.com\/2014\/02\/18\/number-deconvolution\/":2330}	</script>
	<div id="sharing_email" style="display: none;">
		<form action="/" method="post">
			<label for="target_email">Send to Email Address</label>
			<input type="email" name="target_email" id="target_email" value="" />

			
				<label for="source_name">Your Name</label>
				<input type="text" name="source_name" id="source_name" value="" />

				<label for="source_email">Your Email Address</label>
				<input type="email" name="source_email" id="source_email" value="" />

			
			<div class="recaptcha" id="sharing_recaptcha"></div><input type="hidden" name="recaptcha_public_key" id="recaptcha_public_key" value="6LcYW8MSAAAAADBAuEH9yaPcF7lWh11Iq62ZKtoo" />
			<img style="float: right; display: none" class="loading" src="http://s2.wp.com/wp-content/mu-plugins/post-flair/sharing/images/loading.gif?m=1315610318g" alt="loading" width="16" height="16" />
			<input type="submit" value="Send Email" class="sharing_send" />
			<a href="#cancel" class="sharing_cancel">Cancel</a>

			<div class="errors errors-1" style="display: none;">
				Post was not sent - check your email addresses!			</div>

			<div class="errors errors-2" style="display: none;">
				Email check failed, please try again			</div>

			<div class="errors errors-3" style="display: none;">
				Sorry, your blog cannot share posts by email.			</div>
		</form>
	</div>
		<script type="text/javascript">
		jQuery(document).on( 'ready post-load', function(){
			jQuery( 'a.share-twitter' ).on( 'click', function() {
				window.open( jQuery(this).attr( 'href' ), 'wpcomtwitter', 'menubar=1,resizable=1,width=600,height=350' );
				return false;
			});
		});
		</script>
				<script type="text/javascript">
		jQuery(document).on( 'ready post-load', function(){
			jQuery( 'a.share-google-plus-1' ).on( 'click', function() {
				window.open( jQuery(this).attr( 'href' ), 'wpcomgoogle-plus-1', 'menubar=1,resizable=1,width=480,height=550' );
				return false;
			});
		});
		</script>
				<script type="text/javascript">
		jQuery(document).on( 'ready post-load', function(){
			jQuery( 'a.share-linkedin' ).on( 'click', function() {
				window.open( jQuery(this).attr( 'href' ), 'wpcomlinkedin', 'menubar=1,resizable=1,width=580,height=450' );
				return false;
			});
		});
		</script>
				<script type="text/javascript">
		jQuery(document).on( 'ready post-load', function(){
			jQuery( 'a.share-facebook' ).on( 'click', function() {
				window.open( jQuery(this).attr( 'href' ), 'wpcomfacebook', 'menubar=1,resizable=1,width=600,height=400' );
				return false;
			});
		});
		</script>
				<iframe src='http://widgets.wp.com/likes/master.html?ver=20140528#ver=20140528&amp;mp6=1' scrolling='no' id='likes-master' name='likes-master' style='display:none;'></iframe>
		<div id='likes-other-gravatars'><div class="likes-text"><span>%d</span> bloggers like this:</div><ul class="wpl-avatars sd-like-gravatars"></ul></div>
		<script type="text/javascript">
		//<![CDATA[
			var jetpackLikesWidgetQueue = [];
			var jetpackLikesWidgetBatch = [];
			var jetpackLikesMasterReady = false;

			function JetpackLikespostMessage( message, target ) {
				if ( "string" === typeof message ){
					try{
						message = JSON.parse( message );
					}
					catch(e) {
						return;
					}
				}

				pm( {
					target: target,
					type: 'likesMessage',
					data: message,
					origin: '*'
				} );
			}

			function JetpackLikesBatchHandler() {
				var requests = [];
				jQuery( 'div.jetpack-likes-widget-unloaded' ).each( function( i ) {
					if ( jetpackLikesWidgetBatch.indexOf( this.id ) > -1 )
						return;
					jetpackLikesWidgetBatch.push( this.id );
					var regex = /like-(post|comment)-wrapper-(\d+)-(\d+)-(\w+)/;
					var match = regex.exec( this.id );
					if ( ! match || match.length != 5 )
						return;

					var info = {
						blog_id: match[2],
						width:   this.width
					};

					if ( 'post' == match[1] ) {
						info.post_id = match[3];
					} else if ( 'comment' == match[1] ) {
						info.comment_id = match[3];
					}

					info.obj_id = match[4];

					requests.push( info );
				});

				if ( requests.length > 0 ) {
					JetpackLikespostMessage( { event: 'initialBatch', requests: requests }, window.frames['likes-master'] );
				}
			}

			function JetpackLikesMessageListener( event ) {
				if ( "undefined" == typeof event.event )
					return;

				if ( 'masterReady' == event.event ) {
					jQuery( document ).ready( function() {
						jetpackLikesMasterReady = true;

						var stylesData = {
								event: 'injectStyles'
						};

						if ( jQuery( 'iframe.admin-bar-likes-widget' ).length > 0 ) {
							JetpackLikespostMessage( { event: 'adminBarEnabled' }, window.frames[ 'likes-master' ] );

							stylesData.adminBarStyles = {
								background: jQuery( '#wpadminbar .quicklinks li#wp-admin-bar-wpl-like > a' ).css( 'background' ),
								isRtl: ( 'rtl' == jQuery( '#wpadminbar' ).css( 'direction' ) )
							};
						}

						if ( !window.addEventListener )
							jQuery( '#wp-admin-bar-admin-bar-likes-widget' ).hide();

						stylesData.textStyles = {
							color: jQuery( '.sd-text-color').css( 'color' ),
							fontFamily: jQuery( '.sd-text-color' ).css( 'font-family' ),
							fontSize: jQuery( '.sd-text-color' ).css( 'font-size' ),
							direction: jQuery( '.sd-text-color' ).css( 'direction' ),
							fontWeight: jQuery( '.sd-text-color' ).css( 'font-weight' ),
							fontStyle: jQuery( '.sd-text-color' ).css( 'font-style' ),
							textDecoration: jQuery( '.sd-text-color' ).css('text-decoration')
						};

						stylesData.linkStyles = {
							color: jQuery( '.sd-link-color' ).css('color'),
							fontFamily: jQuery( '.sd-link-color' ).css('font-family'),
							fontSize: jQuery( '.sd-link-color' ).css('font-size'),
							textDecoration: jQuery( '.sd-link-color' ).css('text-decoration'),
							fontWeight: jQuery( '.sd-link-color' ).css( 'font-weight' ),
							fontStyle: jQuery( '.sd-link-color' ).css( 'font-style' )
						};

						JetpackLikespostMessage( stylesData, window.frames[ 'likes-master' ] );

						JetpackLikesBatchHandler();

						jQuery( document ).on( 'inview', 'div.jetpack-likes-widget-unloaded', function() {
							jetpackLikesWidgetQueue.push( this.id );
						});
					});
				}

				if ( 'showLikeWidget' == event.event ) {
					jQuery( '#' + event.id + ' .post-likes-widget-placeholder'  ).fadeOut( 'fast', function() {
						jQuery( '#' + event.id + ' .post-likes-widget' ).fadeIn( 'fast', function() {
							JetpackLikespostMessage( { event: 'likeWidgetDisplayed', blog_id: event.blog_id, post_id: event.post_id, obj_id: event.obj_id }, window.frames['likes-master'] );
						});
					});
				}

				if ( 'clickReblogFlair' == event.event ) {
					wpcom_reblog.toggle_reblog_box_flair( event.obj_id );
				}

				if ( 'showOtherGravatars' == event.event ) {
					var $container = jQuery( '#likes-other-gravatars' );
					var $list = $container.find( 'ul' );

					$container.hide();
					$list.html( '' );

					$container.find( '.likes-text span' ).text( event.total );

					jQuery.each( event.likers, function( i, liker ) {
						$list.append( '<li class="' + liker.css_class + '"><a href="' + liker.profile_URL + '" class="wpl-liker" rel="nofollow" target="_parent"><img src="' + liker.avatar_URL + '" alt="' + liker.name + '" width="30" height="30" style="padding-right: 3px;" /></a></li>');
					} );

					var offset = jQuery( "[name='" + event.parent + "']" ).offset();

					$container.css( 'left', offset.left + event.position.left - 10 + 'px' );
					$container.css( 'top', offset.top + event.position.top - 33 + 'px' );

					var rowLength = Math.floor( event.width / 37 );
					var height = ( Math.ceil( event.likers.length / rowLength ) * 37 ) + 13;
					if ( height > 204 ) {
						height = 204;
					}

					$container.css( 'height', height + 'px' );
					$container.css( 'width', rowLength * 37 - 7 + 'px' );

					$list.css( 'width', rowLength * 37 + 'px' );

					$container.fadeIn( 'slow' );

					var scrollbarWidth = $list[0].offsetWidth - $list[0].clientWidth;
					if ( scrollbarWidth > 0 ) {
						$container.width( $container.width() + scrollbarWidth );
						$list.width( $list.width() + scrollbarWidth );
					}
				}
			}

			pm.bind( 'likesMessage', function(e) { JetpackLikesMessageListener(e); } );

			jQuery( document ).click( function( e ) {
				var $container = jQuery( '#likes-other-gravatars' );

				if ( $container.has( e.target ).length === 0 ) {
					$container.fadeOut( 'slow' );
				}
			});

			function JetpackLikesWidgetQueueHandler() {
				var wrapperID;
				if ( ! jetpackLikesMasterReady ) {
					setTimeout( JetpackLikesWidgetQueueHandler, 500 );
					return;
				}

				if ( jetpackLikesWidgetQueue.length > 0 ) {
					// We may have a widget that needs creating now
					var found = false;
					while( jetpackLikesWidgetQueue.length > 0 ) {
						// Grab the first member of the queue that isn't already loading.
						wrapperID = jetpackLikesWidgetQueue.splice( 0, 1 )[0];
						if ( jQuery( '#' + wrapperID ).hasClass( 'jetpack-likes-widget-unloaded' ) ) {
							found = true;
							break;
						}
					}
					if ( ! found ) {
						setTimeout( JetpackLikesWidgetQueueHandler, 500 );
						return;
					}
				} else if ( jQuery( 'div.jetpack-likes-widget-unloaded' ).length > 0 ) {
					// Grab any unloaded widgets for a batch request
					JetpackLikesBatchHandler();

					// Get the next unloaded widget
					wrapperID = jQuery( 'div.jetpack-likes-widget-unloaded' ).first()[0].id;
					if ( ! wrapperID ) {
						// Everything is currently loaded
						setTimeout( JetpackLikesWidgetQueueHandler, 500 );
						return;
					}
				}

				if ( 'undefined' === typeof wrapperID ) {
					setTimeout( JetpackLikesWidgetQueueHandler, 500 );
					return;
				}

				var $wrapper = jQuery( '#' + wrapperID );
				$wrapper.find( 'iframe' ).remove();

				if ( $wrapper.hasClass( 'slim-likes-widget' ) ) {
					$wrapper.find( '.post-likes-widget-placeholder' ).after( "<iframe class='post-likes-widget jetpack-likes-widget' name='" + $wrapper.data( 'name' ) + "' height='22px' width='68px' frameBorder='0' scrolling='no' src='" + $wrapper.data( 'src' ) + "'></iframe>" );
				} else {
					$wrapper.find( '.post-likes-widget-placeholder' ).after( "<iframe class='post-likes-widget jetpack-likes-widget' name='" + $wrapper.data( 'name' ) + "' height='55px' width='100%' frameBorder='0' src='" + $wrapper.data( 'src' ) + "'></iframe>" );
				}

				$wrapper.removeClass( 'jetpack-likes-widget-unloaded' ).addClass( 'jetpack-likes-widget-loading' );

				$wrapper.find( 'iframe' ).load( function( e ) {
					var $iframe = jQuery( e.target );
					$wrapper.removeClass( 'jetpack-likes-widget-loading' ).addClass( 'jetpack-likes-widget-loaded' );

					JetpackLikespostMessage( { event: 'loadLikeWidget', name: $iframe.attr( 'name' ), width: $iframe.width() }, window.frames[ 'likes-master' ] );

					if ( $wrapper.hasClass( 'slim-likes-widget' ) ) {
						$wrapper.find( 'iframe' ).Jetpack( 'resizeable' );
					}
				});
				setTimeout( JetpackLikesWidgetQueueHandler, 250 );
			}
			JetpackLikesWidgetQueueHandler();
		//]]>
		</script>

	<div id="carousel-reblog-box">
		<form action="" name="carousel-reblog">
			<textarea id="carousel-reblog-content" name="carousel-reblog-content" onclick="if ( this.value == 'Add your thoughts here... (optional)' ) { this.value = ''; }" onblur="if ( this.value == '' || this.value == '' ) { this.value = 'Add your thoughts here... (optional)'; }">Add your thoughts here... (optional)</textarea>
			<label for="carousel-reblog-to-blog-id" id="carousel-reblog-lblogid">Post to</label>
			<select name="carousel-reblog-to-blog-id" id="carousel-reblog-to-blog-id">
						</select>

			<div class="submit">
				<span class="canceltext"><a href="" class="cancel">Cancel</a></span>
				<input type="submit" name="carousel-reblog-submit" class="button" id="carousel-reblog-submit" value="Reblog Post" />
				<input type="hidden" id="carousel-reblog-blog-id" value="12758541" />
				<input type="hidden" id="carousel-reblog-blog-url" value="http://liorpachter.wordpress.com" />
				<input type="hidden" id="carousel-reblog-blog-title" value="Bits of DNA" />
				<input type="hidden" id="carousel-reblog-post-url" value="" />
				<input type="hidden" id="carousel-reblog-post-title" value="" />
			</div>

			<input type="hidden" id="_wpnonce" name="_wpnonce" value="8e3b5add07" /><input type="hidden" name="_wp_http_referer" value="/" />		</form>

		<div class="arrow"></div>
	</div>
<link rel='stylesheet' id='all-css-0' href='http://s0.wp.com/_static/??/wp-content/mu-plugins/carousel/jetpack-carousel.css,/wp-content/mu-plugins/tiled-gallery/tiled-gallery.css?m=1401143454j' type='text/css' media='all' />
<script type='text/javascript'>
/* <![CDATA[ */
var jetpackSlideshowSettings = {"spinner":"http:\/\/s1.wp.com\/wp-content\/mu-plugins\/shortcodes\/img\/slideshow-loader.gif","blog_id":"12758541","blog_subdomain":"liorpachter","user_id":"0"};
/* ]]> */
</script>
<script type='text/javascript'>
/* <![CDATA[ */
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"http:\/\/liorpachter.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"4962e725a9","display_exif":"1","display_geo":"1","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/liorpachter.wordpress.com\/wp-login.php?redirect_to=http%3A%2F%2Fliorpachter.wordpress.com%2F2014%2F02%2F18%2Fnumber-deconvolution%2F","local_comments_commenting_as":"<fieldset><label for=\"email\">Email (Required)<\/label> <input type=\"text\" name=\"email\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-email-field\" \/><\/fieldset><fieldset><label for=\"author\">Name (Required)<\/label> <input type=\"text\" name=\"author\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-author-field\" \/><\/fieldset><fieldset><label for=\"url\">Website<\/label> <input type=\"text\" name=\"url\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-url-field\" \/><\/fieldset>","reblog":"Reblog","reblogged":"Reblogged","reblog_add_thoughts":"Add your thoughts here... (optional)","reblogging":"Reblogging...","post_reblog":"Post Reblog","stats_query_args":"blog=12758541&v=wpcom&tz=-7&user_id=0&subd=liorpachter","is_public":"1"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"http:\/\/liorpachter.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"4962e725a9","display_exif":"1","display_geo":"1","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/liorpachter.wordpress.com\/wp-login.php?redirect_to=http%3A%2F%2Fliorpachter.wordpress.com%2F2014%2F02%2F18%2Fnumber-deconvolution%2F","local_comments_commenting_as":"<fieldset><label for=\"email\">Email (Required)<\/label> <input type=\"text\" name=\"email\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-email-field\" \/><\/fieldset><fieldset><label for=\"author\">Name (Required)<\/label> <input type=\"text\" name=\"author\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-author-field\" \/><\/fieldset><fieldset><label for=\"url\">Website<\/label> <input type=\"text\" name=\"url\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-url-field\" \/><\/fieldset>","reblog":"Reblog","reblogged":"Reblogged","reblog_add_thoughts":"Add your thoughts here... (optional)","reblogging":"Reblogging...","post_reblog":"Post Reblog","stats_query_args":"blog=12758541&v=wpcom&tz=-7&user_id=0&subd=liorpachter","is_public":"1"};
/* ]]> */
</script>
<script type='text/javascript' src='http://s0.wp.com/_static/??-eJyNjtEOgjAMRX/IMUkg4cX4LViKdI5trh3I34skI8YH4ttte89J9RwUeCfoRBvWHU4EGF6F4ZP+Oo1JBZvu5FiT68mRLHs46PLgo4DvkD9y80wYlwIWsPg3xJbWNPhZ7fvMkgObcm3u/c0gyK84W6dV43XLjLIB2xwiMh+8Am30idFqgxJaeKi8WJnreCmrc1U2TVXX5g1Lx3tf'></script>
<script type='text/javascript' src='http://platform.twitter.com/widgets.js?ver=20111117'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var recaptcha_options = {"lang":"en"};
/* ]]> */
</script>
<script type='text/javascript' src='http://s0.wp.com/_static/??-eJyVzTEOwjAMheELYZyiLh0QZ0kbk7q4buS4VNyeLgwsRUxP//Dp4VZgWNRJHecViqyZtaJv7E4GvcTh0bMlKEyo9CQjTaz5PNUT/mPLb8dCCXIUIXt914EqS3W4S2TDOkbbLz67o9t8bdrQNl0I3WV6A5XFV+Q='></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script><script type="text/javascript">
			jQuery.extend( infiniteScroll.settings.scripts, ["jquery","mobile-useragent-info","jquery-core","jquery-migrate","postmessage","jquery_inview","jetpack_resize","loggedout-subscribe","spin","jquery.spin","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","jquery-cycle","jetpack-slideshow","swfobject","videopress","jetpack-carousel","twitter-widgets","twitter-widgets-infinity","twitter-widgets-pending","tiled-gallery","sharing-js"] );
			jQuery.extend( infiniteScroll.settings.styles, ["smileyproject","jetpack_likes","loggedout-subscribe","the-neverending-homepage","tarski-infinite-scroll","jetpack-slideshow","wpcom-core-compat-playlist-styles","mp6hacks","tarski","tarski-print","noticons","geo-location-flair","reblogging","sharedaddy","genericons","h4-global","widget-goodreads","jetpack-carousel","tiled-gallery"] );
		</script><script src="http://s.stats.wordpress.com/w.js?21" type="text/javascript"></script>
<script type="text/javascript">
st_go({'blog':'12758541','v':'wpcom','tz':'-7','user_id':'0','subd':'liorpachter'});
ex_go({'crypt':'UE40eW5QN0p8M2Y/RE1LVmwrVi5vQS5fVFtfdHBbPyw1VXIrU3hWLHhmcmw0bWUwNiVnR3N1V2dDZTM4MGFxVndRQl1HNkJtOEQ0dVBbbk9jP2g4T29lWFFNRW1ZYVdrb2hfUjIwd3FvV1klLlZxNUElanc0clpfYXhQa2dsSzNjZTRYLy9Scz9ndWxrOTQyOVRlMHYxXWklPS9ULkEwekMwRTFUNm8vbSs1Yz1iMyZ4dnxMYi4uRFF2amVVZT9wK2FsLTFdeHI2LFpMM2QtXUROMTJSSEhCM011bXc2JXpwVXx4amw2ekJZRDNRVnUmJUI1Z09+bnpubEJBSFNuTEU5VTksQzVrZml6VjRYTjNhRU9jZkMvRm0saElKWGhDTHdnMWdWRi1neDMlTDBYcVQzTCs4dz1zc1NWYnxxRH56N3BGLHEsaE12UjlmYWZZ'});
addLoadEvent(function(){linktracker_init('12758541',0);});
	</script>
<noscript><img src="http://stats.wordpress.com/b.gif?v=noscript" style="height:0px;width:0px;overflow:hidden" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//stats.wordpress.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script></body></html>